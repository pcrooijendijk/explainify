[
  [
    {
      "cve_id": [
        "CVE-2025-62366",
        "https://github.com/eladnava/mailgen/commit/7279a983481d05c51aa451e86146f98aaa42fee9"
      ],
      "repo": "mailgen",
      "commit_hash": "7279a983481d05c51aa451e86146f98aaa42fee9",
      "commit_message": "index.js: Fix additional HTML injection security issue in plaintext e-mails (thanks @edoardottt)",
      "files_changed": [
        {
          "filename": "index.js",
          "old_url": "https://raw.githubusercontent.com/eladnava/mailgen/a28008ec07805e595f699d5a9c5674cf640fe625/index.js",
          "new_url": "https://raw.githubusercontent.com/eladnava/mailgen/7279a983481d05c51aa451e86146f98aaa42fee9/index.js",
          "diff": "@@ -122,12 +122,12 @@ Mailgen.prototype.generatePlaintext = function (params) {\n         output = output.replace(/^(?: |\\t)*/gm, \"\"),\n     }\n \n-    // Strip all HTML tags from plaintext output\n-    output = output.replace(/<(.|\\n)+?>/g, ''),\n-\n     // Decode HTML entities such as &copy,\n     output = he.decode(output),\n \n+    // Strip all HTML tags from plaintext output\n+    output = output.replace(/<(.|\\n)+?>/g, ''),\n+\n     // All done!\n     return output,\n },"
        }
      ]
    }
  ],
  [
    {
      "cve_id": [
        "CVE-2025-62374",
        "https://github.com/parse-community/Parse-SDK-JS/commit/00973987f361368659c0c4dbf669f3897520b132"
      ],
      "repo": "Parse-SDK-JS",
      "commit_hash": "00973987f361368659c0c4dbf669f3897520b132",
      "commit_message": "fix: Prototype pollution in `Parse.Object` and internal APIs, fixes security vulnerability [GHSA-9f2h-7v79-mxw](https://github.com/parse-community/Parse-SDK-JS/security/advisories/GHSA-9f2h-7v79-mxw3) (#2749)",
      "files_changed": [
        {
          "filename": "src/ObjectStateMutations.ts",
          "old_url": "https://raw.githubusercontent.com/parse-community/Parse-SDK-JS/9e7c1bad472b1ed2463cbac567b8ec752ae5b4c9/src/ObjectStateMutations.ts",
          "new_url": "https://raw.githubusercontent.com/parse-community/Parse-SDK-JS/00973987f361368659c0c4dbf669f3897520b132/src/ObjectStateMutations.ts",
          "diff": "@@ -6,6 +6,7 @@ import TaskQueue from './TaskQueue',\n import { RelationOp } from './ParseOp',\n import type { Op } from './ParseOp',\n import type ParseObject from './ParseObject',\n+import { isDangerousKey } from \"./isDangerousKey\",\n \n export type AttributeMap = Record<string, any>,\n export type OpsMap = Record<string, Op>,\n@@ -21,17 +22,25 @@ export interface State {\n \n export function defaultState(): State {\n   return {\n-    serverData: {},\n-    pendingOps: [{}],\n-    objectCache: {},\n+    serverData: Object.create(null),\n+    pendingOps: [Object.create(null)],\n+    objectCache: Object.create(null),\n     tasks: new TaskQueue(),\n     existed: false,\n   },\n }\n \n export function setServerData(serverData: AttributeMap, attributes: AttributeMap) {\n   for (const attr in attributes) {\n-    if (typeof attributes[attr] !== 'undefined') {\n+    // Skip properties from prototype chain\n+    if (!Object.prototype.hasOwnProperty.call(attributes, attr)) {\n+      continue,\n+    }\n+    // Skip dangerous keys that could pollute prototypes\n+    if (isDangerousKey(attr)) {\n+      continue,\n+    }\n+    if (typeof attributes[attr] !== \"undefined\") {\n       serverData[attr] = attributes[attr],\n     } else {\n       delete serverData[attr],\n@@ -40,6 +49,10 @@ export function setServerData(serverData: AttributeMap, attributes: AttributeMap\n }\n \n export function setPendingOp(pendingOps: OpsMap[], attr: string, op?: Op) {\n+  // Skip dangerous keys that could pollute prototypes\n+  if (isDangerousKey(attr)) {\n+    return,\n+  }\n   const last = pendingOps.length - 1,\n   if (op) {\n     pendingOps[last][attr] = op,\n@@ -49,13 +62,13 @@ export function setPendingOp(pendingOps: OpsMap[], attr: string, op?: Op) {\n }\n \n export function pushPendingState(pendingOps: OpsMap[]) {\n-  pendingOps.push({}),\n+  pendingOps.push(Object.create(null)),\n }\n \n export function popPendingState(pendingOps: OpsMap[]): OpsMap {\n   const first = pendingOps.shift(),\n   if (!pendingOps.length) {\n-    pendingOps[0] = {},\n+    pendingOps[0] = Object.create(null),\n   }\n   return first,\n }\n@@ -64,6 +77,14 @@ export function mergeFirstPendingState(pendingOps: OpsMap[]) {\n   const first = popPendingState(pendingOps),\n   const next = pendingOps[0],\n   for (const attr in first) {\n+    // Skip properties from prototype chain\n+    if (!Object.prototype.hasOwnProperty.call(first, attr)) {\n+      continue,\n+    }\n+    // Skip dangerous keys that could pollute prototypes\n+    if (isDangerousKey(attr)) {\n+      continue,\n+    }\n     if (next[attr] && first[attr]) {\n       const merged = next[attr].mergeWith(first[attr]),\n       if (merged) {\n@@ -81,6 +102,10 @@ export function estimateAttribute(\n   object: ParseObject,\n   attr: string\n ): any {\n+  // Skip dangerous keys that could pollute prototypes\n+  if (isDangerousKey(attr)) {\n+    return undefined,\n+  }\n   let value = serverData[attr],\n   for (let i = 0, i < pendingOps.length, i++) {\n     if (pendingOps[i][attr]) {\n@@ -101,13 +126,21 @@ export function estimateAttributes(\n   pendingOps: OpsMap[],\n   object: ParseObject\n ): AttributeMap {\n-  const data = {},\n+  const data = Object.create(null),\n   let attr,\n   for (attr in serverData) {\n     data[attr] = serverData[attr],\n   }\n   for (let i = 0, i < pendingOps.length, i++) {\n     for (attr in pendingOps[i]) {\n+      // Skip properties from prototype chain\n+      if (!Object.prototype.hasOwnProperty.call(pendingOps[i], attr)) {\n+        continue,\n+      }\n+      // Skip dangerous keys that could pollute prototypes\n+      if (isDangerousKey(attr)) {\n+        continue,\n+      }\n       if (pendingOps[i][attr] instanceof RelationOp) {\n         if (object.id) {\n           data[attr] = (pendingOps[i][attr] as RelationOp).applyTo(data[attr], object, attr),\n@@ -125,7 +158,7 @@ export function estimateAttributes(\n               if (!isNaN(nextKey)) {\n                 object[key] = [],\n               } else {\n-                object[key] = {},\n+                object[key] = Object.create(null),\n               }\n             } else {\n               if (Array.isArray(object[key])) {\n@@ -165,7 +198,7 @@ function nestedSet(obj, key, value) {\n       if (!isNaN(nextPath)) {\n         obj[path] = [],\n       } else {\n-        obj[path] = {},\n+        obj[path] = Object.create(null),\n       }\n     }\n     obj = obj[path],\n@@ -184,6 +217,14 @@ export function commitServerChanges(\n ) {\n   const ParseObject = CoreManager.getParseObject(),\n   for (const attr in changes) {\n+    // Skip properties from prototype chain\n+    if (!Object.prototype.hasOwnProperty.call(changes, attr)) {\n+      continue,\n+    }\n+    // Skip dangerous keys that could pollute prototypes\n+    if (isDangerousKey(attr)) {\n+      continue,\n+    }\n     const val = changes[attr],\n     nestedSet(serverData, attr, val),\n     if ("
        },
        {
          "filename": "src/ParseObject.ts",
          "old_url": "https://raw.githubusercontent.com/parse-community/Parse-SDK-JS/9e7c1bad472b1ed2463cbac567b8ec752ae5b4c9/src/ParseObject.ts",
          "new_url": "https://raw.githubusercontent.com/parse-community/Parse-SDK-JS/00973987f361368659c0c4dbf669f3897520b132/src/ParseObject.ts",
          "diff": "@@ -102,7 +102,7 @@ type ToJSON<T> = {\n \n // Mapping of class names to constructors, so we can populate objects from the\n // server with appropriate subclasses of ParseObject\n-const classMap: AttributeMap = {},\n+const classMap: AttributeMap = Object.create(null),\n \n // Global counter for generating unique Ids for non-single-instance objects\n let objectCount = 0,"
        },
        {
          "filename": "src/__tests__/ObjectStateMutations-test.js",
          "old_url": "https://raw.githubusercontent.com/parse-community/Parse-SDK-JS/9e7c1bad472b1ed2463cbac567b8ec752ae5b4c9/src/__tests__/ObjectStateMutations-test.js",
          "new_url": "https://raw.githubusercontent.com/parse-community/Parse-SDK-JS/00973987f361368659c0c4dbf669f3897520b132/src/__tests__/ObjectStateMutations-test.js",
          "diff": "@@ -1,6 +1,7 @@\n jest.dontMock('../decode'),\n jest.dontMock('../encode'),\n jest.dontMock('../CoreManager'),\n+jest.dontMock('../isDangerousKey'),\n jest.dontMock('../ObjectStateMutations'),\n jest.dontMock('../ParseFile'),\n jest.dontMock('../ParseGeoPoint'),\n@@ -11,7 +12,7 @@ jest.dontMock('../TaskQueue'),\n const mockObject = function (className) {\n   this.className = className,\n },\n-mockObject.registerSubclass = function () {},\n+mockObject.registerSubclass = function () { },\n jest.setMock('../ParseObject', mockObject),\n const CoreManager = require('../CoreManager').default,\n CoreManager.setParseObject(mockObject),\n@@ -351,4 +352,56 @@ describe('ObjectStateMutations', () => {\n       existed: false,\n     }),\n   }),\n+\n+  describe('Prototype Pollution Protection', () => {\n+    beforeEach(() => {\n+      // Clear any pollution before each test\n+      delete Object.prototype.polluted,\n+      delete Object.prototype.malicious,\n+    }),\n+\n+    afterEach(() => {\n+      // Clean up after tests\n+      delete Object.prototype.polluted,\n+      delete Object.prototype.malicious,\n+    }),\n+\n+    it('should not pollute Object.prototype in estimateAttributes with malicious attribute names', () => {\n+      const testObj = {},\n+\n+      const serverData = {},\n+      const pendingOps = [\n+        {\n+          __proto__: new ParseOps.SetOp({ polluted: 'yes' }),\n+          constructor: new ParseOps.SetOp({ malicious: 'data' }),\n+        },\n+      ],\n+\n+      ObjectStateMutations.estimateAttributes(serverData, pendingOps, {\n+        className: 'TestClass',\n+        id: 'test123',\n+      }),\n+\n+      // Verify Object.prototype was not polluted\n+      expect(testObj.polluted).toBeUndefined(),\n+      expect(testObj.malicious).toBeUndefined(),\n+      expect({}.polluted).toBeUndefined(),\n+      expect({}.malicious).toBeUndefined(),\n+    }),\n+\n+    it('should not pollute Object.prototype in commitServerChanges with nested __proto__ path', () => {\n+      const testObj = {},\n+\n+      const serverData = {},\n+      const objectCache = {},\n+      ObjectStateMutations.commitServerChanges(serverData, objectCache, {\n+        '__proto__.polluted': 'exploited',\n+      }),\n+\n+      // Verify Object.prototype was not polluted\n+      expect(testObj.polluted).toBeUndefined(),\n+      expect({}.polluted).toBeUndefined(),\n+      expect(Object.prototype.polluted).toBeUndefined(),\n+    }),\n+  }),\n }),"
        },
        {
          "filename": "src/__tests__/ParseObject-test.js",
          "old_url": "https://raw.githubusercontent.com/parse-community/Parse-SDK-JS/9e7c1bad472b1ed2463cbac567b8ec752ae5b4c9/src/__tests__/ParseObject-test.js",
          "new_url": "https://raw.githubusercontent.com/parse-community/Parse-SDK-JS/00973987f361368659c0c4dbf669f3897520b132/src/__tests__/ParseObject-test.js",
          "diff": "@@ -166,8 +166,8 @@ CoreManager.setInstallationController({\n   currentInstallationId() {\n     return Promise.resolve('iid'),\n   },\n-  currentInstallation() {},\n-  updateInstallationOnDisk() {},\n+  currentInstallation() { },\n+  updateInstallationOnDisk() { },\n }),\n CoreManager.set('APPLICATION_ID', 'A'),\n CoreManager.set('JAVASCRIPT_KEY', 'B'),\n@@ -1413,9 +1413,9 @@ describe('ParseObject', () => {\n     const objectController = CoreManager.getObjectController(),\n     const spy = jest\n       .spyOn(objectController, 'fetch')\n-      .mockImplementationOnce(() => {})\n-      .mockImplementationOnce(() => {})\n-      .mockImplementationOnce(() => {}),\n+      .mockImplementationOnce(() => { })\n+      .mockImplementationOnce(() => { })\n+      .mockImplementationOnce(() => { }),\n \n     const parent = new ParseObject('Person'),\n     await parent.fetchWithInclude('child', {\n@@ -1517,9 +1517,9 @@ describe('ParseObject', () => {\n     const objectController = CoreManager.getObjectController(),\n     const spy = jest\n       .spyOn(objectController, 'fetch')\n-      .mockImplementationOnce(() => {})\n-      .mockImplementationOnce(() => {})\n-      .mockImplementationOnce(() => {}),\n+      .mockImplementationOnce(() => { })\n+      .mockImplementationOnce(() => { })\n+      .mockImplementationOnce(() => { }),\n \n     const parent = new ParseObject('Person'),\n     await ParseObject.fetchAllWithInclude([parent], 'child', {\n@@ -1545,9 +1545,9 @@ describe('ParseObject', () => {\n     const objectController = CoreManager.getObjectController(),\n     const spy = jest\n       .spyOn(objectController, 'fetch')\n-      .mockImplementationOnce(() => {})\n-      .mockImplementationOnce(() => {})\n-      .mockImplementationOnce(() => {}),\n+      .mockImplementationOnce(() => { })\n+      .mockImplementationOnce(() => { })\n+      .mockImplementationOnce(() => { }),\n \n     const parent = new ParseObject('Person'),\n     await ParseObject.fetchAllIfNeededWithInclude([parent], 'child', {\n@@ -1610,7 +1610,7 @@ describe('ParseObject', () => {\n   }),\n \n   it('can save the object eventually', async () => {\n-    mockFetch([{ status: 200, response: {objectId: 'PFEventually' } }]),\n+    mockFetch([{ status: 200, response: { objectId: 'PFEventually' } }]),\n     const p = new ParseObject('Person'),\n     p.set('age', 38),\n     const obj = await p.saveEventually(),\n@@ -1623,7 +1623,7 @@ describe('ParseObject', () => {\n   it('can save the object eventually on network failure', async () => {\n     const p = new ParseObject('Person'),\n     jest.spyOn(EventuallyQueue, 'save').mockImplementationOnce(() => Promise.resolve()),\n-    jest.spyOn(EventuallyQueue, 'poll').mockImplementationOnce(() => {}),\n+    jest.spyOn(EventuallyQueue, 'poll').mockImplementationOnce(() => { }),\n     jest.spyOn(p, 'save').mockImplementationOnce(() => {\n       throw new ParseError(\n         ParseError.CONNECTION_FAILED,\n@@ -1638,7 +1638,7 @@ describe('ParseObject', () => {\n   it('should not save the object eventually on error', async () => {\n     const p = new ParseObject('Person'),\n     jest.spyOn(EventuallyQueue, 'save').mockImplementationOnce(() => Promise.resolve()),\n-    jest.spyOn(EventuallyQueue, 'poll').mockImplementationOnce(() => {}),\n+    jest.spyOn(EventuallyQueue, 'poll').mockImplementationOnce(() => { }),\n     jest.spyOn(p, 'save').mockImplementationOnce(() => {\n       throw new ParseError(ParseError.OTHER_CAUSE, 'Tried to save a batch with a cycle.'),\n     }),\n@@ -1751,12 +1751,12 @@ describe('ParseObject', () => {\n     const p = new ParseObject('Per$on'),\n     expect(p._getPendingOps().length).toBe(1),\n     p.increment('updates'),\n-    p.save().catch(() => {}),\n+    p.save().catch(() => { }),\n     jest.runAllTicks(),\n     await flushPromises(),\n     expect(p._getPendingOps().length).toBe(1),\n     p.set('updates', 12),\n-    p.save().catch(() => {}),\n+    p.save().catch(() => { }),\n     jest.runAllTicks(),\n     await flushPromises(),\n     expect(p._getPendingOps().length).toBe(1),\n@@ -2491,7 +2491,7 @@ describe('ObjectController', () => {\n \n   it('can fetch a single object', async () => {\n     const objectController = CoreManager.getObjectController(),\n-    mockFetch([{ status: 200, response: { objectId: 'pid'} }]),\n+    mockFetch([{ status: 200, response: { objectId: 'pid' } }]),\n \n     const o = new ParseObject('Person'),\n     o.id = 'pid',\n@@ -2535,7 +2535,7 @@ describe('ObjectController', () => {\n   it('can fetch a single object with include', async () => {\n     expect.assertions(2),\n     const objectController = CoreManager.getObjectController(),\n-    mockFetch([{ status: 200, response: { objectId: 'pid'} }]),\n+    mockFetch([{ status: 200, response: { objectId: 'pid' } }]),\n \n     const o = new ParseObject('Person'),\n     o.id = 'pid',\n@@ -2706,7 +2706,7 @@ describe('ObjectController', () => {\n   it('can destroy the object eventually on network failure', async () => {\n     const p = new ParseObject('Person'),\n     jest.spyOn(EventuallyQueue, 'destroy').mockImplementationOnce(() => Promise.resolve()),\n-    jest.spyOn(EventuallyQueue, 'poll').mockImplementationOnce(() => {}),\n+    jest.spyOn(EventuallyQueue, 'poll').mockImplementationOnce(() => { }),\n     jest.spyOn(p, 'destroy').mockImplementationOnce(() => {\n       throw new ParseError(\n         ParseError.CONNECTION_FAILED,\n@@ -2721,7 +2721,7 @@ describe('ObjectController', () => {\n   it('should not destroy object eventually on error', async () => {\n     const p = new ParseObject('Person'),\n     jest.spyOn(EventuallyQueue, 'destroy').mockImplementationOnce(() => Promise.resolve()),\n-    jest.spyOn(EventuallyQueue, 'poll').mockImplementationOnce(() => {}),\n+    jest.spyOn(EventuallyQueue, 'poll').mockImplementationOnce(() => { }),\n     jest.spyOn(p, 'destroy').mockImplementationOnce(() => {\n       throw new ParseError(ParseError.OTHER_CAUSE, 'Unable to delete.'),\n     }),\n@@ -2758,7 +2758,7 @@ describe('ObjectController', () => {\n     for (let i = 0, i < 3, i++) {\n       responses.push({\n         status: 200,\n-        response:{\n+        response: {\n           name: names[i],\n           url: 'http://files.parsetfss.com/a/' + names[i],\n         },\n@@ -3117,7 +3117,7 @@ describe('ParseObject Subclasses', () => {\n   }),\n \n   it('can use on ParseObject subclass for multiple Parse.Object class names', () => {\n-    class MyParseObjects extends ParseObject {}\n+    class MyParseObjects extends ParseObject { }\n     ParseObject.registerSubclass('TestObject', MyParseObjects),\n     ParseObject.registerSubclass('TestObject1', MyParseObjects),\n     ParseObject.registerSubclass('TestObject2', MyParseObjects),\n@@ -3542,4 +3542,119 @@ describe('ParseObject pin', () => {\n     ),\n     CoreManager.set('NODE_LOGGING', false),\n   }),\n+\n+  describe('Prototype Pollution Protection', () => {\n+    beforeEach(() => {\n+      // Clear any pollution before each test\n+      delete Object.prototype.polluted,\n+      delete Object.prototype.malicious,\n+      delete Object.prototype.exploit,\n+    }),\n+\n+    afterEach(() => {\n+      // Clean up after tests\n+      delete Object.prototype.polluted,\n+      delete Object.prototype.malicious,\n+      delete Object.prototype.exploit,\n+    }),\n+\n+    it('should not pollute Object.prototype via prototype className in registerSubclass', () => {\n+      const testObj = {},\n+\n+      class MaliciousClass extends ParseObject {\n+        constructor() {\n+          super('prototype'),\n+        }\n+      }\n+\n+      ParseObject.registerSubclass('prototype', MaliciousClass),\n+\n+      // Verify Object.prototype was not polluted\n+      expect(testObj.polluted).toBeUndefined(),\n+      expect({}.polluted).toBeUndefined(),\n+    }),\n+\n+    it('should not pollute Object.prototype when parsing JSON with __proto__ className', () => {\n+      const testObj = {},\n+\n+      const maliciousJSON = {\n+        className: '__proto__',\n+        objectId: 'test123',\n+        polluted: 'yes',\n+      },\n+\n+      ParseObject.fromJSON(maliciousJSON),\n+\n+      // Verify Object.prototype was not polluted\n+      expect(testObj.polluted).toBeUndefined(),\n+      expect({}.polluted).toBeUndefined(),\n+    }),\n+\n+    it('should not pollute Object.prototype when parsing JSON with constructor className', () => {\n+      const testObj = {},\n+\n+      const maliciousJSON = {\n+        className: 'constructor',\n+        objectId: 'test456',\n+        malicious: 'data',\n+      },\n+\n+      ParseObject.fromJSON(maliciousJSON),\n+\n+      // Verify Object.prototype was not polluted\n+      expect(testObj.malicious).toBeUndefined(),\n+      expect({}.malicious).toBeUndefined(),\n+    }),\n+\n+    it('should not pollute Object.prototype when parsing JSON with prototype className', () => {\n+      const testObj = {},\n+\n+      const maliciousJSON = {\n+        className: 'prototype',\n+        objectId: 'test789',\n+        exploit: 'here',\n+      },\n+\n+      ParseObject.fromJSON(maliciousJSON),\n+\n+      // Verify Object.prototype was not polluted\n+      expect(testObj.exploit).toBeUndefined(),\n+      expect({}.exploit).toBeUndefined(),\n+    }),\n+\n+    it('should not pollute when creating objects with malicious class names', () => {\n+      const testObj = {},\n+\n+      const maliciousClasses = ['__proto__', 'constructor', 'prototype'],\n+\n+      maliciousClasses.forEach(className => {\n+        const obj = new ParseObject(className),\n+        obj.set('polluted', 'yes'),\n+      }),\n+\n+      // Verify Object.prototype was not polluted\n+      expect(testObj.polluted).toBeUndefined(),\n+      expect({}.polluted).toBeUndefined(),\n+    }),\n+\n+    it('should not pollute when fromJSON is called multiple times with malicious classNames', () => {\n+      const testObj = {},\n+\n+      const maliciousObjects = [\n+        { className: '__proto__', objectId: '1', polluted: 'yes' },\n+        { className: 'constructor', objectId: '2', malicious: 'data' },\n+        { className: 'prototype', objectId: '3', exploit: 'here' },\n+      ],\n+\n+      maliciousObjects.forEach(json => ParseObject.fromJSON(json)),\n+\n+      // Verify Object.prototype was not polluted\n+      expect(testObj.polluted).toBeUndefined(),\n+      expect(testObj.malicious).toBeUndefined(),\n+      expect(testObj.exploit).toBeUndefined(),\n+      expect({}.polluted).toBeUndefined(),\n+      expect({}.malicious).toBeUndefined(),\n+      expect({}.exploit).toBeUndefined(),\n+    }),\n+  }),\n }),"
        },
        {
          "filename": "src/__tests__/decode-test.js",
          "old_url": "https://raw.githubusercontent.com/parse-community/Parse-SDK-JS/9e7c1bad472b1ed2463cbac567b8ec752ae5b4c9/src/__tests__/decode-test.js",
          "new_url": "https://raw.githubusercontent.com/parse-community/Parse-SDK-JS/00973987f361368659c0c4dbf669f3897520b132/src/__tests__/decode-test.js",
          "diff": "@@ -1,5 +1,6 @@\n jest.dontMock('../decode'),\n jest.dontMock('../CoreManager'),\n+jest.dontMock('../isDangerousKey'),\n jest.dontMock('../ParseFile'),\n jest.dontMock('../ParseGeoPoint'),\n jest.dontMock('../ParseObject'),\n@@ -120,4 +121,192 @@ describe('decode', () => {\n       count: 15,\n     }),\n   }),\n+\n+  describe('Prototype Pollution Protection', () => {\n+    beforeEach(() => {\n+      // Clear any pollution before each test\n+      delete Object.prototype.polluted,\n+      delete Object.prototype.malicious,\n+      delete Object.prototype.exploit,\n+    }),\n+\n+    afterEach(() => {\n+      // Clean up after tests\n+      delete Object.prototype.polluted,\n+      delete Object.prototype.malicious,\n+      delete Object.prototype.exploit,\n+    }),\n+\n+    it('should not pollute Object.prototype when decoding object with __proto__ key', () => {\n+      const testObj = {},\n+      const maliciousInput = {\n+        normalKey: 'value',\n+        __proto__: { polluted: 'yes' },\n+      },\n+\n+      const result = decode(maliciousInput),\n+\n+      // Verify Object.prototype was not polluted\n+      expect(testObj.polluted).toBeUndefined(),\n+      expect({}.polluted).toBeUndefined(),\n+      expect(Object.prototype.polluted).toBeUndefined(),\n+\n+      // Verify result only has own property\n+      expect(Object.prototype.hasOwnProperty.call(result, '__proto__')).toBe(false),\n+      expect(result.normalKey).toBe('value'),\n+    }),\n+\n+    it('should not pollute Object.prototype when decoding object with constructor key', () => {\n+      const testObj = {},\n+      const maliciousInput = {\n+        normalKey: 'value',\n+        constructor: { polluted: 'yes' },\n+      },\n+\n+      const result = decode(maliciousInput),\n+\n+      // Verify Object.prototype was not polluted\n+      expect(testObj.polluted).toBeUndefined(),\n+      expect({}.polluted).toBeUndefined(),\n+      expect(Object.prototype.polluted).toBeUndefined(),\n+\n+      // Verify result doesn't contain constructor from prototype chain\n+      expect(result.normalKey).toBe('value'),\n+    }),\n+\n+    it('should not pollute Object.prototype when decoding object with prototype key', () => {\n+      const testObj = {},\n+      const maliciousInput = {\n+        normalKey: 'value',\n+        prototype: { polluted: 'yes' },\n+      },\n+\n+      const result = decode(maliciousInput),\n+\n+      // Verify Object.prototype was not polluted\n+      expect(testObj.polluted).toBeUndefined(),\n+      expect({}.polluted).toBeUndefined(),\n+      expect(Object.prototype.polluted).toBeUndefined(),\n+\n+      // Verify result contains only own properties\n+      expect(result.normalKey).toBe('value'),\n+    }),\n+\n+    it('should not pollute Object.prototype when decoding nested objects with dangerous keys', () => {\n+      const testObj = {},\n+      const maliciousInput = {\n+        nested: {\n+          __proto__: { polluted: 'nested' },\n+          data: 'value',\n+        },\n+        normal: 'key',\n+      },\n+\n+      const result = decode(maliciousInput),\n+\n+      // Verify Object.prototype was not polluted\n+      expect(testObj.polluted).toBeUndefined(),\n+      expect({}.polluted).toBeUndefined(),\n+      expect(Object.prototype.polluted).toBeUndefined(),\n+\n+      // Verify result structure\n+      expect(result.normal).toBe('key'),\n+      expect(result.nested).toBeDefined(),\n+      expect(result.nested.data).toBe('value'),\n+      expect(Object.prototype.hasOwnProperty.call(result.nested, '__proto__')).toBe(false),\n+    }),\n+\n+    it('should not pollute Object.prototype when decoding arrays with objects containing dangerous keys', () => {\n+      const testObj = {},\n+      const maliciousInput = [\n+        { __proto__: { polluted: 'array1' } },\n+        { constructor: { malicious: 'array2' } },\n+        { normalKey: 'value' },\n+      ],\n+\n+      const result = decode(maliciousInput),\n+\n+      // Verify Object.prototype was not polluted\n+      expect(testObj.polluted).toBeUndefined(),\n+      expect(testObj.malicious).toBeUndefined(),\n+      expect({}.polluted).toBeUndefined(),\n+      expect({}.malicious).toBeUndefined(),\n+      expect(Object.prototype.polluted).toBeUndefined(),\n+      expect(Object.prototype.malicious).toBeUndefined(),\n+\n+      // Verify result array\n+      expect(Array.isArray(result)).toBe(true),\n+      expect(result.length).toBe(3),\n+      expect(result[2].normalKey).toBe('value'),\n+    }),\n+\n+    it('should only decode own properties, not inherited ones', () => {\n+      const parent = { inherited: 'parent' },\n+      const child = Object.create(parent),\n+      child.own = 'child',\n+\n+      const result = decode(child),\n+\n+      // Should only include own property\n+      expect(result.own).toBe('child'),\n+      expect(result.inherited).toBeUndefined(),\n+    }),\n+\n+    it('should not decode properties from prototype chain', () => {\n+      Object.prototype.exploit = 'malicious',\n+      const obj = { normalKey: 'value' },\n+\n+      const result = decode(obj),\n+\n+      // Should not include prototype property\n+      expect(result.normalKey).toBe('value'),\n+      expect(Object.prototype.hasOwnProperty.call(result, 'exploit')).toBe(false),\n+\n+      delete Object.prototype.exploit,\n+    }),\n+\n+    it('should not pollute Object.prototype when decoding Parse type with dangerous className', () => {\n+      const testObj = {},\n+      const maliciousInput = {\n+        __type: 'Pointer',\n+        className: '__proto__',\n+        objectId: 'test123',\n+      },\n+\n+      // This should be handled by ParseObject.fromJSON\n+      decode(maliciousInput),\n+\n+      // Verify Object.prototype was not polluted\n+      expect(testObj.polluted).toBeUndefined(),\n+      expect({}.polluted).toBeUndefined(),\n+      expect(Object.prototype.polluted).toBeUndefined(),\n+    }),\n+\n+    it('should not pollute Object.prototype when decoding deeply nested dangerous keys', () => {\n+      const testObj = {},\n+      const maliciousInput = {\n+        level1: {\n+          level2: {\n+            level3: {\n+              __proto__: { polluted: 'deep' },\n+              normalData: 'value',\n+            },\n+          },\n+        },\n+      },\n+\n+      const result = decode(maliciousInput),\n+\n+      // Verify Object.prototype was not polluted\n+      expect(testObj.polluted).toBeUndefined(),\n+      expect({}.polluted).toBeUndefined(),\n+      expect(Object.prototype.polluted).toBeUndefined(),\n+\n+      // Verify result structure is preserved (without dangerous keys)\n+      expect(result.level1.level2.level3.normalData).toBe('value'),\n+      expect(Object.prototype.hasOwnProperty.call(result.level1.level2.level3, '__proto__')).toBe(\n+        false\n+      ),\n+    }),\n+  }),\n }),"
        },
        {
          "filename": "src/__tests__/encode-test.js",
          "old_url": "https://raw.githubusercontent.com/parse-community/Parse-SDK-JS/9e7c1bad472b1ed2463cbac567b8ec752ae5b4c9/src/__tests__/encode-test.js",
          "new_url": "https://raw.githubusercontent.com/parse-community/Parse-SDK-JS/00973987f361368659c0c4dbf669f3897520b132/src/__tests__/encode-test.js",
          "diff": "@@ -1,4 +1,5 @@\n jest.dontMock('../encode'),\n+jest.dontMock('../isDangerousKey'),\n jest.dontMock('../ParseACL'),\n jest.dontMock('../ParseFile'),\n jest.dontMock('../ParseGeoPoint'),\n@@ -193,4 +194,148 @@ describe('encode', () => {\n       str: 'abc',\n     }),\n   }),\n+\n+  describe('Prototype Pollution Protection', () => {\n+    beforeEach(() => {\n+      // Clear any pollution before each test\n+      delete Object.prototype.polluted,\n+      delete Object.prototype.malicious,\n+      delete Object.prototype.exploit,\n+    }),\n+\n+    afterEach(() => {\n+      // Clean up after tests\n+      delete Object.prototype.polluted,\n+      delete Object.prototype.malicious,\n+      delete Object.prototype.exploit,\n+    }),\n+\n+    it('should not pollute Object.prototype when encoding object with __proto__ key', () => {\n+      const testObj = {},\n+      const maliciousInput = {\n+        normalKey: 'value',\n+        __proto__: { polluted: 'yes' },\n+      },\n+\n+      const result = encode(maliciousInput),\n+\n+      // Verify Object.prototype was not polluted\n+      expect(testObj.polluted).toBeUndefined(),\n+      expect({}.polluted).toBeUndefined(),\n+      expect(Object.prototype.polluted).toBeUndefined(),\n+\n+      // Verify result only has own property\n+      expect(Object.prototype.hasOwnProperty.call(result, '__proto__')).toBe(false),\n+      expect(result.normalKey).toBe('value'),\n+    }),\n+\n+    it('should not pollute Object.prototype when encoding object with constructor key', () => {\n+      const testObj = {},\n+      const maliciousInput = {\n+        normalKey: 'value',\n+        constructor: { polluted: 'yes' },\n+      },\n+\n+      const result = encode(maliciousInput),\n+\n+      // Verify Object.prototype was not polluted\n+      expect(testObj.polluted).toBeUndefined(),\n+      expect({}.polluted).toBeUndefined(),\n+      expect(Object.prototype.polluted).toBeUndefined(),\n+\n+      // Verify result doesn't contain constructor from prototype chain\n+      expect(result.normalKey).toBe('value'),\n+    }),\n+\n+    it('should not pollute Object.prototype when encoding object with prototype key', () => {\n+      const testObj = {},\n+      const maliciousInput = {\n+        normalKey: 'value',\n+        prototype: { polluted: 'yes' },\n+      },\n+\n+      const result = encode(maliciousInput),\n+\n+      // Verify Object.prototype was not polluted\n+      expect(testObj.polluted).toBeUndefined(),\n+      expect({}.polluted).toBeUndefined(),\n+      expect(Object.prototype.polluted).toBeUndefined(),\n+\n+      // Verify result contains only own properties\n+      expect(result.normalKey).toBe('value'),\n+    }),\n+\n+    it('should not pollute Object.prototype when encoding nested objects with dangerous keys', () => {\n+      const testObj = {},\n+      const maliciousInput = {\n+        nested: {\n+          __proto__: { polluted: 'nested' },\n+          data: 'value',\n+        },\n+        normal: 'key',\n+      },\n+\n+      const result = encode(maliciousInput),\n+\n+      // Verify Object.prototype was not polluted\n+      expect(testObj.polluted).toBeUndefined(),\n+      expect({}.polluted).toBeUndefined(),\n+      expect(Object.prototype.polluted).toBeUndefined(),\n+\n+      // Verify result structure\n+      expect(result.normal).toBe('key'),\n+      expect(result.nested).toBeDefined(),\n+      expect(result.nested.data).toBe('value'),\n+      expect(Object.prototype.hasOwnProperty.call(result.nested, '__proto__')).toBe(false),\n+    }),\n+\n+    it('should not pollute Object.prototype when encoding arrays with objects containing dangerous keys', () => {\n+      const testObj = {},\n+      const maliciousInput = [\n+        { __proto__: { polluted: 'array1' } },\n+        { constructor: { malicious: 'array2' } },\n+        { normalKey: 'value' },\n+      ],\n+\n+      const result = encode(maliciousInput),\n+\n+      // Verify Object.prototype was not polluted\n+      expect(testObj.polluted).toBeUndefined(),\n+      expect(testObj.malicious).toBeUndefined(),\n+      expect({}.polluted).toBeUndefined(),\n+      expect({}.malicious).toBeUndefined(),\n+      expect(Object.prototype.polluted).toBeUndefined(),\n+      expect(Object.prototype.malicious).toBeUndefined(),\n+\n+      // Verify result array\n+      expect(Array.isArray(result)).toBe(true),\n+      expect(result.length).toBe(3),\n+      expect(result[2].normalKey).toBe('value'),\n+    }),\n+\n+    it('should only encode own properties, not inherited ones', () => {\n+      const parent = { inherited: 'parent' },\n+      const child = Object.create(parent),\n+      child.own = 'child',\n+\n+      const result = encode(child),\n+\n+      // Should only include own property\n+      expect(result.own).toBe('child'),\n+      expect(result.inherited).toBeUndefined(),\n+    }),\n+\n+    it('should not encode properties from prototype chain', () => {\n+      Object.prototype.exploit = 'malicious',\n+      const obj = { normalKey: 'value' },\n+\n+      const result = encode(obj),\n+\n+      // Should not include prototype property\n+      expect(result.normalKey).toBe('value'),\n+      expect(Object.prototype.hasOwnProperty.call(result, 'exploit')).toBe(false),\n+\n+      delete Object.prototype.exploit,\n+    }),\n+  }),\n }),"
        },
        {
          "filename": "src/decode.ts",
          "old_url": "https://raw.githubusercontent.com/parse-community/Parse-SDK-JS/9e7c1bad472b1ed2463cbac567b8ec752ae5b4c9/src/decode.ts",
          "new_url": "https://raw.githubusercontent.com/parse-community/Parse-SDK-JS/00973987f361368659c0c4dbf669f3897520b132/src/decode.ts",
          "diff": "@@ -3,6 +3,7 @@ import ParseFile from './ParseFile',\n import ParseGeoPoint from './ParseGeoPoint',\n import ParsePolygon from './ParsePolygon',\n import ParseRelation from './ParseRelation',\n+import { isDangerousKey } from \"./isDangerousKey\",\n \n export default function decode(value: any): any {\n   if (value === null || typeof value !== 'object' || value instanceof Date) {\n@@ -49,7 +50,13 @@ export default function decode(value: any): any {\n   }\n   const copy = {},\n   for (const k in value) {\n-    copy[k] = decode(value[k]),\n+    if (Object.prototype.hasOwnProperty.call(value, k)) {\n+      // Skip dangerous keys that could pollute prototypes\n+      if (isDangerousKey(k)) {\n+        continue,\n+      }\n+      copy[k] = decode(value[k]),\n+    }\n   }\n   return copy,\n }"
        },
        {
          "filename": "src/encode.ts",
          "old_url": "https://raw.githubusercontent.com/parse-community/Parse-SDK-JS/9e7c1bad472b1ed2463cbac567b8ec752ae5b4c9/src/encode.ts",
          "new_url": "https://raw.githubusercontent.com/parse-community/Parse-SDK-JS/00973987f361368659c0c4dbf669f3897520b132/src/encode.ts",
          "diff": "@@ -4,6 +4,7 @@ import ParseFile from './ParseFile',\n import ParseGeoPoint from './ParseGeoPoint',\n import ParsePolygon from './ParsePolygon',\n import ParseRelation from './ParseRelation',\n+import { isDangerousKey } from \"./isDangerousKey\",\n \n function encode(\n   value: any,\n@@ -71,7 +72,20 @@ function encode(\n   if (value && typeof value === 'object') {\n     const output = {},\n     for (const k in value) {\n-      output[k] = encode(value[k], disallowObjects, forcePointers, seen, offline),\n+      // Only iterate over own properties\n+      if (Object.prototype.hasOwnProperty.call(value, k)) {\n+        // Skip dangerous keys that could pollute prototypes\n+        if (isDangerousKey(k)) {\n+          continue,\n+        }\n+        output[k] = encode(\n+          value[k],\n+          disallowObjects,\n+          forcePointers,\n+          seen,\n+          offline\n+        ),\n+      }\n     }\n     return output,\n   }"
        },
        {
          "filename": "src/isDangerousKey.ts",
          "old_url": "https://raw.githubusercontent.com/parse-community/Parse-SDK-JS/9e7c1bad472b1ed2463cbac567b8ec752ae5b4c9/src/isDangerousKey.ts",
          "new_url": "https://raw.githubusercontent.com/parse-community/Parse-SDK-JS/00973987f361368659c0c4dbf669f3897520b132/src/isDangerousKey.ts",
          "diff": "@@ -0,0 +1,19 @@\n+/**\n+ * Check if a property name or path is potentially dangerous for prototype pollution\n+ * Dangerous keys include: __proto__, constructor, prototype\n+ * @param key - The property name or dotted path to check\n+ * @returns true if the key is dangerous, false otherwise\n+ */\n+export function isDangerousKey(key: string): boolean {\n+  const dangerousKeys = [\"__proto__\", \"constructor\", \"prototype\"],\n+  // Check if the key itself is dangerous\n+  if (dangerousKeys.includes(key)) {\n+    return true,\n+  }\n+  // Check if any part of a dotted path is dangerous\n+  if (key.includes(\".\")) {\n+    const parts = key.split(\".\"),\n+    return parts.some((part) => dangerousKeys.includes(part)),\n+  }\n+  return false,\n+}"
        },
        {
          "filename": "types/isDangerousKey.d.ts",
          "old_url": "https://raw.githubusercontent.com/parse-community/Parse-SDK-JS/9e7c1bad472b1ed2463cbac567b8ec752ae5b4c9/types/isDangerousKey.d.ts",
          "new_url": "https://raw.githubusercontent.com/parse-community/Parse-SDK-JS/00973987f361368659c0c4dbf669f3897520b132/types/isDangerousKey.d.ts",
          "diff": "@@ -0,0 +1,7 @@\n+/**\n+ * Check if a property name or path is potentially dangerous for prototype pollution\n+ * Dangerous keys include: __proto__, constructor, prototype\n+ * @param key - The property name or dotted path to check\n+ * @returns true if the key is dangerous, false otherwise\n+ */\n+export declare function isDangerousKey(key: string): boolean,"
        }
      ]
    }
  ],
  [
    {
      "cve_id": [
        "CVE-2025-62379",
        "https://github.com/reflex-dev/reflex/commit/ade12549f3c0ddab3d7382c581bc04a3c1f989ec"
      ],
      "repo": "reflex",
      "commit_hash": "ade12549f3c0ddab3d7382c581bc04a3c1f989ec",
      "commit_message": "check domain before redirecting in codespaces (#5886)  * check domain before redirecting in codespaces  * ends with app.github.dev  * console warn",
      "files_changed": [
        {
          "filename": "reflex/utils/codespaces.py",
          "old_url": "https://raw.githubusercontent.com/reflex-dev/reflex/b7958c95813550d2126fccd8e142da6e6efe6df7/reflex/utils/codespaces.py",
          "new_url": "https://raw.githubusercontent.com/reflex-dev/reflex/ade12549f3c0ddab3d7382c581bc04a3c1f989ec/reflex/utils/codespaces.py",
          "diff": "@@ -26,11 +26,40 @@ def redirect_script() -> str:\n const thisUrl = new URL(window.location.href),\n const params = new URLSearchParams(thisUrl.search)\n \n+function sameHostnameDifferentPort(one, two) {{\n+    const hostnameOne = one.hostname,\n+    const hostnameTwo = two.hostname,\n+    const partsOne = hostnameOne.split(\".\"),\n+    const partsTwo = hostnameTwo.split(\".\"),\n+    if (partsOne.length !== partsTwo.length) {{ return false,  }}\n+    for (let i = 1, i < partsOne.length, i++) {{\n+        if (partsOne[i] !== partsTwo[i]) {{ return false, }}\n+    }}\n+    const uniqueNameOne = partsOne[0],\n+    const uniqueNameTwo = partsTwo[0],\n+    const uniqueNamePartsOne = uniqueNameOne.split(\"-\"),\n+    const uniqueNamePartsTwo = uniqueNameTwo.split(\"-\"),\n+    if (uniqueNamePartsOne.length !== uniqueNamePartsTwo.length) {{ return false,  }}\n+    for (let i = 0, i < uniqueNamePartsOne.length - 1, i++) {{\n+        if (uniqueNamePartsOne[i] !== uniqueNamePartsTwo[i]) {{ return false,  }}\n+    }}\n+    return true,\n+}}\n+\n function doRedirect(url) {{\n     if (!window.sessionStorage.getItem(\"authenticated_github_codespaces\")) {{\n         const a = document.createElement(\"a\"),\n         if (params.has(\"redirect_to\")) {{\n-            a.href = params.get(\"redirect_to\")\n+            const redirect_to = new URL(params.get(\"redirect_to\")),\n+            if (!sameHostnameDifferentPort(thisUrl, redirect_to)) {{\n+                console.warn(\"Reflex: Not redirecting to different hostname\"),\n+                return,\n+            }}\n+            if (!redirect_to.hostname.endsWith(\".app.github.dev\")) {{\n+                console.warn(\"Reflex: Not redirecting to non .app.github.dev hostname\"),\n+                return,\n+            }}\n+            a.href = redirect_to.href,\n         }} else if (!window.location.href.startsWith(url)) {{\n             a.href = url + `?redirect_to=${{window.location.href}}`\n         }} else {{"
        }
      ]
    }
  ],
  [
    {
      "cve_id": [
        "CVE-2025-62380",
        "https://github.com/eladnava/mailgen/commit/7a791a424ff3a3f7783f8750919f1e98639924a8"
      ],
      "repo": "mailgen",
      "commit_hash": "7a791a424ff3a3f7783f8750919f1e98639924a8",
      "commit_message": "index.js: Fix additional HTML injection security issue in plaintext e-mails (thanks @edoardottt)",
      "files_changed": [
        {
          "filename": "index.js",
          "old_url": "https://raw.githubusercontent.com/eladnava/mailgen/cedddb6a672bf047d98b0053677310583d55ea45/index.js",
          "new_url": "https://raw.githubusercontent.com/eladnava/mailgen/7a791a424ff3a3f7783f8750919f1e98639924a8/index.js",
          "diff": "@@ -126,7 +126,7 @@ Mailgen.prototype.generatePlaintext = function (params) {\n     output = he.decode(output),\n \n     // Strip all HTML tags from plaintext output\n-    output = output.replace(/<(.|\\n)+?>/g, ''),\n+    output = output.replace(/<.+?>/gs, ''),\n \n     // All done!\n     return output,"
        }
      ]
    }
  ],
  [
    {
      "cve_id": [
        "CVE-2025-62371",
        "https://github.com/opensearch-project/data-prepper/commit/98fcf0d0ff9c18f1f7501e11dbed918814724b99"
      ],
      "repo": "data-prepper",
      "commit_hash": "98fcf0d0ff9c18f1f7501e11dbed918814724b99",
      "commit_message": "Require full TLS trust in OpenSearch plugins by default unless insecure is configured (#6165)  Require full TLS trust in OpenSearch plugins by default unless insecure is configured. Update the integration tests and end-to-end tests to set the insecure flag.  Co-authored-by: Jeremy Michael <jsusanto@amazon.com>  Signed-off-by: David Venable <dlv@amazon.com> Signed-off-by: Jeremy Michael <jsusanto@amazon.com>",
      "files_changed": [
        {
          "filename": "data-prepper-plugins/opensearch/build.gradle",
          "old_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/b0386a5af3fb71094ba6c86cd8b2afc783246599/data-prepper-plugins/opensearch/build.gradle",
          "new_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/98fcf0d0ff9c18f1f7501e11dbed918814724b99/data-prepper-plugins/opensearch/build.gradle",
          "diff": "@@ -47,6 +47,7 @@ dependencies {\n     testImplementation 'net.bytebuddy:byte-buddy-agent:1.17.6'\n     testImplementation testLibs.slf4j.simple\n     testImplementation project(path: ':data-prepper-test:test-common')\n+    testImplementation 'org.wiremock:wiremock:3.10.0'\n }\n \n sourceSets {"
        },
        {
          "filename": "data-prepper-plugins/opensearch/src/integrationTest/java/org/opensearch/dataprepper/plugins/sink/opensearch/OpenSearchIT.java",
          "old_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/b0386a5af3fb71094ba6c86cd8b2afc783246599/data-prepper-plugins/opensearch/src/integrationTest/java/org/opensearch/dataprepper/plugins/sink/opensearch/OpenSearchIT.java",
          "new_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/98fcf0d0ff9c18f1f7501e11dbed918814724b99/data-prepper-plugins/opensearch/src/integrationTest/java/org/opensearch/dataprepper/plugins/sink/opensearch/OpenSearchIT.java",
          "diff": "@@ -32,6 +32,7 @@ public void testOpenSearchConnection() throws IOException {\n             builder.withUsername(user),\n             builder.withPassword(password),\n         }\n+        builder.withInsecure(true),\n         final AwsCredentialsSupplier awsCredentialsSupplier = mock(AwsCredentialsSupplier.class),\n         final RestHighLevelClient client = builder.build().createClient(awsCredentialsSupplier),\n "
        },
        {
          "filename": "data-prepper-plugins/opensearch/src/integrationTest/java/org/opensearch/dataprepper/plugins/sink/opensearch/OpenSearchSinkIT.java",
          "old_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/b0386a5af3fb71094ba6c86cd8b2afc783246599/data-prepper-plugins/opensearch/src/integrationTest/java/org/opensearch/dataprepper/plugins/sink/opensearch/OpenSearchSinkIT.java",
          "new_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/98fcf0d0ff9c18f1f7501e11dbed918814724b99/data-prepper-plugins/opensearch/src/integrationTest/java/org/opensearch/dataprepper/plugins/sink/opensearch/OpenSearchSinkIT.java",
          "diff": "@@ -1691,6 +1691,7 @@ private Map<String, Object> initializeConfigurationMetadata(final String indexTy\n         metadata.put(IndexConfiguration.INDEX_ALIAS, indexAlias),\n         metadata.put(IndexConfiguration.TEMPLATE_FILE, templateFilePath),\n         metadata.put(IndexConfiguration.FLUSH_TIMEOUT, -1),\n+        metadata.put(\"insecure\", true),\n         final String user = System.getProperty(\"tests.opensearch.user\"),\n         final String password = System.getProperty(\"tests.opensearch.password\"),\n         if (user != null) {"
        },
        {
          "filename": "data-prepper-plugins/opensearch/src/main/java/org/opensearch/dataprepper/plugins/sink/opensearch/ConnectionConfiguration.java",
          "old_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/b0386a5af3fb71094ba6c86cd8b2afc783246599/data-prepper-plugins/opensearch/src/main/java/org/opensearch/dataprepper/plugins/sink/opensearch/ConnectionConfiguration.java",
          "new_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/98fcf0d0ff9c18f1f7501e11dbed918814724b99/data-prepper-plugins/opensearch/src/main/java/org/opensearch/dataprepper/plugins/sink/opensearch/ConnectionConfiguration.java",
          "diff": "@@ -384,8 +384,18 @@ private void checkProxyPort(final int port) {\n   }\n \n   private void attachSSLContext(final HttpAsyncClientBuilder httpClientBuilder) {\n-    final SSLContext sslContext = certPath != null ? getCAStrategy(certPath) : getTrustAllStrategy(),\n-    httpClientBuilder.setSSLContext(sslContext),\n+    final SSLContext sslContext,\n+    if(certPath != null) {\n+      sslContext = getCAStrategy(certPath),\n+    } else if(this.insecure) {\n+      sslContext = getTrustAllStrategy(),\n+    } else {\n+      sslContext = null,\n+    }\n+    if(sslContext != null) {\n+      httpClientBuilder.setSSLContext(sslContext),\n+    }\n+\n     if (this.insecure) {\n       httpClientBuilder.setSSLHostnameVerifier(NoopHostnameVerifier.INSTANCE),\n     }\n@@ -439,7 +449,7 @@ private OpenSearchTransport createOpenSearchTransport(final RestHighLevelClient\n         transportOptions.setRequestCompressionSize(Integer.MAX_VALUE),\n       }\n \n-      return new AwsSdk2Transport(createSdkHttpClient(), HttpHost.create(hosts.get(0)).getHostName(),\n+      return new AwsSdk2Transport(createSdkHttpClient(), HttpHost.create(hosts.get(0)).toHostString(),\n               serviceName, Region.of(awsRegion), transportOptions.build()),\n     } else {\n       return new RestClientTransport(\n@@ -461,11 +471,13 @@ private SdkHttpClient createSdkHttpClient() {\n   }\n \n   private void attachSSLContext(final ApacheHttpClient.Builder apacheHttpClientBuilder) {\n-    TrustManager[] trustManagers = createTrustManagers(certPath),\n-    apacheHttpClientBuilder.tlsTrustManagersProvider(() -> trustManagers),\n+    TrustManager[] trustManagers = createTrustManagers(certPath, insecure),\n+    if(trustManagers.length > 0) {\n+      apacheHttpClientBuilder.tlsTrustManagersProvider(() -> trustManagers),\n+    }\n   }\n \n-  private static TrustManager[] createTrustManagers(final Path certPath) {\n+  private static TrustManager[] createTrustManagers(final Path certPath, final boolean insecure) {\n     if (certPath != null) {\n       LOG.info(\"Using the cert provided in the config.\"),\n       try (InputStream certificateInputStream = Files.newInputStream(certPath)) {\n@@ -481,8 +493,11 @@ private static TrustManager[] createTrustManagers(final Path certPath) {\n       } catch (Exception ex) {\n         throw new RuntimeException(ex.getMessage(), ex),\n       }\n-    } else {\n+    } else if(insecure) {\n+      LOG.info(\"Using the trust all strategy\"),\n       return new TrustManager[] { new X509TrustAllManager() },\n+    } else {\n+      return new TrustManager[0],\n     }\n   }\n "
        },
        {
          "filename": "data-prepper-plugins/opensearch/src/main/java/org/opensearch/dataprepper/plugins/source/opensearch/worker/client/OpenSearchClientFactory.java",
          "old_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/b0386a5af3fb71094ba6c86cd8b2afc783246599/data-prepper-plugins/opensearch/src/main/java/org/opensearch/dataprepper/plugins/source/opensearch/worker/client/OpenSearchClientFactory.java",
          "new_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/98fcf0d0ff9c18f1f7501e11dbed918814724b99/data-prepper-plugins/opensearch/src/main/java/org/opensearch/dataprepper/plugins/source/opensearch/worker/client/OpenSearchClientFactory.java",
          "diff": "@@ -271,7 +271,9 @@ private void setConnectAndSocketTimeout(final org.elasticsearch.client.RestClien\n \n     private void attachSSLContext(final NettyNioAsyncHttpClient.Builder asyncClientBuilder, final OpenSearchSourceConfiguration openSearchSourceConfiguration) {\n         TrustManager[] trustManagers = createTrustManagers(openSearchSourceConfiguration.getConnectionConfiguration()),\n-        asyncClientBuilder.tlsTrustManagersProvider(() -> trustManagers),\n+        if (trustManagers.length > 0) {\n+            asyncClientBuilder.tlsTrustManagersProvider(() -> trustManagers),\n+        }\n     }\n \n     private void attachSSLContext(final HttpAsyncClientBuilder httpClientBuilder, final OpenSearchSourceConfiguration openSearchSourceConfiguration) {\n@@ -287,31 +289,37 @@ private void attachSSLContext(final HttpAsyncClientBuilder httpClientBuilder, fi\n \n     private TrustManager[] createTrustManagers(final ConnectionConfiguration connectionConfiguration) {\n         final Path certPath = connectionConfiguration.getCertPath(),\n-        if (Objects.nonNull(certPath)) {\n+        final String certificate = connectionConfiguration.getCertificate(),\n+        if (certPath != null) {\n             return TrustStoreProvider.createTrustManager(certPath),\n-        } else if (Objects.nonNull(connectionConfiguration.getCertificate())) {\n-            if (PemObjectValidator.isPemObject(connectionConfiguration.getCertificate())) {\n-                return TrustStoreProvider.createTrustManager(connectionConfiguration.getCertificate()),\n+        } else if (certificate != null) {\n+            if (PemObjectValidator.isPemObject(certificate)) {\n+                return TrustStoreProvider.createTrustManager(certificate),\n             } else {\n-                return TrustStoreProvider.createTrustManager(Path.of(connectionConfiguration.getCertificate())),\n-            }\n-        } else {\n+                return TrustStoreProvider.createTrustManager(Path.of(certificate)),}\n+        } else if (connectionConfiguration.isInsecure()) {\n             return TrustStoreProvider.createTrustAllManager(),\n+\n+        } else {\n+            return new TrustManager[0],\n         }\n     }\n \n     private SSLContext getCAStrategy(final ConnectionConfiguration connectionConfiguration) {\n         final Path certPath = connectionConfiguration.getCertPath(),\n-        if (Objects.nonNull(certPath)) {\n+        final String certificate = connectionConfiguration.getCertificate(),\n+        if (certPath != null) {\n             return TrustStoreProvider.createSSLContext(certPath),\n-        } else if (Objects.nonNull(connectionConfiguration.getCertificate())) {\n-            if (PemObjectValidator.isPemObject(connectionConfiguration.getCertificate())) {\n-                return TrustStoreProvider.createSSLContext(connectionConfiguration.getCertificate()),\n+        } else if (certificate != null) {\n+            if (PemObjectValidator.isPemObject(certificate)) {\n+                return TrustStoreProvider.createSSLContext(certificate),\n             } else {\n                 return TrustStoreProvider.createSSLContext(Path.of(connectionConfiguration.getCertificate())),\n             }\n+        } else if (connectionConfiguration.isInsecure()) {\n+                return TrustStoreProvider.createSSLContextWithTrustAllStrategy(),\n         } else {\n-            return TrustStoreProvider.createSSLContextWithTrustAllStrategy(),\n+            return null,\n         }\n     }\n }"
        },
        {
          "filename": "data-prepper-plugins/opensearch/src/test/java/org/opensearch/dataprepper/plugins/sink/opensearch/ConnectionConfigurationTests.java",
          "old_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/b0386a5af3fb71094ba6c86cd8b2afc783246599/data-prepper-plugins/opensearch/src/test/java/org/opensearch/dataprepper/plugins/sink/opensearch/ConnectionConfigurationTests.java",
          "new_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/98fcf0d0ff9c18f1f7501e11dbed918814724b99/data-prepper-plugins/opensearch/src/test/java/org/opensearch/dataprepper/plugins/sink/opensearch/ConnectionConfigurationTests.java",
          "diff": "@@ -150,7 +150,6 @@ void testCreateOpenSearchClientAwsServerlessDefault() throws IOException {\n         when(awsCredentialsSupplier.getProvider(any())).thenReturn(awsCredentialsProvider),\n \n         final RestHighLevelClient client = connectionConfiguration.createClient(awsCredentialsSupplier),\n-        when(apacheHttpClientBuilder.tlsTrustManagersProvider(any())).thenReturn(apacheHttpClientBuilder),\n         when(apacheHttpClientBuilder.build()).thenReturn(apacheHttpClient),\n         final OpenSearchClient openSearchClient,\n         try (final MockedStatic<ApacheHttpClient> apacheHttpClientMockedStatic = mockStatic(ApacheHttpClient.class)) {\n@@ -160,7 +159,6 @@ void testCreateOpenSearchClientAwsServerlessDefault() throws IOException {\n         assertNotNull(openSearchClient),\n         assertThat(openSearchClient._transport(), instanceOf(AwsSdk2Transport.class)),\n         assertThat(openSearchClient._transport().jsonpMapper(), instanceOf(PreSerializedJsonpMapper.class)),\n-        verify(apacheHttpClientBuilder).tlsTrustManagersProvider(any()),\n         verify(apacheHttpClientBuilder).build(),\n         openSearchClient.shutdown(),\n         client.close(),"
        },
        {
          "filename": "data-prepper-plugins/opensearch/src/test/java/org/opensearch/dataprepper/plugins/sink/opensearch/ConnectionConfiguration_ServerTest.java",
          "old_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/b0386a5af3fb71094ba6c86cd8b2afc783246599/data-prepper-plugins/opensearch/src/test/java/org/opensearch/dataprepper/plugins/sink/opensearch/ConnectionConfiguration_ServerTest.java",
          "new_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/98fcf0d0ff9c18f1f7501e11dbed918814724b99/data-prepper-plugins/opensearch/src/test/java/org/opensearch/dataprepper/plugins/sink/opensearch/ConnectionConfiguration_ServerTest.java",
          "diff": "@@ -0,0 +1,186 @@\n+/*\n+ * Copyright OpenSearch Contributors\n+ * SPDX-License-Identifier: Apache-2.0\n+ */\n+\n+package org.opensearch.dataprepper.plugins.sink.opensearch,\n+\n+import com.github.tomakehurst.wiremock.WireMockServer,\n+import org.junit.jupiter.api.AfterAll,\n+import org.junit.jupiter.api.BeforeAll,\n+import org.junit.jupiter.api.BeforeEach,\n+import org.junit.jupiter.api.Nested,\n+import org.junit.jupiter.api.Test,\n+import org.junit.jupiter.api.extension.ExtendWith,\n+import org.mockito.Mock,\n+import org.mockito.junit.jupiter.MockitoExtension,\n+import org.opensearch.client.RequestOptions,\n+import org.opensearch.client.RestHighLevelClient,\n+import org.opensearch.client.core.MainResponse,\n+import org.opensearch.client.opensearch.OpenSearchClient,\n+import org.opensearch.client.opensearch.core.InfoResponse,\n+import org.opensearch.dataprepper.aws.api.AwsCredentialsSupplier,\n+import software.amazon.awssdk.auth.credentials.AnonymousCredentialsProvider,\n+\n+import javax.net.ssl.SSLHandshakeException,\n+import java.io.IOException,\n+import java.util.Collections,\n+import java.util.Map,\n+import java.util.UUID,\n+\n+import static com.github.tomakehurst.wiremock.client.WireMock.get,\n+import static com.github.tomakehurst.wiremock.client.WireMock.jsonResponse,\n+import static com.github.tomakehurst.wiremock.core.WireMockConfiguration.options,\n+import static org.hamcrest.CoreMatchers.equalTo,\n+import static org.hamcrest.CoreMatchers.notNullValue,\n+import static org.hamcrest.MatcherAssert.assertThat,\n+import static org.junit.jupiter.api.Assertions.assertThrows,\n+import static org.mockito.ArgumentMatchers.any,\n+import static org.mockito.Mockito.when,\n+\n+@ExtendWith(MockitoExtension.class)\n+class ConnectionConfiguration_ServerTest {\n+    private static WireMockServer wireMockServer,\n+\n+    @Mock\n+    private AwsCredentialsSupplier awsCredentialsSupplier,\n+\n+    private String host,\n+\n+    private String clusterUuid,\n+\n+    @BeforeAll\n+    static void setUpAll() {\n+        wireMockServer = new WireMockServer(options()\n+                .httpDisabled(true)\n+                .dynamicHttpsPort()\n+                .keystorePath(\"src/test/resources/test_keystore.jks\")\n+                .keystorePassword(\"password\")\n+                .keyManagerPassword(\"password\")\n+        ),\n+\n+        wireMockServer.start(),\n+    }\n+\n+    @AfterAll\n+    static void tearDownAll() {\n+        wireMockServer.stop(),\n+    }\n+\n+    @BeforeEach\n+    void setUp() {\n+        host = \"https://localhost:\" + wireMockServer.httpsPort(),\n+\n+        clusterUuid = UUID.randomUUID().toString(),\n+        final Map<String, Object> responseBody = Map.of(\n+                \"name\", \"opensearch\",\n+                \"cluster_name\", \"opensearch\",\n+                \"cluster_uuid\", clusterUuid,\n+                \"version\", Map.of(\n+                        \"number\", \"2.10.0\",\n+                        \"build_hash\", \"abcdefg\",\n+                        \"build_date\", \"20241212\",\n+                        \"build_type\", \"testing\",\n+                        \"distribution\", \"datapreppertesting\",\n+                        \"build_snapshot\", \"false\",\n+                        \"lucene_version\", \"8\",\n+                        \"minimum_wire_compatibility_version\", \"2.10.0\",\n+                        \"minimum_index_compatibility_version\", \"2.10.0\"\n+                ),\n+                \"tagline\", \"You Know, for Search\"\n+        ),\n+        wireMockServer.stubFor(get(\"/\").willReturn(jsonResponse(responseBody, 200))),\n+    }\n+\n+    @Nested\n+    class DefaultConfiguration {\n+        private ConnectionConfiguration createObjectUnderTest() {\n+            return new ConnectionConfiguration.Builder(Collections.singletonList(host))\n+                    .build(),\n+        }\n+\n+        @Test\n+        void createClient_will_not_trust_self_signed_certificates_by_default() {\n+            final RestHighLevelClient client = createObjectUnderTest().createClient(awsCredentialsSupplier),\n+            assertThat(client, notNullValue()),\n+\n+            assertThrows(SSLHandshakeException.class, () -> client.info(RequestOptions.DEFAULT)),\n+        }\n+\n+        @Test\n+        void createOpenSearchClient_will_not_trust_self_signed_certificates_by_default() {\n+            final ConnectionConfiguration objectUnderTest = createObjectUnderTest(),\n+            final OpenSearchClient openSearchClient = objectUnderTest.createOpenSearchClient(objectUnderTest.createClient(awsCredentialsSupplier), awsCredentialsSupplier),\n+            assertThat(openSearchClient, notNullValue()),\n+\n+            assertThrows(SSLHandshakeException.class, openSearchClient::info),\n+        }\n+    }\n+\n+    @Nested\n+    class DefaultSigV4Configuration {\n+        @BeforeEach\n+        void setUp() {\n+            when(awsCredentialsSupplier.getProvider(any())).thenReturn(AnonymousCredentialsProvider.create()),\n+        }\n+\n+        private ConnectionConfiguration createObjectUnderTest() {\n+            return new ConnectionConfiguration.Builder(Collections.singletonList(host))\n+                    .withAwsSigv4(true)\n+                    .withAwsRegion(\"us-east-1\")\n+                    .build(),\n+        }\n+\n+        @Test\n+        void createClient_will_not_trust_self_signed_certificates_by_default() {\n+            final RestHighLevelClient client = createObjectUnderTest().createClient(awsCredentialsSupplier),\n+            assertThat(client, notNullValue()),\n+\n+            assertThrows(SSLHandshakeException.class, () -> client.info(RequestOptions.DEFAULT)),\n+        }\n+\n+        @Test\n+        void createOpenSearchClient_will_not_trust_self_signed_certificates_by_default() {\n+            final ConnectionConfiguration objectUnderTest = createObjectUnderTest(),\n+            final OpenSearchClient openSearchClient = objectUnderTest.createOpenSearchClient(objectUnderTest.createClient(awsCredentialsSupplier), awsCredentialsSupplier),\n+            assertThat(openSearchClient, notNullValue()),\n+\n+            assertThrows(SSLHandshakeException.class, openSearchClient::info),\n+        }\n+    }\n+\n+    @Nested\n+    class InsecureConfiguration {\n+        private ConnectionConfiguration createObjectUnderTest() {\n+            return new ConnectionConfiguration.Builder(Collections.singletonList(host))\n+                    .withInsecure(true)\n+                    .build(),\n+        }\n+\n+        @Test\n+        void createClient_will_trust_self_signed_certificates_if_insecure() throws IOException {\n+            final RestHighLevelClient client = createObjectUnderTest().createClient(awsCredentialsSupplier),\n+            assertThat(client, notNullValue()),\n+\n+            final MainResponse infoResponse = client.info(RequestOptions.DEFAULT),\n+\n+            assertThat(infoResponse, notNullValue()),\n+            assertThat(infoResponse.getClusterName(), equalTo(\"opensearch\")),\n+            assertThat(infoResponse.getClusterUuid(), equalTo(clusterUuid)),\n+        }\n+\n+\n+        @Test\n+        void createOpenSearchClient_will_trust_self_signed_certificates_if_insecure() throws IOException {\n+            final ConnectionConfiguration objectUnderTest = createObjectUnderTest(),\n+            final OpenSearchClient openSearchClient = objectUnderTest.createOpenSearchClient(objectUnderTest.createClient(awsCredentialsSupplier), awsCredentialsSupplier),\n+            assertThat(openSearchClient, notNullValue()),\n+\n+            final InfoResponse infoResponse = openSearchClient.info(),\n+\n+            assertThat(infoResponse, notNullValue()),\n+            assertThat(infoResponse.clusterName(), equalTo(\"opensearch\")),\n+            assertThat(infoResponse.clusterUuid(), equalTo(clusterUuid)),\n+        }\n+    }\n+}\n\\ No newline at end of file"
        },
        {
          "filename": "data-prepper-plugins/opensearch/src/test/java/org/opensearch/dataprepper/plugins/source/opensearch/worker/client/OpenSearchClientFactoryTest.java",
          "old_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/b0386a5af3fb71094ba6c86cd8b2afc783246599/data-prepper-plugins/opensearch/src/test/java/org/opensearch/dataprepper/plugins/source/opensearch/worker/client/OpenSearchClientFactoryTest.java",
          "new_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/98fcf0d0ff9c18f1f7501e11dbed918814724b99/data-prepper-plugins/opensearch/src/test/java/org/opensearch/dataprepper/plugins/source/opensearch/worker/client/OpenSearchClientFactoryTest.java",
          "diff": "@@ -27,6 +27,8 @@\n import software.amazon.awssdk.regions.Region,\n \n import javax.net.ssl.SSLContext,\n+import javax.net.ssl.TrustManager,\n+\n import java.nio.file.Path,\n import java.time.Duration,\n import java.util.Collections,\n@@ -41,6 +43,7 @@\n import static org.mockito.Mockito.mock,\n import static org.mockito.Mockito.mockStatic,\n import static org.mockito.Mockito.never,\n+import static org.mockito.Mockito.times,\n import static org.mockito.Mockito.verify,\n import static org.mockito.Mockito.verifyNoInteractions,\n import static org.mockito.Mockito.when,\n@@ -409,9 +412,40 @@ void createSdkAsyncHttpClient_with_self_signed_certificate() {\n         lenient().when(openSearchSourceConfiguration.getConnectionConfiguration()).thenReturn(connectionConfiguration),\n         lenient().when(connectionConfiguration.getCertPath()).thenReturn(path),\n         try (MockedStatic<TrustStoreProvider> trustStoreProviderMockedStatic = mockStatic(TrustStoreProvider.class)) {\n+            TrustManager[] mockTrustManagers = new TrustManager[] { mock(TrustManager.class) },\n+            trustStoreProviderMockedStatic.when(() -> TrustStoreProvider.createTrustManager(path)).thenReturn(mockTrustManagers),\n             final SdkAsyncHttpClient sdkAsyncHttpClient = createObjectUnderTest().createSdkAsyncHttpClient(openSearchSourceConfiguration),\n             assertThat(sdkAsyncHttpClient, notNullValue()),\n             trustStoreProviderMockedStatic.verify(() -> TrustStoreProvider.createTrustManager(path)),\n         }\n     }\n+    @Test\n+    void createSdkAsyncHttpClient_with_secure_configuration_and_no_cert_path_does_not_trust_all_managers() {\n+        when(connectionConfiguration.getCertPath()).thenReturn(null),\n+        when(connectionConfiguration.isInsecure()).thenReturn(false),\n+        when(connectionConfiguration.getConnectTimeout()).thenReturn(Duration.ofSeconds(30)),\n+        try (MockedStatic<TrustStoreProvider> trustStoreProviderMockedStatic = mockStatic(TrustStoreProvider.class)) {\n+            final SdkAsyncHttpClient sdkAsyncHttpClient = createObjectUnderTest().createSdkAsyncHttpClient(openSearchSourceConfiguration),\n+            assertThat(sdkAsyncHttpClient, notNullValue()),\n+            trustStoreProviderMockedStatic.verify(() -> TrustStoreProvider.createTrustAllManager(), never()),\n+            trustStoreProviderMockedStatic.verify(() -> TrustStoreProvider.createTrustManager(any(Path.class)), never()),\n+        }\n+    }\n+    \n+    @Test\n+    void createSdkAsyncHttpClient_with_insecure_configuration_and_no_cert_path_trusts_all_managers() {\n+        when(connectionConfiguration.getCertPath()).thenReturn(null),\n+        when(connectionConfiguration.isInsecure()).thenReturn(true),\n+        when(connectionConfiguration.getConnectTimeout()).thenReturn(Duration.ofSeconds(30)),\n+        try (MockedStatic<TrustStoreProvider> trustStoreProviderMockedStatic = mockStatic(TrustStoreProvider.class)) {\n+            TrustManager[] mockTrustManagers = new TrustManager[] { mock(TrustManager.class) },\n+            trustStoreProviderMockedStatic.when(() -> TrustStoreProvider.createTrustAllManager())\n+                    .thenReturn(mockTrustManagers),\n+            final SdkAsyncHttpClient sdkAsyncHttpClient = createObjectUnderTest().createSdkAsyncHttpClient(openSearchSourceConfiguration),\n+            assertThat(sdkAsyncHttpClient, notNullValue()),\n+            trustStoreProviderMockedStatic.verify(() -> TrustStoreProvider.createTrustAllManager(), times(1)),\n+            trustStoreProviderMockedStatic.verify(() -> TrustStoreProvider.createTrustManager(any(Path.class)), never()),\n+        }\n+    }\n+\n }"
        },
        {
          "filename": "data-prepper-plugins/opensearch/src/test/resources/test_keystore.jks",
          "old_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/b0386a5af3fb71094ba6c86cd8b2afc783246599/data-prepper-plugins/opensearch/src/test/resources/test_keystore.jks",
          "new_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/98fcf0d0ff9c18f1f7501e11dbed918814724b99/data-prepper-plugins/opensearch/src/test/resources/test_keystore.jks",
          "diff": " "
        },
        {
          "filename": "e2e-test/log/src/integrationTest/java/org/opensearch/dataprepper/integration/log/EndToEndBasicLogTest.java",
          "old_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/b0386a5af3fb71094ba6c86cd8b2afc783246599/e2e-test/log/src/integrationTest/java/org/opensearch/dataprepper/integration/log/EndToEndBasicLogTest.java",
          "new_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/98fcf0d0ff9c18f1f7501e11dbed918814724b99/e2e-test/log/src/integrationTest/java/org/opensearch/dataprepper/integration/log/EndToEndBasicLogTest.java",
          "diff": "@@ -132,6 +132,7 @@ private RestHighLevelClient prepareOpenSearchRestHighLevelClient() {\n                 Collections.singletonList(\"https://127.0.0.1:9200\")),\n         builder.withUsername(\"admin\"),\n         builder.withPassword(\"admin\"),\n+        builder.withInsecure(true),\n         return builder.build().createClient(null),\n     }\n "
        },
        {
          "filename": "e2e-test/log/src/integrationTest/java/org/opensearch/dataprepper/integration/log/ParallelGrokStringSubstituteLogTest.java",
          "old_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/b0386a5af3fb71094ba6c86cd8b2afc783246599/e2e-test/log/src/integrationTest/java/org/opensearch/dataprepper/integration/log/ParallelGrokStringSubstituteLogTest.java",
          "new_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/98fcf0d0ff9c18f1f7501e11dbed918814724b99/e2e-test/log/src/integrationTest/java/org/opensearch/dataprepper/integration/log/ParallelGrokStringSubstituteLogTest.java",
          "diff": "@@ -100,6 +100,7 @@ private RestHighLevelClient prepareOpenSearchRestHighLevelClient() {\n                 Collections.singletonList(\"https://127.0.0.1:9200\")),\n         builder.withUsername(\"admin\"),\n         builder.withPassword(\"admin\"),\n+        builder.withInsecure(true),\n         final AwsCredentialsSupplier awsCredentialsSupplier = mock(AwsCredentialsSupplier.class),\n         return builder.build().createClient(awsCredentialsSupplier),\n     }"
        },
        {
          "filename": "e2e-test/log/src/integrationTest/resources/basic-grok-e2e-pipeline-date-pattern-index.yml",
          "old_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/b0386a5af3fb71094ba6c86cd8b2afc783246599/e2e-test/log/src/integrationTest/resources/basic-grok-e2e-pipeline-date-pattern-index.yml",
          "new_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/98fcf0d0ff9c18f1f7501e11dbed918814724b99/e2e-test/log/src/integrationTest/resources/basic-grok-e2e-pipeline-date-pattern-index.yml",
          "diff": "@@ -11,5 +11,6 @@ grok-pipeline:\n         hosts: [ \"https://node-0.example.com:9200\" ]\n         username: \"admin\"\n         password: \"admin\"\n+        insecure: true\n         index: \"test-grok-index-%{yyyy.MM.dd}\"\n         flush_timeout: 5000"
        },
        {
          "filename": "e2e-test/log/src/integrationTest/resources/basic-grok-e2e-pipeline-with-aws-secrets.yml",
          "old_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/b0386a5af3fb71094ba6c86cd8b2afc783246599/e2e-test/log/src/integrationTest/resources/basic-grok-e2e-pipeline-with-aws-secrets.yml",
          "new_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/98fcf0d0ff9c18f1f7501e11dbed918814724b99/e2e-test/log/src/integrationTest/resources/basic-grok-e2e-pipeline-with-aws-secrets.yml",
          "diff": "@@ -17,5 +17,6 @@ grok-pipeline:\n         hosts: [ \"https://node-0.example.com:9200\" ]\n         username: \"${{aws_secrets:opensearch-sink:username}}\"\n         password: \"${{aws_secrets:opensearch-sink:password}}\"\n+        insecure: true\n         index: \"test-grok-index\"\n         flush_timeout: 5000\n\\ No newline at end of file"
        },
        {
          "filename": "e2e-test/log/src/integrationTest/resources/basic-grok-e2e-pipeline.yml",
          "old_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/b0386a5af3fb71094ba6c86cd8b2afc783246599/e2e-test/log/src/integrationTest/resources/basic-grok-e2e-pipeline.yml",
          "new_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/98fcf0d0ff9c18f1f7501e11dbed918814724b99/e2e-test/log/src/integrationTest/resources/basic-grok-e2e-pipeline.yml",
          "diff": "@@ -12,5 +12,6 @@ grok-pipeline:\n         hosts: [ \"https://node-0.example.com:9200\" ]\n         username: \"admin\"\n         password: \"admin\"\n+        insecure: true\n         index: \"test-grok-index\"\n         flush_timeout: 5000"
        },
        {
          "filename": "e2e-test/log/src/integrationTest/resources/parallel-grok-substitute-e2e-pipeline.yml",
          "old_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/b0386a5af3fb71094ba6c86cd8b2afc783246599/e2e-test/log/src/integrationTest/resources/parallel-grok-substitute-e2e-pipeline.yml",
          "new_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/98fcf0d0ff9c18f1f7501e11dbed918814724b99/e2e-test/log/src/integrationTest/resources/parallel-grok-substitute-e2e-pipeline.yml",
          "diff": "@@ -22,6 +22,7 @@ pipeline2:\n         hosts: [ \"https://node-0.example.com:9200\" ]\n         username: \"admin\"\n         password: \"admin\"\n+        insecure: true\n         index: \"test-substitute-index\"\n         flush_timeout: 5000\n \n@@ -38,5 +39,6 @@ pipeline3:\n         hosts: [ \"https://node-0.example.com:9200\" ]\n         username: \"admin\"\n         password: \"admin\"\n+        insecure: true\n         index: \"test-grok-index\"\n         flush_timeout: 5000"
        },
        {
          "filename": "e2e-test/peerforwarder/src/integrationTest/java/org/opensearch/dataprepper/integration/peerforwarder/EndToEndLogMetricsTest.java",
          "old_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/b0386a5af3fb71094ba6c86cd8b2afc783246599/e2e-test/peerforwarder/src/integrationTest/java/org/opensearch/dataprepper/integration/peerforwarder/EndToEndLogMetricsTest.java",
          "new_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/98fcf0d0ff9c18f1f7501e11dbed918814724b99/e2e-test/peerforwarder/src/integrationTest/java/org/opensearch/dataprepper/integration/peerforwarder/EndToEndLogMetricsTest.java",
          "diff": "@@ -181,6 +181,7 @@ private RestHighLevelClient prepareOpenSearchRestHighLevelClient() {\n                 Collections.singletonList(\"https://127.0.0.1:9200\")),\n         builder.withUsername(\"admin\"),\n         builder.withPassword(\"admin\"),\n+        builder.withInsecure(true),\n         final AwsCredentialsSupplier awsCredentialsSupplier = mock(AwsCredentialsSupplier.class),\n         return builder.build().createClient(awsCredentialsSupplier),\n     }"
        },
        {
          "filename": "e2e-test/peerforwarder/src/integrationTest/java/org/opensearch/dataprepper/integration/peerforwarder/EndToEndPeerForwarderTest.java",
          "old_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/b0386a5af3fb71094ba6c86cd8b2afc783246599/e2e-test/peerforwarder/src/integrationTest/java/org/opensearch/dataprepper/integration/peerforwarder/EndToEndPeerForwarderTest.java",
          "new_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/98fcf0d0ff9c18f1f7501e11dbed918814724b99/e2e-test/peerforwarder/src/integrationTest/java/org/opensearch/dataprepper/integration/peerforwarder/EndToEndPeerForwarderTest.java",
          "diff": "@@ -117,6 +117,7 @@ private RestHighLevelClient prepareOpenSearchRestHighLevelClient() {\n                 Collections.singletonList(\"https://127.0.0.1:9200\")),\n         builder.withUsername(\"admin\"),\n         builder.withPassword(\"admin\"),\n+        builder.withInsecure(true),\n         final AwsCredentialsSupplier awsCredentialsSupplier = mock(AwsCredentialsSupplier.class),\n         return builder.build().createClient(awsCredentialsSupplier),\n     }"
        },
        {
          "filename": "e2e-test/peerforwarder/src/integrationTest/resources/aggregate-e2e-pipeline.yml",
          "old_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/b0386a5af3fb71094ba6c86cd8b2afc783246599/e2e-test/peerforwarder/src/integrationTest/resources/aggregate-e2e-pipeline.yml",
          "new_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/98fcf0d0ff9c18f1f7501e11dbed918814724b99/e2e-test/peerforwarder/src/integrationTest/resources/aggregate-e2e-pipeline.yml",
          "diff": "@@ -12,5 +12,6 @@ aggregate-pipeline:\n         hosts: [ \"https://node-0.example.com:9200\" ]\n         username: \"admin\"\n         password: \"admin\"\n+        insecure: true\n         index: \"test-peer-forwarder-index\"\n         flush_timeout: 5000\n\\ No newline at end of file"
        },
        {
          "filename": "e2e-test/peerforwarder/src/integrationTest/resources/log-metrics-pipeline.yml",
          "old_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/b0386a5af3fb71094ba6c86cd8b2afc783246599/e2e-test/peerforwarder/src/integrationTest/resources/log-metrics-pipeline.yml",
          "new_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/98fcf0d0ff9c18f1f7501e11dbed918814724b99/e2e-test/peerforwarder/src/integrationTest/resources/log-metrics-pipeline.yml",
          "diff": "@@ -16,5 +16,6 @@ aggregate-pipeline:\n         hosts: [ \"https://node-0.example.com:9200\" ]\n         username: \"admin\"\n         password: \"admin\"\n+        insecure: true\n         index: \"test-log-metrics-index\"\n         flush_timeout: 5000"
        },
        {
          "filename": "e2e-test/trace/src/integrationTest/java/org/opensearch/dataprepper/integration/trace/EndToEndRawSpanTest.java",
          "old_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/b0386a5af3fb71094ba6c86cd8b2afc783246599/e2e-test/trace/src/integrationTest/java/org/opensearch/dataprepper/integration/trace/EndToEndRawSpanTest.java",
          "new_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/98fcf0d0ff9c18f1f7501e11dbed918814724b99/e2e-test/trace/src/integrationTest/java/org/opensearch/dataprepper/integration/trace/EndToEndRawSpanTest.java",
          "diff": "@@ -115,6 +115,7 @@ public void testPipelineEndToEnd() {\n                 Collections.singletonList(\"https://127.0.0.1:9200\")),\n         builder.withUsername(\"admin\"),\n         builder.withPassword(\"admin\"),\n+        builder.withInsecure(true),\n         final RestHighLevelClient restHighLevelClient = builder.build().createClient(null),\n         // Wait for data to flow through pipeline and be indexed by ES\n         await().atLeast(3, TimeUnit.SECONDS).atMost(20, TimeUnit.SECONDS).untilAsserted("
        },
        {
          "filename": "e2e-test/trace/src/integrationTest/java/org/opensearch/dataprepper/integration/trace/EndToEndServiceMapTest.java",
          "old_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/b0386a5af3fb71094ba6c86cd8b2afc783246599/e2e-test/trace/src/integrationTest/java/org/opensearch/dataprepper/integration/trace/EndToEndServiceMapTest.java",
          "new_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/98fcf0d0ff9c18f1f7501e11dbed918814724b99/e2e-test/trace/src/integrationTest/java/org/opensearch/dataprepper/integration/trace/EndToEndServiceMapTest.java",
          "diff": "@@ -81,6 +81,7 @@ public void testPipelineEndToEnd() {\n                 Collections.singletonList(\"https://127.0.0.1:9200\")),\n         builder.withUsername(\"admin\"),\n         builder.withPassword(\"admin\"),\n+        builder.withInsecure(true),\n         final AwsCredentialsSupplier awsCredentialsSupplier = mock(AwsCredentialsSupplier.class),\n         final RestHighLevelClient restHighLevelClient = builder.build().createClient(awsCredentialsSupplier),\n "
        },
        {
          "filename": "e2e-test/trace/src/integrationTest/resources/raw-span-e2e-pipeline-from-build.yml",
          "old_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/b0386a5af3fb71094ba6c86cd8b2afc783246599/e2e-test/trace/src/integrationTest/resources/raw-span-e2e-pipeline-from-build.yml",
          "new_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/98fcf0d0ff9c18f1f7501e11dbed918814724b99/e2e-test/trace/src/integrationTest/resources/raw-span-e2e-pipeline-from-build.yml",
          "diff": "@@ -18,5 +18,6 @@ raw-pipeline:\n         hosts: [ \"https://node-0.example.com:9200\" ]\n         username: \"admin\"\n         password: \"admin\"\n+        insecure: true\n         index_type: trace-analytics-raw\n         flush_timeout: 5000\n\\ No newline at end of file"
        },
        {
          "filename": "e2e-test/trace/src/integrationTest/resources/raw-span-e2e-pipeline-latest-release.yml",
          "old_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/b0386a5af3fb71094ba6c86cd8b2afc783246599/e2e-test/trace/src/integrationTest/resources/raw-span-e2e-pipeline-latest-release.yml",
          "new_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/98fcf0d0ff9c18f1f7501e11dbed918814724b99/e2e-test/trace/src/integrationTest/resources/raw-span-e2e-pipeline-latest-release.yml",
          "diff": "@@ -18,5 +18,6 @@ raw-pipeline:\n         hosts: [ \"https://node-0.example.com:9200\" ]\n         username: \"admin\"\n         password: \"admin\"\n+        insecure: true\n         index_type: trace-analytics-raw\n         flush_timeout: 5000"
        },
        {
          "filename": "e2e-test/trace/src/integrationTest/resources/raw-span-e2e-pipeline.yml",
          "old_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/b0386a5af3fb71094ba6c86cd8b2afc783246599/e2e-test/trace/src/integrationTest/resources/raw-span-e2e-pipeline.yml",
          "new_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/98fcf0d0ff9c18f1f7501e11dbed918814724b99/e2e-test/trace/src/integrationTest/resources/raw-span-e2e-pipeline.yml",
          "diff": "@@ -16,10 +16,12 @@ raw-pipeline:\n         hosts: [ \"https://node-0.example.com:9200\" ]\n         username: \"admin\"\n         password: \"admin\"\n+        insecure: true\n   sink:\n     - opensearch:\n         hosts: [ \"https://node-0.example.com:9200\" ]\n         username: \"admin\"\n         password: \"admin\"\n+        insecure: true\n         index_type: trace-analytics-raw\n         flush_timeout: 5000"
        },
        {
          "filename": "e2e-test/trace/src/integrationTest/resources/service-map-e2e-pipeline.yml",
          "old_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/b0386a5af3fb71094ba6c86cd8b2afc783246599/e2e-test/trace/src/integrationTest/resources/service-map-e2e-pipeline.yml",
          "new_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/98fcf0d0ff9c18f1f7501e11dbed918814724b99/e2e-test/trace/src/integrationTest/resources/service-map-e2e-pipeline.yml",
          "diff": "@@ -18,5 +18,6 @@ service-map-pipeline:\n         hosts: [\"https://node-0.example.com:9200\"]\n         username: \"admin\"\n         password: \"admin\"\n+        insecure: true\n         index_type: trace-analytics-service-map\n         flush_timeout: 5000"
        }
      ]
    }
  ],
  [
    {
      "cve_id": [
        "CVE-2025-62371",
        "https://github.com/opensearch-project/data-prepper/commit/b0386a5af3fb71094ba6c86cd8b2afc783246599"
      ],
      "repo": "data-prepper",
      "commit_hash": "b0386a5af3fb71094ba6c86cd8b2afc783246599",
      "commit_message": "Use standard TLS when downloading the database from an HTTP URL. (#6163)  Signed-off-by: David Venable <dlv@amazon.com>",
      "files_changed": [
        {
          "filename": "data-prepper-plugins/geoip-processor/src/main/java/org/opensearch/dataprepper/plugins/geoip/extension/databasedownload/DBSource.java",
          "old_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/db11ce8f27ebca018980b2bca863f7173de9ce56/data-prepper-plugins/geoip-processor/src/main/java/org/opensearch/dataprepper/plugins/geoip/extension/databasedownload/DBSource.java",
          "new_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/b0386a5af3fb71094ba6c86cd8b2afc783246599/data-prepper-plugins/geoip-processor/src/main/java/org/opensearch/dataprepper/plugins/geoip/extension/databasedownload/DBSource.java",
          "diff": "@@ -5,50 +5,7 @@\n \n package org.opensearch.dataprepper.plugins.geoip.extension.databasedownload,\n \n-import javax.net.ssl.HostnameVerifier,\n-import javax.net.ssl.HttpsURLConnection,\n-import javax.net.ssl.SSLContext,\n-import javax.net.ssl.SSLSession,\n-import javax.net.ssl.TrustManager,\n-import javax.net.ssl.X509TrustManager,\n-import java.security.KeyManagementException,\n-import java.security.NoSuchAlgorithmException,\n-import java.security.SecureRandom,\n-import java.security.cert.CertificateException,\n-import java.security.cert.X509Certificate,\n-\n public interface DBSource {\n     String MAXMIND_DATABASE_EXTENSION = \".mmdb\",\n     void initiateDownload() throws Exception,\n-\n-    /**\n-     * initiateSSL\n-     * @throws NoSuchAlgorithmException NoSuchAlgorithmException\n-     * @throws KeyManagementException KeyManagementException\n-     */\n-    default void initiateSSL() throws NoSuchAlgorithmException, KeyManagementException {\n-        final TrustManager[] trustAllCerts = new TrustManager[]{\n-                new X509TrustManager() {\n-                    public X509Certificate[] getAcceptedIssuers() {\n-                        return null,\n-                    }\n-                    public void checkServerTrusted(X509Certificate[] certs, String authType) throws CertificateException {\n-                        return,\n-                    }\n-                    public void checkClientTrusted(X509Certificate[] certs, String authType) throws CertificateException {\n-                        return,\n-                    }\n-                }\n-        },\n-\n-        final SSLContext sc = SSLContext.getInstance(\"TLS\"),\n-        sc.init(null, trustAllCerts, new SecureRandom()),\n-        HttpsURLConnection.setDefaultSSLSocketFactory(sc.getSocketFactory()),\n-        final HostnameVerifier hostnameVerifier = new HostnameVerifier() {\n-            public boolean verify(String urlHostName, SSLSession session) {\n-                return true,\n-            }\n-        },\n-        HttpsURLConnection.setDefaultHostnameVerifier(hostnameVerifier),\n-    }\n }"
        },
        {
          "filename": "data-prepper-plugins/geoip-processor/src/main/java/org/opensearch/dataprepper/plugins/geoip/extension/databasedownload/HttpDBDownloadService.java",
          "old_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/db11ce8f27ebca018980b2bca863f7173de9ce56/data-prepper-plugins/geoip-processor/src/main/java/org/opensearch/dataprepper/plugins/geoip/extension/databasedownload/HttpDBDownloadService.java",
          "new_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/b0386a5af3fb71094ba6c86cd8b2afc783246599/data-prepper-plugins/geoip-processor/src/main/java/org/opensearch/dataprepper/plugins/geoip/extension/databasedownload/HttpDBDownloadService.java",
          "diff": "@@ -53,7 +53,6 @@ public void initiateDownload() {\n         for (final String key: databasePaths) {\n             geoIPFileManager.createDirectoryIfNotExist(tarDir),\n             try {\n-                initiateSSL(),\n                 buildRequestAndDownloadFile(maxMindDatabaseConfig.getDatabasePaths().get(key), downloadTarFilepath),\n                 final File tarFile = decompressAndgetTarFile(tarDir, downloadTarFilepath),\n                 unTarFile(tarFile, new File(destinationDirectory), key),"
        }
      ]
    }
  ],
  [
    {
      "cve_id": [
        "CVE-2025-62371",
        "https://github.com/opensearch-project/data-prepper/commit/db11ce8f27ebca018980b2bca863f7173de9ce56"
      ],
      "repo": "data-prepper",
      "commit_hash": "db11ce8f27ebca018980b2bca863f7173de9ce56",
      "commit_message": "Change \"SSL\" to \"TLS\" (#6164)  Signed-off-by: David Venable <dlv@amazon.com>",
      "files_changed": [
        {
          "filename": "data-prepper-plugins/geoip-processor/src/main/java/org/opensearch/dataprepper/plugins/geoip/extension/databasedownload/DBSource.java",
          "old_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/ebe334df7f19d569b2effd486f98e429337e5d87/data-prepper-plugins/geoip-processor/src/main/java/org/opensearch/dataprepper/plugins/geoip/extension/databasedownload/DBSource.java",
          "new_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/db11ce8f27ebca018980b2bca863f7173de9ce56/data-prepper-plugins/geoip-processor/src/main/java/org/opensearch/dataprepper/plugins/geoip/extension/databasedownload/DBSource.java",
          "diff": "@@ -41,7 +41,7 @@ public void checkClientTrusted(X509Certificate[] certs, String authType) throws\n                 }\n         },\n \n-        final SSLContext sc = SSLContext.getInstance(\"SSL\"),\n+        final SSLContext sc = SSLContext.getInstance(\"TLS\"),\n         sc.init(null, trustAllCerts, new SecureRandom()),\n         HttpsURLConnection.setDefaultSSLSocketFactory(sc.getSocketFactory()),\n         final HostnameVerifier hostnameVerifier = new HostnameVerifier() {"
        },
        {
          "filename": "data-prepper-plugins/kafka-plugins/src/main/java/org/opensearch/dataprepper/plugins/kafka/util/CustomClientSslEngineFactory.java",
          "old_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/ebe334df7f19d569b2effd486f98e429337e5d87/data-prepper-plugins/kafka-plugins/src/main/java/org/opensearch/dataprepper/plugins/kafka/util/CustomClientSslEngineFactory.java",
          "new_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/db11ce8f27ebca018980b2bca863f7173de9ce56/data-prepper-plugins/kafka-plugins/src/main/java/org/opensearch/dataprepper/plugins/kafka/util/CustomClientSslEngineFactory.java",
          "diff": "@@ -40,7 +40,7 @@ private TrustManager[] getTrustManager() {\n     @Override\n     public SSLEngine createClientSslEngine(final String peerHost, final int peerPort, final String endpointIdentification) {\n         try {\n-            final SSLContext sslContext = SSLContext.getInstance(\"SSL\"),\n+            final SSLContext sslContext = SSLContext.getInstance(\"TLS\"),\n             sslContext.init(null, getTrustManager(), new SecureRandom()),\n             SSLEngine sslEngine = sslContext.createSSLEngine(peerHost, peerPort),\n             sslEngine.setUseClientMode(true),"
        },
        {
          "filename": "data-prepper-plugins/kafka-plugins/src/main/java/org/opensearch/dataprepper/plugins/kafka/util/InsecureSslEngineFactory.java",
          "old_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/ebe334df7f19d569b2effd486f98e429337e5d87/data-prepper-plugins/kafka-plugins/src/main/java/org/opensearch/dataprepper/plugins/kafka/util/InsecureSslEngineFactory.java",
          "new_url": "https://raw.githubusercontent.com/opensearch-project/data-prepper/db11ce8f27ebca018980b2bca863f7173de9ce56/data-prepper-plugins/kafka-plugins/src/main/java/org/opensearch/dataprepper/plugins/kafka/util/InsecureSslEngineFactory.java",
          "diff": "@@ -39,7 +39,7 @@ public void checkServerTrusted(X509Certificate[] certs, String authType) {\n     public SSLEngine createClientSslEngine(String peerHost, int peerPort, String endpointIdentification) {\n         TrustManager[] trustManagers = new TrustManager[]{ INSECURE_TRUST_MANAGER },\n         try {\n-            SSLContext sslContext = SSLContext.getInstance(\"SSL\"),\n+            SSLContext sslContext = SSLContext.getInstance(\"TLS\"),\n             sslContext.init(null, trustManagers, new SecureRandom()),\n             SSLEngine sslEngine = sslContext.createSSLEngine(peerHost, peerPort),\n             sslEngine.setUseClientMode(true),"
        }
      ]
    }
  ],
  [
    {
      "cve_id": [
        "CVE-2025-62381",
        "https://github.com/ciscoheat/sveltekit-superforms/commit/4a1310dd1a94176bb22036662c530dad48059ca4"
      ],
      "repo": "sveltekit-superforms",
      "commit_hash": "4a1310dd1a94176bb22036662c530dad48059ca4",
      "commit_message": "Fixed prototype pollution when using dataType: 'json'",
      "files_changed": [
        {
          "filename": "CHANGELOG.md",
          "old_url": "https://raw.githubusercontent.com/ciscoheat/sveltekit-superforms/58faeb9f7d9b73d8f9724a92519e6bb55a07f4ee/CHANGELOG.md",
          "new_url": "https://raw.githubusercontent.com/ciscoheat/sveltekit-superforms/4a1310dd1a94176bb22036662c530dad48059ca4/CHANGELOG.md",
          "diff": "@@ -5,7 +5,7 @@ Headlines: Added, Changed, Deprecated, Removed, Fixed, Security\n The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\n and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n \n-## [2.27.3] - 2025-10-14\n+## [2.27.4] - 2025-10-14\n \n ### Security\n "
        },
        {
          "filename": "src/lib/traversal.ts",
          "old_url": "https://raw.githubusercontent.com/ciscoheat/sveltekit-superforms/58faeb9f7d9b73d8f9724a92519e6bb55a07f4ee/src/lib/traversal.ts",
          "new_url": "https://raw.githubusercontent.com/ciscoheat/sveltekit-superforms/4a1310dd1a94176bb22036662c530dad48059ca4/src/lib/traversal.ts",
          "diff": "@@ -52,6 +52,12 @@ export function traversePath<T extends object>(\n \tmodifier?: (data: PathData) => undefined | unknown | void\n ): PathData | undefined {\n \tif (!realPath.length) return undefined,\n+\n+\t// Prevent prototype injection\n+\tif (realPath.includes('__proto__') || realPath.includes('prototype')) {\n+\t\tthrow new Error(\"Cannot set an object's `__proto__` or `prototype` property\"),\n+\t}\n+\n \tconst path = [realPath[0]],\n \n \tlet parent = obj,"
        }
      ]
    }
  ],
  [
    {
      "cve_id": [
        "CVE-2025-62382",
        "https://github.com/blakeblackshear/frigate/commit/d7f7cd7be16bfe7a12766b797da6b8add687ccd9"
      ],
      "repo": "frigate",
      "commit_hash": "d7f7cd7be16bfe7a12766b797da6b8add687ccd9",
      "commit_message": "best thumbnail endpoint should pass correct extension param (#19930)",
      "files_changed": [
        {
          "filename": "frigate/api/media.py",
          "old_url": "https://raw.githubusercontent.com/blakeblackshear/frigate/6591210050bd10705863afa67648d3eccc615471/frigate/api/media.py",
          "new_url": "https://raw.githubusercontent.com/blakeblackshear/frigate/d7f7cd7be16bfe7a12766b797da6b8add687ccd9/frigate/api/media.py",
          "diff": "@@ -1598,7 +1598,7 @@ def label_thumbnail(request: Request, camera_name: str, label: str):\n     try:\n         event_id = event_query.scalar()\n \n-        return event_thumbnail(request, event_id, 60)\n+        return event_thumbnail(request, event_id, Extension.jpg, 60)\n     except DoesNotExist:\n         frame = np.zeros((175, 175, 3), np.uint8)\n         ret, jpg = cv2.imencode(\".jpg\", frame, [int(cv2.IMWRITE_JPEG_QUALITY), 70])"
        }
      ]
    }
  ],
  [
    {
      "cve_id": [
        "CVE-2025-62410",
        "https://github.com/capricorn86/happy-dom/commit/f4bd4ebe3fe5abd2be2bcea1c07043c8b0b70eea"
      ],
      "repo": "happy-dom",
      "commit_hash": "f4bd4ebe3fe5abd2be2bcea1c07043c8b0b70eea",
      "commit_message": "fix: [#0] Adds frozen intrinsics flag to server-renderer workers (#1934)",
      "files_changed": [
        {
          "filename": "packages/@happy-dom/server-renderer/src/ServerRenderer.ts",
          "old_url": "https://raw.githubusercontent.com/capricorn86/happy-dom/f45d92e176acf0232aade63ee4ddac8747252a79/packages/@happy-dom/server-renderer/src/ServerRenderer.ts",
          "new_url": "https://raw.githubusercontent.com/capricorn86/happy-dom/f4bd4ebe3fe5abd2be2bcea1c07043c8b0b70eea/packages/@happy-dom/server-renderer/src/ServerRenderer.ts",
          "diff": "@@ -197,7 +197,7 @@ export default class ServerRenderer {\n \t\t\t\t\treturn,\n \t\t\t\t}\n \t\t\t\tconst worker = new Worker(new URL('ServerRendererWorker.js', import.meta.url), {\n-\t\t\t\t\texecArgv: ['--disallow-code-generation-from-strings'],\n+\t\t\t\t\texecArgv: ['--disallow-code-generation-from-strings', '--frozen-intrinsics'],\n \t\t\t\t\tworkerData: {\n \t\t\t\t\t\tconfiguration: configuration\n \t\t\t\t\t}"
        },
        {
          "filename": "packages/@happy-dom/server-renderer/test/ServerRenderer.test.ts",
          "old_url": "https://raw.githubusercontent.com/capricorn86/happy-dom/f45d92e176acf0232aade63ee4ddac8747252a79/packages/@happy-dom/server-renderer/test/ServerRenderer.test.ts",
          "new_url": "https://raw.githubusercontent.com/capricorn86/happy-dom/f4bd4ebe3fe5abd2be2bcea1c07043c8b0b70eea/packages/@happy-dom/server-renderer/test/ServerRenderer.test.ts",
          "diff": "@@ -58,7 +58,10 @@ describe('ServerRenderer', () => {\n \t\t\t\t\t'file://' + Path.resolve(Path.join('src', 'ServerRendererWorker.js'))\n \t\t\t\t),\n \n-\t\t\t\texpect(worker.execArgv).toEqual(['--disallow-code-generation-from-strings']),\n+\t\t\t\texpect(worker.execArgv).toEqual([\n+\t\t\t\t\t'--disallow-code-generation-from-strings',\n+\t\t\t\t\t'--frozen-intrinsics'\n+\t\t\t\t]),\n \n \t\t\t\texpect(worker.workerData.configuration.cache.directory).toBe(\n \t\t\t\t\tPath.resolve(Path.join('happy-dom', 'cache'))\n@@ -216,7 +219,10 @@ describe('ServerRenderer', () => {\n \t\t\t\t\t'file://' + Path.resolve(Path.join('src', 'ServerRendererWorker.js'))\n \t\t\t\t),\n \n-\t\t\t\texpect(worker.execArgv).toEqual(['--disallow-code-generation-from-strings']),\n+\t\t\t\texpect(worker.execArgv).toEqual([\n+\t\t\t\t\t'--disallow-code-generation-from-strings',\n+\t\t\t\t\t'--frozen-intrinsics'\n+\t\t\t\t]),\n \n \t\t\t\texpect(worker.workerData.configuration.cache.directory).toBe(\n \t\t\t\t\tPath.resolve(Path.join('happy-dom', 'cache'))"
        }
      ]
    }
  ],
  [
    {
      "cve_id": [
        "CVE-2025-62375",
        "https://github.com/in-toto/go-witness/commit/04ff20b600e28ce8fd1aa287534dd383a1cfefb9"
      ],
      "repo": "go-witness",
      "commit_hash": "04ff20b600e28ce8fd1aa287534dd383a1cfefb9",
      "commit_message": "add the current valid AWS certs to valid AWS identity documents (#576)  * add the current valid AWS certs to valid AWS identity documents * fallback to IID region if it's not present in the config ---------  Signed-off-by: John Kjell <john.kjell@control-plane.io>",
      "files_changed": [
        {
          "filename": "Makefile",
          "old_url": "https://raw.githubusercontent.com/in-toto/go-witness/4edc0c53af886b7b246297ce59ebeededaf27d7f/Makefile",
          "new_url": "https://raw.githubusercontent.com/in-toto/go-witness/04ff20b600e28ce8fd1aa287534dd383a1cfefb9/Makefile",
          "diff": "@@ -34,3 +34,7 @@ lint: ## Run the linter\n \t@golangci-lint run\n \t@go fmt ./...\n \t@go vet ./...\n+\n+.PHONY: check-aws-certs\n+check-aws-certs: ## Check the AWS public keys used to verify AWS IID documents\n+\tGOWORK=off go run -C ./attestation/aws-iid/check-certs/ . ../aws-certs.go\n\\ No newline at end of file"
        },
        {
          "filename": "attestation/aws-iid/aws-certs.go",
          "old_url": "https://raw.githubusercontent.com/in-toto/go-witness/4edc0c53af886b7b246297ce59ebeededaf27d7f/attestation/aws-iid/aws-certs.go",
          "new_url": "https://raw.githubusercontent.com/in-toto/go-witness/04ff20b600e28ce8fd1aa287534dd383a1cfefb9/attestation/aws-iid/aws-certs.go",
          "diff": "@@ -0,0 +1,638 @@\n+// Copyright 2025 The Witness Contributors\n+//\n+// Licensed under the Apache License, Version 2.0 (the \"License\"),\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+//\n+//      http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package aws_iid\n+\n+import \"fmt\"\n+\n+func getRegionCert(region string) (string, error) {\n+\tcert, found := awsRegionCerts[region]\n+\tif !found {\n+\t\treturn \"\", fmt.Errorf(\"AWS region %s cert must be provided as input\", region)\n+\t}\n+\n+\treturn cert, nil\n+}\n+\n+// https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/verify-iid.html\n+// There is a different public cert for every AWS region\n+// You can find the one you need for verification here:\n+// https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/regions-certs.html\n+var awsRegionCerts map[string]string = map[string]string{\n+\t\"us-east-1\": `-----BEGIN CERTIFICATE-----\n+MIIDITCCAoqgAwIBAgIUE1y2NIKCU+Rg4uu4u32koG9QEYIwDQYJKoZIhvcNAQEL\n+BQAwXDELMAkGA1UEBhMCVVMxGTAXBgNVBAgTEFdhc2hpbmd0b24gU3RhdGUxEDAO\n+BgNVBAcTB1NlYXR0bGUxIDAeBgNVBAoTF0FtYXpvbiBXZWIgU2VydmljZXMgTExD\n+MB4XDTI0MDQyOTE3MzQwMVoXDTI5MDQyODE3MzQwMVowXDELMAkGA1UEBhMCVVMx\n+GTAXBgNVBAgTEFdhc2hpbmd0b24gU3RhdGUxEDAOBgNVBAcTB1NlYXR0bGUxIDAe\n+BgNVBAoTF0FtYXpvbiBXZWIgU2VydmljZXMgTExDMIGfMA0GCSqGSIb3DQEBAQUA\n+A4GNADCBiQKBgQCHvRjf/0kStpJ248khtIaN8qkDN3tkw4VjvA9nvPl2anJO+eIB\n+UqPfQG09kZlwpWpmyO8bGB2RWqWxCwuB/dcnIob6w420k9WY5C0IIGtDRNauN3ku\n+vGXkw3HEnF0EjYr0pcyWUvByWY4KswZV42X7Y7XSS13hOIcL6NLA+H94/QIDAQAB\n+o4HfMIHcMAsGA1UdDwQEAwIHgDAdBgNVHQ4EFgQUJdbMCBXKtvCcWdwUUizvtUF2\n+UTgwgZkGA1UdIwSBkTCBjoAUJdbMCBXKtvCcWdwUUizvtUF2UTihYKReMFwxCzAJ\n+BgNVBAYTAlVTMRkwFwYDVQQIExBXYXNoaW5ndG9uIFN0YXRlMRAwDgYDVQQHEwdT\n+ZWF0dGxlMSAwHgYDVQQKExdBbWF6b24gV2ViIFNlcnZpY2VzIExMQ4IUE1y2NIKC\n+U+Rg4uu4u32koG9QEYIwEgYDVR0TAQH/BAgwBgEB/wIBADANBgkqhkiG9w0BAQsF\n+AAOBgQAlxSmwcWnhT4uAeSinJuz+1BTcKhVSWb5jT8pYjQb8ZoZkXXRGb09mvYeU\n+NeqOBr27rvRAnaQ/9LUQf72+SahDFuS4CMI8nwowytqbmwquqFr4dxA/SDADyRiF\n+ea1UoMuNHTY49J/1vPomqsVn7mugTp+TbjqCfOJTpu0temHcFA==\n+-----END CERTIFICATE-----`,\n+\n+\t\"us-east-2\": `-----BEGIN CERTIFICATE-----\n+MIIDITCCAoqgAwIBAgIUVJTc+hOU+8Gk3JlqsX438Dk5c58wDQYJKoZIhvcNAQEL\n+BQAwXDELMAkGA1UEBhMCVVMxGTAXBgNVBAgTEFdhc2hpbmd0b24gU3RhdGUxEDAO\n+BgNVBAcTB1NlYXR0bGUxIDAeBgNVBAoTF0FtYXpvbiBXZWIgU2VydmljZXMgTExD\n+MB4XDTI0MDQyOTE3MTE0OVoXDTI5MDQyODE3MTE0OVowXDELMAkGA1UEBhMCVVMx\n+GTAXBgNVBAgTEFdhc2hpbmd0b24gU3RhdGUxEDAOBgNVBAcTB1NlYXR0bGUxIDAe\n+BgNVBAoTF0FtYXpvbiBXZWIgU2VydmljZXMgTExDMIGfMA0GCSqGSIb3DQEBAQUA\n+A4GNADCBiQKBgQCHvRjf/0kStpJ248khtIaN8qkDN3tkw4VjvA9nvPl2anJO+eIB\n+UqPfQG09kZlwpWpmyO8bGB2RWqWxCwuB/dcnIob6w420k9WY5C0IIGtDRNauN3ku\n+vGXkw3HEnF0EjYr0pcyWUvByWY4KswZV42X7Y7XSS13hOIcL6NLA+H94/QIDAQAB\n+o4HfMIHcMAsGA1UdDwQEAwIHgDAdBgNVHQ4EFgQUJdbMCBXKtvCcWdwUUizvtUF2\n+UTgwgZkGA1UdIwSBkTCBjoAUJdbMCBXKtvCcWdwUUizvtUF2UTihYKReMFwxCzAJ\n+BgNVBAYTAlVTMRkwFwYDVQQIExBXYXNoaW5ndG9uIFN0YXRlMRAwDgYDVQQHEwdT\n+ZWF0dGxlMSAwHgYDVQQKExdBbWF6b24gV2ViIFNlcnZpY2VzIExMQ4IUVJTc+hOU\n++8Gk3JlqsX438Dk5c58wEgYDVR0TAQH/BAgwBgEB/wIBADANBgkqhkiG9w0BAQsF\n+AAOBgQAywJQaVNWJqW0R0T0xVOSoN1GLk9x9kKEuN67RN9CLin4dA97qa7Mr5W4P\n+FZ6vnh5CjOhQBRXV9xJUeYSdqVItNAUFK/fEzDdjf1nUfPlQ3OJ49u6CV01NoJ9m\n+usvY9kWcV46dqn2bk2MyfTTgvmeqP8fiMRPxxnVRkSzlldP5Fg==\n+-----END CERTIFICATE-----`,\n+\n+\t\"us-west-1\": `-----BEGIN CERTIFICATE-----\n+MIIDITCCAoqgAwIBAgIUK2zmY9PUSTR7rc1k2OwPYu4+g7wwDQYJKoZIhvcNAQEL\n+BQAwXDELMAkGA1UEBhMCVVMxGTAXBgNVBAgTEFdhc2hpbmd0b24gU3RhdGUxEDAO\n+BgNVBAcTB1NlYXR0bGUxIDAeBgNVBAoTF0FtYXpvbiBXZWIgU2VydmljZXMgTExD\n+MB4XDTI0MDQyOTE3MDI0M1oXDTI5MDQyODE3MDI0M1owXDELMAkGA1UEBhMCVVMx\n+GTAXBgNVBAgTEFdhc2hpbmd0b24gU3RhdGUxEDAOBgNVBAcTB1NlYXR0bGUxIDAe\n+BgNVBAoTF0FtYXpvbiBXZWIgU2VydmljZXMgTExDMIGfMA0GCSqGSIb3DQEBAQUA\n+A4GNADCBiQKBgQCHvRjf/0kStpJ248khtIaN8qkDN3tkw4VjvA9nvPl2anJO+eIB\n+UqPfQG09kZlwpWpmyO8bGB2RWqWxCwuB/dcnIob6w420k9WY5C0IIGtDRNauN3ku\n+vGXkw3HEnF0EjYr0pcyWUvByWY4KswZV42X7Y7XSS13hOIcL6NLA+H94/QIDAQAB\n+o4HfMIHcMAsGA1UdDwQEAwIHgDAdBgNVHQ4EFgQUJdbMCBXKtvCcWdwUUizvtUF2\n+UTgwgZkGA1UdIwSBkTCBjoAUJdbMCBXKtvCcWdwUUizvtUF2UTihYKReMFwxCzAJ\n+BgNVBAYTAlVTMRkwFwYDVQQIExBXYXNoaW5ndG9uIFN0YXRlMRAwDgYDVQQHEwdT\n+ZWF0dGxlMSAwHgYDVQQKExdBbWF6b24gV2ViIFNlcnZpY2VzIExMQ4IUK2zmY9PU\n+STR7rc1k2OwPYu4+g7wwEgYDVR0TAQH/BAgwBgEB/wIBADANBgkqhkiG9w0BAQsF\n+AAOBgQA1Ng4QmN4n7iPh5CnadSOc0ZfM7by0dBePwZJyGvOHdaw6P6E/vEk76KsC\n+Q8p+akuzVzVPkU4kBK/TRqLp19wEWoVwhhTaxHjQ1tTRHqXIVlrkw4JrtFbeNM21\n+GlkSLonuzmNZdivn9WuQYeGe7nUD4w3q9GgiF3CPorJe+UxtbA==\n+-----END CERTIFICATE-----`,\n+\n+\t\"us-west-2\": `-----BEGIN CERTIFICATE-----\n+MIIDITCCAoqgAwIBAgIUFx8PxCkbHwpD31bOyCtyz3GclbgwDQYJKoZIhvcNAQEL\n+BQAwXDELMAkGA1UEBhMCVVMxGTAXBgNVBAgTEFdhc2hpbmd0b24gU3RhdGUxEDAO\n+BgNVBAcTB1NlYXR0bGUxIDAeBgNVBAoTF0FtYXpvbiBXZWIgU2VydmljZXMgTExD\n+MB4XDTI0MDQyOTE3MjM1OVoXDTI5MDQyODE3MjM1OVowXDELMAkGA1UEBhMCVVMx\n+GTAXBgNVBAgTEFdhc2hpbmd0b24gU3RhdGUxEDAOBgNVBAcTB1NlYXR0bGUxIDAe\n+BgNVBAoTF0FtYXpvbiBXZWIgU2VydmljZXMgTExDMIGfMA0GCSqGSIb3DQEBAQUA\n+A4GNADCBiQKBgQCHvRjf/0kStpJ248khtIaN8qkDN3tkw4VjvA9nvPl2anJO+eIB\n+UqPfQG09kZlwpWpmyO8bGB2RWqWxCwuB/dcnIob6w420k9WY5C0IIGtDRNauN3ku\n+vGXkw3HEnF0EjYr0pcyWUvByWY4KswZV42X7Y7XSS13hOIcL6NLA+H94/QIDAQAB\n+o4HfMIHcMAsGA1UdDwQEAwIHgDAdBgNVHQ4EFgQUJdbMCBXKtvCcWdwUUizvtUF2\n+UTgwgZkGA1UdIwSBkTCBjoAUJdbMCBXKtvCcWdwUUizvtUF2UTihYKReMFwxCzAJ\n+BgNVBAYTAlVTMRkwFwYDVQQIExBXYXNoaW5ndG9uIFN0YXRlMRAwDgYDVQQHEwdT\n+ZWF0dGxlMSAwHgYDVQQKExdBbWF6b24gV2ViIFNlcnZpY2VzIExMQ4IUFx8PxCkb\n+HwpD31bOyCtyz3GclbgwEgYDVR0TAQH/BAgwBgEB/wIBADANBgkqhkiG9w0BAQsF\n+AAOBgQBzOl+9Xy1+UsbUBI95HO9mbbdnuX+aMJXgG9uFZNjgNEbMcvx+h8P9IMko\n+z7PzFdheQQ1NLjsHH9mSR1SyC4m9ja6BsejH5nLBWyCdjfdP3muZM4O5+r7vUa1O\n+dWU+hP/T7DUrPAIVMOE7mpYa+WPWJrN6BlRwQkKQ7twm9kDalA==\n+-----END CERTIFICATE-----`,\n+\n+\t\"af-south-1\": `-----BEGIN CERTIFICATE-----\n+MIICNjCCAZ+gAwIBAgIJAKumfZiRrNvHMA0GCSqGSIb3DQEBCwUAMFwxCzAJBgNV\n+BAYTAlVTMRkwFwYDVQQIExBXYXNoaW5ndG9uIFN0YXRlMRAwDgYDVQQHEwdTZWF0\n+dGxlMSAwHgYDVQQKExdBbWF6b24gV2ViIFNlcnZpY2VzIExMQzAgFw0xOTExMjcw\n+NzE0MDVaGA8yMTk5MDUwMjA3MTQwNVowXDELMAkGA1UEBhMCVVMxGTAXBgNVBAgT\n+EFdhc2hpbmd0b24gU3RhdGUxEDAOBgNVBAcTB1NlYXR0bGUxIDAeBgNVBAoTF0Ft\n+YXpvbiBXZWIgU2VydmljZXMgTExDMIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKB\n+gQDFd571nUzVtke3rPyRkYfvs3jh0C0EMzzG72boyUNjnfw1+m0TeFraTLKb9T6F\n+7TuB/ZEN+vmlYqr2+5Va8U8qLbPF0bRH+FdaKjhgWZdYXxGzQzU3ioy5W5ZM1VyB\n+7iUsxEAlxsybC3ziPYaHI42UiTkQNahmoroNeqVyHNnBpQIDAQABMA0GCSqGSIb3\n+DQEBCwUAA4GBAAJLylWyElEgOpW4B1XPyRVD4pAds8Guw2+krgqkY0HxLCdjosuH\n+RytGDGN+q75aAoXzW5a7SGpxLxk6Hfv0xp3RjDHsoeP0i1d8MD3hAC5ezxS4oukK\n+s5gbPOnokhKTMPXbTdRn5ZifCbWlx+bYN/mTYKvxho7b5SVg2o1La9aK\n+-----END CERTIFICATE-----`,\n+\n+\t\"ap-east-1\": `-----BEGIN CERTIFICATE-----\n+MIICSzCCAbQCCQDtQvkVxRvK9TANBgkqhkiG9w0BAQsFADBqMQswCQYDVQQGEwJV\n+UzETMBEGA1UECBMKV2FzaGluZ3RvbjEQMA4GA1UEBxMHU2VhdHRsZTEYMBYGA1UE\n+ChMPQW1hem9uLmNvbSBJbmMuMRowGAYDVQQDExFlYzIuYW1hem9uYXdzLmNvbTAe\n+Fw0xOTAyMDMwMzAwMDZaFw0yOTAyMDIwMzAwMDZaMGoxCzAJBgNVBAYTAlVTMRMw\n+EQYDVQQIEwpXYXNoaW5ndG9uMRAwDgYDVQQHEwdTZWF0dGxlMRgwFgYDVQQKEw9B\n+bWF6b24uY29tIEluYy4xGjAYBgNVBAMTEWVjMi5hbWF6b25hd3MuY29tMIGfMA0G\n+CSqGSIb3DQEBAQUAA4GNADCBiQKBgQC1kkHXYTfc7gY5Q55JJhjTieHAgacaQkiR\n+Pity9QPDE3b+NXDh4UdP1xdIw73JcIIG3sG9RhWiXVCHh6KkuCTqJfPUknIKk8vs\n+M3RXflUpBe8Pf+P92pxqPMCz1Fr2NehS3JhhpkCZVGxxwLC5gaG0Lr4rFORubjYY\n+Rh84dK98VwIDAQABMA0GCSqGSIb3DQEBCwUAA4GBAA6xV9f0HMqXjPHuGILDyaNN\n+dKcvplNFwDTydVg32MNubAGnecoEBtUPtxBsLoVYXCOb+b5/ZMDubPF9tU/vSXuo\n+TpYM5Bq57gJzDRaBOntQbX9bgHiUxw6XZWaTS/6xjRJDT5p3S1E0mPI3lP/eJv4o\n+Ezk5zb3eIf10/sqt4756\n+-----END CERTIFICATE-----`,\n+\n+\t\"ap-south-2\": `-----BEGIN CERTIFICATE-----\n+MIICMzCCAZygAwIBAgIGAXjwLj9CMA0GCSqGSIb3DQEBBQUAMFwxCzAJBgNVBAYTAlVTMRkwFwYDVQQIDBBXYXNoaW5ndG9uIFN0YXRlMRAwDgYDVQQHDAdTZWF0dGxlMSAwHgYDVQQKDBdBbWF6b24gV2ViIFNlcnZpY2VzIExMQzAgFw0yMTA0MjAxNjQ3NDVaGA8yMjAwMDQyMDE2NDc0NVowXDELMAkGA1UEBhMCVVMxGTAXBgNVBAgMEFdhc2hpbmd0b24gU3RhdGUxEDAOBgNVBAcMB1NlYXR0bGUxIDAeBgNVBAoMF0FtYXpvbiBXZWIgU2VydmljZXMgTExDMIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQDTwHu0ND+sFcobrjvcAYm0PNRD8f4R1jAzvoLt2+qGeOTAyO1Httj6cmsYN3AP1hN5iYuppFiYsl2eNPa/CD0Vg0BAfDFlV5rzjpA0j7TJabVh4kj7JvtD+xYMi6wEQA4x6SPONY4OeZ2+8o/HS8nucpWDVdPRO6ciWUlMhjmDmwIDAQABMA0GCSqGSIb3DQEBBQUAA4GBAAy6sgTdRkTqELHBeWj69q60xHyUmsWqHAQNXKVc9ApWGG4onzuqlMbGETwUZ9mTq2vxlV0KvuetCDNS5u4cJsxe/TGGbYP0yP2qfMl0cCImzRI5W0gn8gogdervfeT7nH5ih0TWEy/QDWfkQ601L4erm4yh4YQq8vcqAPSkf04N\n+-----END CERTIFICATE-----`,\n+\n+\t\"ap-southeast-3\": `-----BEGIN CERTIFICATE-----\n+MIICMzCCAZygAwIBAgIGAXbVDG2yMA0GCSqGSIb3DQEBBQUAMFwxCzAJBgNVBAYTAlVTMRkwFwYDVQQIDBBXYXNoaW5ndG9uIFN0YXRlMRAwDgYDVQQHDAdTZWF0dGxlMSAwHgYDVQQKDBdBbWF6b24gV2ViIFNlcnZpY2VzIExMQzAgFw0yMTAxMDYwMDE1MzBaGA8yMjAwMDEwNjAwMTUzMFowXDELMAkGA1UEBhMCVVMxGTAXBgNVBAgMEFdhc2hpbmd0b24gU3RhdGUxEDAOBgNVBAcMB1NlYXR0bGUxIDAeBgNVBAoMF0FtYXpvbiBXZWIgU2VydmljZXMgTExDMIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQCnCS/Vbt0gQ1ebWcur2hSO7PnJifE4OPxQ7RgSAlc4/spJp1sDP+ZrS0LO1ZJfKhXf1R9S3AUwLnsc7b+IuVXdY5LK9RKqu64nyXP5dx170zoL8loEyCSuRR2fs+04i2QsWBVP+KFNAn7P5L1EHRjkgTO8kjNKviwRV+OkP9ab5wIDAQABMA0GCSqGSIb3DQEBBQUAA4GBAI4WUy6+DKh0JDSzQEZNyBgNlSoSuC2owtMxCwGB6nBfzzfcekWvs6eofLTSGovrReX7MtVgrcJBZjmPIentw5dWUs+87w/g9lNwUnUt0ZHYyh2tuBG6hVJuUEwDJ/z3wDd6wQviLOTF3MITawt9P8siR1hXqLJNxpjRQFZrgHqi\n+-----END CERTIFICATE-----`,\n+\n+\t\"ap-southeast-5\": `-----BEGIN CERTIFICATE-----\n+MIICNjCCAZ+gAwIBAgIJAMuBl6rhZCJKMA0GCSqGSIb3DQEBCwUAMFwxCzAJBgNV\n+BAYTAlVTMRkwFwYDVQQIExBXYXNoaW5ndG9uIFN0YXRlMRAwDgYDVQQHEwdTZWF0\n+dGxlMSAwHgYDVQQKExdBbWF6b24gV2ViIFNlcnZpY2VzIExMQzAgFw0yNDAxMDMx\n+MjU3NTRaGA8yMjAzMDYwOTEyNTc1NFowXDELMAkGA1UEBhMCVVMxGTAXBgNVBAgT\n+EFdhc2hpbmd0b24gU3RhdGUxEDAOBgNVBAcTB1NlYXR0bGUxIDAeBgNVBAoTF0Ft\n+YXpvbiBXZWIgU2VydmljZXMgTExDMIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKB\n+gQDFuKydxZsordNH7bLwIluEGOkX7/CdLdpeqkDKEhQkFwzpRxaX4EAlkGh2/o7D\n+8qneC9cGQhqSG5WVVBrmZG7sfkFOM4m1AtY++kfv+MYto1VFgLk1xJbkpq1r4YeQ\n+Ul+ZsJYsZpyX/t+g8s7rW0OVcBsYx4L75bf34z38mwK8PQIDAQABMA0GCSqGSIb3\n+DQEBCwUAA4GBADD9C4pWL8RUvF1CJW8kExj35xmozlFlmrKs8Zpi8+Eg6q+W9dgd\n+xMdH95tgZtmVMDqlvVR+DK0iO1BNpqPjrqWkk2tTLivpS+sGzCE/jCl18Q28Rk71\n+/A3gLD7Rtbq5TKNvuFCHwYMjrTDHI6aBjIaAlDm4e2/j/OxVtHyZGTre\n+-----END CERTIFICATE-----`,\n+\n+\t\"ap-southeast-4\": `-----BEGIN CERTIFICATE-----\n+MIICMzCCAZygAwIBAgIGAXjSh40SMA0GCSqGSIb3DQEBBQUAMFwxCzAJBgNVBAYTAlVTMRkwFwYDVQQIDBBXYXNoaW5ndG9uIFN0YXRlMRAwDgYDVQQHDAdTZWF0dGxlMSAwHgYDVQQKDBdBbWF6b24gV2ViIFNlcnZpY2VzIExMQzAgFw0yMTA0MTQyMjM2NDJaGA8yMjAwMDQxNDIyMzY0MlowXDELMAkGA1UEBhMCVVMxGTAXBgNVBAgMEFdhc2hpbmd0b24gU3RhdGUxEDAOBgNVBAcMB1NlYXR0bGUxIDAeBgNVBAoMF0FtYXpvbiBXZWIgU2VydmljZXMgTExDMIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQDHezwQr2VQpQSTW5TXNefiQrP+qWTGAbGsPeMX4hBMjAJUKys2NIRcRZaLM/BCew2FIPVjNtlaj6Gwn9ipU4Mlz3zIwAMWi1AvGMSreppt+wV6MRtfOjh0Dvj/veJe88aEZJMozNgkJFRS+WFWsckQeL56tf6kY6QTlNo8V/0CsQIDAQABMA0GCSqGSIb3DQEBBQUAA4GBAF7vpPghH0FRo5gu49EArRNPrIvW1egMdZHrzJNqbztLCtV/wcgkqIwwuXYj+1rhlL+/iMpQWjdVGEqIZSeXn5fLmdx50eegFCwND837r9e8XYTiQS143Sxt9+Yi6BZ7U7YD8kK9NBWoJxFqUeHdpRCs0O7COjT3gwm7ZxvAmssh\n+-----END CERTIFICATE-----`,\n+\n+\t\"ap-south-1\": `-----BEGIN CERTIFICATE-----\n+MIIDITCCAoqgAwIBAgIUDLA+x6tTAP3LRTr0z6nOxfsozdMwDQYJKoZIhvcNAQEL\n+BQAwXDELMAkGA1UEBhMCVVMxGTAXBgNVBAgTEFdhc2hpbmd0b24gU3RhdGUxEDAO\n+BgNVBAcTB1NlYXR0bGUxIDAeBgNVBAoTF0FtYXpvbiBXZWIgU2VydmljZXMgTExD\n+MB4XDTI0MDQyOTE0MTMwMVoXDTI5MDQyODE0MTMwMVowXDELMAkGA1UEBhMCVVMx\n+GTAXBgNVBAgTEFdhc2hpbmd0b24gU3RhdGUxEDAOBgNVBAcTB1NlYXR0bGUxIDAe\n+BgNVBAoTF0FtYXpvbiBXZWIgU2VydmljZXMgTExDMIGfMA0GCSqGSIb3DQEBAQUA\n+A4GNADCBiQKBgQCHvRjf/0kStpJ248khtIaN8qkDN3tkw4VjvA9nvPl2anJO+eIB\n+UqPfQG09kZlwpWpmyO8bGB2RWqWxCwuB/dcnIob6w420k9WY5C0IIGtDRNauN3ku\n+vGXkw3HEnF0EjYr0pcyWUvByWY4KswZV42X7Y7XSS13hOIcL6NLA+H94/QIDAQAB\n+o4HfMIHcMAsGA1UdDwQEAwIHgDAdBgNVHQ4EFgQUJdbMCBXKtvCcWdwUUizvtUF2\n+UTgwgZkGA1UdIwSBkTCBjoAUJdbMCBXKtvCcWdwUUizvtUF2UTihYKReMFwxCzAJ\n+BgNVBAYTAlVTMRkwFwYDVQQIExBXYXNoaW5ndG9uIFN0YXRlMRAwDgYDVQQHEwdT\n+ZWF0dGxlMSAwHgYDVQQKExdBbWF6b24gV2ViIFNlcnZpY2VzIExMQ4IUDLA+x6tT\n+AP3LRTr0z6nOxfsozdMwEgYDVR0TAQH/BAgwBgEB/wIBADANBgkqhkiG9w0BAQsF\n+AAOBgQAZ7rYKoAwwiiH1M5GJbrT/BEk3OO2VrEPw8ZxgpqQ/EKlzMlOs/0Cyrmp7\n+UYyUgYFQe5nq37Z94rOUSeMgv/WRxaMwrLlLqD78cuF9DSkXaZIX/kECtVaUnjk8\n+BZx0QhoIHOpQocJUSlm/dLeMuE0+0A3HNR6JVktGsUdv9ulmKw==\n+-----END CERTIFICATE-----`,\n+\n+\t\"ap-southeast-6\": `-----BEGIN CERTIFICATE-----\n+MIICNjCCAZ+gAwIBAgIJAMwLuv2/tk9eMA0GCSqGSIb3DQEBCwUAMFwxCzAJBgNV\n+BAYTAlVTMRkwFwYDVQQIExBXYXNoaW5ndG9uIFN0YXRlMRAwDgYDVQQHEwdTZWF0\n+dGxlMSAwHgYDVQQKExdBbWF6b24gV2ViIFNlcnZpY2VzIExMQzAgFw0yNDEyMDMx\n+MjEzMDZaGA8yMjA0MDUwOTEyMTMwNlowXDELMAkGA1UEBhMCVVMxGTAXBgNVBAgT\n+EFdhc2hpbmd0b24gU3RhdGUxEDAOBgNVBAcTB1NlYXR0bGUxIDAeBgNVBAoTF0Ft\n+YXpvbiBXZWIgU2VydmljZXMgTExDMIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKB\n+gQC9AsBNOUfT8QpguHqMa9KIXP/YgZOmfRAgEmnLawXTC+40bXUq/fwkx+cebv3x\n+CvUx6d07/iDlD0dm2Hf2HhPFi3OaCI7vc8c3lqVBvcn6yzmwv35aSEvtVm2NHC3j\n+4/q6vN2tR2d4a1Mf1QhmOVOJ7XiTEbCy2Yv+pmDbk29V6QIDAQABMA0GCSqGSIb3\n+DQEBCwUAA4GBAHatyQIy612kio8neI4r5VBg8pgKwW+mFGpAzOQ2NOyLCNoOeD1D\n+lPRaSipBN7WW6jpIozbz7JG7Kqc3N+piPGGzfjH9gZjqUqfOGrNHB5nsxvMHI2iL\n+e+DwsOcV7qnEVRgYbmLYOVKJv25wCsM/Tcn/6f6scEX2x+H0hAh7rwiP\n+-----END CERTIFICATE-----`,\n+\n+\t\"ap-northeast-3\": `-----BEGIN CERTIFICATE-----\n+MIIDITCCAoqgAwIBAgIUHTRhxHhBZFOGvTFKxHoy9+f5Hl8wDQYJKoZIhvcNAQEL\n+BQAwXDELMAkGA1UEBhMCVVMxGTAXBgNVBAgTEFdhc2hpbmd0b24gU3RhdGUxEDAO\n+BgNVBAcTB1NlYXR0bGUxIDAeBgNVBAoTF0FtYXpvbiBXZWIgU2VydmljZXMgTExD\n+MB4XDTI0MDQyOTE2NTQwN1oXDTI5MDQyODE2NTQwN1owXDELMAkGA1UEBhMCVVMx\n+GTAXBgNVBAgTEFdhc2hpbmd0b24gU3RhdGUxEDAOBgNVBAcTB1NlYXR0bGUxIDAe\n+BgNVBAoTF0FtYXpvbiBXZWIgU2VydmljZXMgTExDMIGfMA0GCSqGSIb3DQEBAQUA\n+A4GNADCBiQKBgQCHvRjf/0kStpJ248khtIaN8qkDN3tkw4VjvA9nvPl2anJO+eIB\n+UqPfQG09kZlwpWpmyO8bGB2RWqWxCwuB/dcnIob6w420k9WY5C0IIGtDRNauN3ku\n+vGXkw3HEnF0EjYr0pcyWUvByWY4KswZV42X7Y7XSS13hOIcL6NLA+H94/QIDAQAB\n+o4HfMIHcMAsGA1UdDwQEAwIHgDAdBgNVHQ4EFgQUJdbMCBXKtvCcWdwUUizvtUF2\n+UTgwgZkGA1UdIwSBkTCBjoAUJdbMCBXKtvCcWdwUUizvtUF2UTihYKReMFwxCzAJ\n+BgNVBAYTAlVTMRkwFwYDVQQIExBXYXNoaW5ndG9uIFN0YXRlMRAwDgYDVQQHEwdT\n+ZWF0dGxlMSAwHgYDVQQKExdBbWF6b24gV2ViIFNlcnZpY2VzIExMQ4IUHTRhxHhB\n+ZFOGvTFKxHoy9+f5Hl8wEgYDVR0TAQH/BAgwBgEB/wIBADANBgkqhkiG9w0BAQsF\n+AAOBgQAUZx7DcYbhWNTD4BNGhr5beruT2OUoGHH9J73UKxwdqeb9bH1LIWhIZO0X\n+/1mjn3bWBgCwfoS8gjZwsVB6fZbNBRy8urdBZJ87xF/4JPBjt7S9oGx/zthDUYrC\n+yK0Y0v4G0PgiS81CvYLg09LpmYhLSJbXENlkC04v5yxdKxZxyg==\n+-----END CERTIFICATE-----`,\n+\n+\t\"ap-northeast-2\": `-----BEGIN CERTIFICATE-----\n+MIIDITCCAoqgAwIBAgIUbBSn2UIO6vYk4iNWV0RPxJJtHlgwDQYJKoZIhvcNAQEL\n+BQAwXDELMAkGA1UEBhMCVVMxGTAXBgNVBAgTEFdhc2hpbmd0b24gU3RhdGUxEDAO\n+BgNVBAcTB1NlYXR0bGUxIDAeBgNVBAoTF0FtYXpvbiBXZWIgU2VydmljZXMgTExD\n+MB4XDTI0MDQyOTEzMzg0NloXDTI5MDQyODEzMzg0NlowXDELMAkGA1UEBhMCVVMx\n+GTAXBgNVBAgTEFdhc2hpbmd0b24gU3RhdGUxEDAOBgNVBAcTB1NlYXR0bGUxIDAe\n+BgNVBAoTF0FtYXpvbiBXZWIgU2VydmljZXMgTExDMIGfMA0GCSqGSIb3DQEBAQUA\n+A4GNADCBiQKBgQCHvRjf/0kStpJ248khtIaN8qkDN3tkw4VjvA9nvPl2anJO+eIB\n+UqPfQG09kZlwpWpmyO8bGB2RWqWxCwuB/dcnIob6w420k9WY5C0IIGtDRNauN3ku\n+vGXkw3HEnF0EjYr0pcyWUvByWY4KswZV42X7Y7XSS13hOIcL6NLA+H94/QIDAQAB\n+o4HfMIHcMAsGA1UdDwQEAwIHgDAdBgNVHQ4EFgQUJdbMCBXKtvCcWdwUUizvtUF2\n+UTgwgZkGA1UdIwSBkTCBjoAUJdbMCBXKtvCcWdwUUizvtUF2UTihYKReMFwxCzAJ\n+BgNVBAYTAlVTMRkwFwYDVQQIExBXYXNoaW5ndG9uIFN0YXRlMRAwDgYDVQQHEwdT\n+ZWF0dGxlMSAwHgYDVQQKExdBbWF6b24gV2ViIFNlcnZpY2VzIExMQ4IUbBSn2UIO\n+6vYk4iNWV0RPxJJtHlgwEgYDVR0TAQH/BAgwBgEB/wIBADANBgkqhkiG9w0BAQsF\n+AAOBgQAmjTjalG8MGLqWTC2uYqEM8nzI3px1eo0ArvFRsyqQ3fgmWcQpxExqUqRy\n+l3+2134Kv8dFab04Gut5wlfRtc2OwPKKicmv/IXGN+9bKFnQFjTqif08NIzrDZch\n+aFT/uvxrIiM+oN2YsHq66GUhO2+xVRXDXVxM/VObFgPERbJpyA==\n+-----END CERTIFICATE-----`,\n+\n+\t\"ap-southeast-1\": `-----BEGIN CERTIFICATE-----\n+MIIDITCCAoqgAwIBAgIUSqP6ih+++5KF07NXngrWf26mhSUwDQYJKoZIhvcNAQEL\n+BQAwXDELMAkGA1UEBhMCVVMxGTAXBgNVBAgTEFdhc2hpbmd0b24gU3RhdGUxEDAO\n+BgNVBAcTB1NlYXR0bGUxIDAeBgNVBAoTF0FtYXpvbiBXZWIgU2VydmljZXMgTExD\n+MB4XDTI0MDQyOTE0MzAxNFoXDTI5MDQyODE0MzAxNFowXDELMAkGA1UEBhMCVVMx\n+GTAXBgNVBAgTEFdhc2hpbmd0b24gU3RhdGUxEDAOBgNVBAcTB1NlYXR0bGUxIDAe\n+BgNVBAoTF0FtYXpvbiBXZWIgU2VydmljZXMgTExDMIGfMA0GCSqGSIb3DQEBAQUA\n+A4GNADCBiQKBgQCHvRjf/0kStpJ248khtIaN8qkDN3tkw4VjvA9nvPl2anJO+eIB\n+UqPfQG09kZlwpWpmyO8bGB2RWqWxCwuB/dcnIob6w420k9WY5C0IIGtDRNauN3ku\n+vGXkw3HEnF0EjYr0pcyWUvByWY4KswZV42X7Y7XSS13hOIcL6NLA+H94/QIDAQAB\n+o4HfMIHcMAsGA1UdDwQEAwIHgDAdBgNVHQ4EFgQUJdbMCBXKtvCcWdwUUizvtUF2\n+UTgwgZkGA1UdIwSBkTCBjoAUJdbMCBXKtvCcWdwUUizvtUF2UTihYKReMFwxCzAJ\n+BgNVBAYTAlVTMRkwFwYDVQQIExBXYXNoaW5ndG9uIFN0YXRlMRAwDgYDVQQHEwdT\n+ZWF0dGxlMSAwHgYDVQQKExdBbWF6b24gV2ViIFNlcnZpY2VzIExMQ4IUSqP6ih++\n++5KF07NXngrWf26mhSUwEgYDVR0TAQH/BAgwBgEB/wIBADANBgkqhkiG9w0BAQsF\n+AAOBgQAw13BxW11U/JL58j//Fmk7qqtrZTqXmaz1qm2WlIpJpW750MOcP4ux1uPy\n+eM0RdVZ4jHSMv5gtLAv/PjExBfw9n6vNCk+5GZG4Xec5DoapBZHXmfMo93sjxBFP\n+4x9rWn0GuwAVO9ukjYPevq2Rerilrq5VvppHtbATVNY2qecXDA==\n+-----END CERTIFICATE-----`,\n+\n+\t\"ap-southeast-2\": `-----BEGIN CERTIFICATE-----\n+MIIDITCCAoqgAwIBAgIUFxWyAdk4oiXIOC9PxcgjYYh71mwwDQYJKoZIhvcNAQEL\n+BQAwXDELMAkGA1UEBhMCVVMxGTAXBgNVBAgTEFdhc2hpbmd0b24gU3RhdGUxEDAO\n+BgNVBAcTB1NlYXR0bGUxIDAeBgNVBAoTF0FtYXpvbiBXZWIgU2VydmljZXMgTExD\n+MB4XDTI0MDQyOTE1MjE0M1oXDTI5MDQyODE1MjE0M1owXDELMAkGA1UEBhMCVVMx\n+GTAXBgNVBAgTEFdhc2hpbmd0b24gU3RhdGUxEDAOBgNVBAcTB1NlYXR0bGUxIDAe\n+BgNVBAoTF0FtYXpvbiBXZWIgU2VydmljZXMgTExDMIGfMA0GCSqGSIb3DQEBAQUA\n+A4GNADCBiQKBgQCHvRjf/0kStpJ248khtIaN8qkDN3tkw4VjvA9nvPl2anJO+eIB\n+UqPfQG09kZlwpWpmyO8bGB2RWqWxCwuB/dcnIob6w420k9WY5C0IIGtDRNauN3ku\n+vGXkw3HEnF0EjYr0pcyWUvByWY4KswZV42X7Y7XSS13hOIcL6NLA+H94/QIDAQAB\n+o4HfMIHcMAsGA1UdDwQEAwIHgDAdBgNVHQ4EFgQUJdbMCBXKtvCcWdwUUizvtUF2\n+UTgwgZkGA1UdIwSBkTCBjoAUJdbMCBXKtvCcWdwUUizvtUF2UTihYKReMFwxCzAJ\n+BgNVBAYTAlVTMRkwFwYDVQQIExBXYXNoaW5ndG9uIFN0YXRlMRAwDgYDVQQHEwdT\n+ZWF0dGxlMSAwHgYDVQQKExdBbWF6b24gV2ViIFNlcnZpY2VzIExMQ4IUFxWyAdk4\n+oiXIOC9PxcgjYYh71mwwEgYDVR0TAQH/BAgwBgEB/wIBADANBgkqhkiG9w0BAQsF\n+AAOBgQByjeQe6lr7fiIhoGdjBXYzDfkX0lGGvMIhRh57G1bbceQfaYdZd7Ptc0jl\n+bpycKGaTvhUdkpMOiV2Hi9dOOYawkdhyJDstmDNKu6P9+b6Kak8He5z3NU1tUR2Y\n+uTwcz7Ye8Nldx//ws3raErfTI7D6s9m63OX8cAJ/f8bNgikwpw==\n+-----END CERTIFICATE-----`,\n+\n+\t\"ap-east-2\": `-----BEGIN CERTIFICATE-----\n+MIICNjCCAZ+gAwIBAgIJAJp99iy2CTXhMA0GCSqGSIb3DQEBCwUAMFwxCzAJBgNV\n+BAYTAlVTMRkwFwYDVQQIExBXYXNoaW5ndG9uIFN0YXRlMRAwDgYDVQQHEwdTZWF0\n+dGxlMSAwHgYDVQQKExdBbWF6b24gV2ViIFNlcnZpY2VzIExMQzAgFw0yNDA1MjQx\n+NDU2MDZaGA8yMjAzMTAyOTE0NTYwNlowXDELMAkGA1UEBhMCVVMxGTAXBgNVBAgT\n+EFdhc2hpbmd0b24gU3RhdGUxEDAOBgNVBAcTB1NlYXR0bGUxIDAeBgNVBAoTF0Ft\n+YXpvbiBXZWIgU2VydmljZXMgTExDMIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKB\n+gQDVdclgpcIIsyGG51ag8srK1UMfZ+Ioh9E4PLP+cIWQcdh31fv2kV/whKaqfmif\n+NefBIuHrTPl6L+mJQbsfeA+TiEx1Jamg4lwQfhgQKQCmjZVMaHtK+AY57TVN6FBf\n+HXxT2MXzktsKxAQRGeSTFQYrHRzSYvu+GkA+37lqyWRfcwIDAQABMA0GCSqGSIb3\n+DQEBCwUAA4GBABM+WL9MSb7z3bNOTMkM7ufPucvj+LQ+zEAJiuG0rcB8GI2DhGB5\n+8ohDPUGlq1cP7vYmDkWG/FrLqx7qgIzkbE3zfb62JpdOuX73oHgCmikMfq8ifoS1\n+CUxMQ36PDZBFNPzGZxv5kEORqhmGBkYCoIUCQ43jh3aALR/A/rfzfij8\n+-----END CERTIFICATE-----`,\n+\n+\t\"ap-southeast-7\": `-----BEGIN CERTIFICATE-----\n+MIICNjCCAZ+gAwIBAgIJAIuIHAhL0xWcMA0GCSqGSIb3DQEBCwUAMFwxCzAJBgNV\n+BAYTAlVTMRkwFwYDVQQIExBXYXNoaW5ndG9uIFN0YXRlMRAwDgYDVQQHEwdTZWF0\n+dGxlMSAwHgYDVQQKExdBbWF6b24gV2ViIFNlcnZpY2VzIExMQzAgFw0yNDA0MTIx\n+NTI4NTZaGA8yMjAzMDkxNzE1Mjg1NlowXDELMAkGA1UEBhMCVVMxGTAXBgNVBAgT\n+EFdhc2hpbmd0b24gU3RhdGUxEDAOBgNVBAcTB1NlYXR0bGUxIDAeBgNVBAoTF0Ft\n+YXpvbiBXZWIgU2VydmljZXMgTExDMIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKB\n+gQCajgAeOauwvqGDLrHvxujnZ1BnkMzwjrycMUTkj8jqNtWoDQWUJVNPZJILosEU\n+VwK2I3oNkEsx/ryl9XfXcNNceoYfVEPzkTzozrZyuOG66FWtUU1LKeJ7h9/rX0Zd\n+9lZEokrdr6dLPt9FsHWaK5ExlUnWBjNltcQLkkKqoeYaFwIDAQABMA0GCSqGSIb3\n+DQEBCwUAA4GBAE4G5G+FvKTsX3T7BEcId7f5LSCc2J9gZRDiWn2oTr40CrBMOzJT\n+KsWr9W89YXW3gaGWltzc0WCwYQbJZgAkuEAZItJjbhdnns87ZbsFO+NZhc6gDtjA\n+WC3dPlSB9b6rfVoVW9O6Xwa7iNXZo8ddYVJ/ZOIv/totUz9qJt4DmmKk\n+-----END CERTIFICATE-----`,\n+\n+\t\"ap-northeast-1\": `-----BEGIN CERTIFICATE-----\n+MIIDITCCAoqgAwIBAgIULgwDh7TiDrPPBJwscqDwiBHkEFQwDQYJKoZIhvcNAQEL\n+BQAwXDELMAkGA1UEBhMCVVMxGTAXBgNVBAgTEFdhc2hpbmd0b24gU3RhdGUxEDAO\n+BgNVBAcTB1NlYXR0bGUxIDAeBgNVBAoTF0FtYXpvbiBXZWIgU2VydmljZXMgTExD\n+MB4XDTI0MDQyOTEyMjMxMFoXDTI5MDQyODEyMjMxMFowXDELMAkGA1UEBhMCVVMx\n+GTAXBgNVBAgTEFdhc2hpbmd0b24gU3RhdGUxEDAOBgNVBAcTB1NlYXR0bGUxIDAe\n+BgNVBAoTF0FtYXpvbiBXZWIgU2VydmljZXMgTExDMIGfMA0GCSqGSIb3DQEBAQUA\n+A4GNADCBiQKBgQCHvRjf/0kStpJ248khtIaN8qkDN3tkw4VjvA9nvPl2anJO+eIB\n+UqPfQG09kZlwpWpmyO8bGB2RWqWxCwuB/dcnIob6w420k9WY5C0IIGtDRNauN3ku\n+vGXkw3HEnF0EjYr0pcyWUvByWY4KswZV42X7Y7XSS13hOIcL6NLA+H94/QIDAQAB\n+o4HfMIHcMAsGA1UdDwQEAwIHgDAdBgNVHQ4EFgQUJdbMCBXKtvCcWdwUUizvtUF2\n+UTgwgZkGA1UdIwSBkTCBjoAUJdbMCBXKtvCcWdwUUizvtUF2UTihYKReMFwxCzAJ\n+BgNVBAYTAlVTMRkwFwYDVQQIExBXYXNoaW5ndG9uIFN0YXRlMRAwDgYDVQQHEwdT\n+ZWF0dGxlMSAwHgYDVQQKExdBbWF6b24gV2ViIFNlcnZpY2VzIExMQ4IULgwDh7Ti\n+DrPPBJwscqDwiBHkEFQwEgYDVR0TAQH/BAgwBgEB/wIBADANBgkqhkiG9w0BAQsF\n+AAOBgQBtjAglBde1t4F9EHCZOj4qnY6Gigy07Ou54i+lR77MhbpzE8V28Li9l+YT\n+QMIn6SzJqU3/fIycIro1OVY1lHmaKYgPGSEZxBenSBHfzwDLRmC9oRp4QMe0BjOC\n+gepj1lUoiN7OA6PtA+ycNlsP0oJvdBjhvayLiuM3tUfLTrgHbw==\n+-----END CERTIFICATE-----`,\n+\n+\t\"ca-central-1\": `-----BEGIN CERTIFICATE-----\n+MIIDITCCAoqgAwIBAgIUIrLgixJJB5C4G8z6pZ5rB0JU2aQwDQYJKoZIhvcNAQEL\n+BQAwXDELMAkGA1UEBhMCVVMxGTAXBgNVBAgTEFdhc2hpbmd0b24gU3RhdGUxEDAO\n+BgNVBAcTB1NlYXR0bGUxIDAeBgNVBAoTF0FtYXpvbiBXZWIgU2VydmljZXMgTExD\n+MB4XDTI0MDQyOTE1MzU0M1oXDTI5MDQyODE1MzU0M1owXDELMAkGA1UEBhMCVVMx\n+GTAXBgNVBAgTEFdhc2hpbmd0b24gU3RhdGUxEDAOBgNVBAcTB1NlYXR0bGUxIDAe\n+BgNVBAoTF0FtYXpvbiBXZWIgU2VydmljZXMgTExDMIGfMA0GCSqGSIb3DQEBAQUA\n+A4GNADCBiQKBgQCHvRjf/0kStpJ248khtIaN8qkDN3tkw4VjvA9nvPl2anJO+eIB\n+UqPfQG09kZlwpWpmyO8bGB2RWqWxCwuB/dcnIob6w420k9WY5C0IIGtDRNauN3ku\n+vGXkw3HEnF0EjYr0pcyWUvByWY4KswZV42X7Y7XSS13hOIcL6NLA+H94/QIDAQAB\n+o4HfMIHcMAsGA1UdDwQEAwIHgDAdBgNVHQ4EFgQUJdbMCBXKtvCcWdwUUizvtUF2\n+UTgwgZkGA1UdIwSBkTCBjoAUJdbMCBXKtvCcWdwUUizvtUF2UTihYKReMFwxCzAJ\n+BgNVBAYTAlVTMRkwFwYDVQQIExBXYXNoaW5ndG9uIFN0YXRlMRAwDgYDVQQHEwdT\n+ZWF0dGxlMSAwHgYDVQQKExdBbWF6b24gV2ViIFNlcnZpY2VzIExMQ4IUIrLgixJJ\n+B5C4G8z6pZ5rB0JU2aQwEgYDVR0TAQH/BAgwBgEB/wIBADANBgkqhkiG9w0BAQsF\n+AAOBgQBHiQJmzyFAaSYs8SpiRijIDZW2RIo7qBKb/pI3rqK6yOWDlPuMr6yNI81D\n+IrKGGftg4Z+2KETYU4x76HSf0s//vfH3QA57qFaAwddhKYy4BhteFQl/Wex3xTlX\n+LiwI07kwJvJy3mS6UfQ4HcvZy219tY+0iyOWrz/jVxwq7TOkCw==\n+-----END CERTIFICATE-----`,\n+\n+\t\"ca-west-1\": `-----BEGIN CERTIFICATE-----\n+MIICMzCCAZygAwIBAgIGAYPou9weMA0GCSqGSIb3DQEBBQUAMFwxCzAJBgNVBAYTAlVTMRkwFwYDVQQIDBBXYXNoaW5ndG9uIFN0YXRlMRAwDgYDVQQHDAdTZWF0dGxlMSAwHgYDVQQKDBdBbWF6b24gV2ViIFNlcnZpY2VzIExMQzAgFw0yMjEwMTgwMTM2MDlaGA8yMjAxMTAxODAxMzYwOVowXDELMAkGA1UEBhMCVVMxGTAXBgNVBAgMEFdhc2hpbmd0b24gU3RhdGUxEDAOBgNVBAcMB1NlYXR0bGUxIDAeBgNVBAoMF0FtYXpvbiBXZWIgU2VydmljZXMgTExDMIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQDK1kIcG5Q6adBXQM75GldfTSiXl7tn54p10TnspI0ErDdb2B6q2Ji/v4XBVH13ZCMgqlRHMqV8AWI5iO6gFn2A9sN3AZXTMqwtZeiDdebq3k6Wt7ieYvpXTg0qvgsjQIovRZWaBDBJy9x8C2hW+w9lMQjFHkJ7Jy/PHCJ69EzebQIDAQABMA0GCSqGSIb3DQEBBQUAA4GBAGe9Snkz1A6rHBH6/5kDtYvtPYwhx2sXNxztbhkXErFk40Nw5l459NZxEeudxJBLoCkkSgYjhRcOZ/gvDVtWG7qyb6fAqgoisyAbk8K9LzxSim2S1nmT9vD84B/t/VvwQBylc+ej8kRxMH7fquZLp7IXfmtBzyUqu6Dpbne+chG2\n+-----END CERTIFICATE-----`,\n+\n+\t\"cn-north-1\": `-----BEGIN CERTIFICATE-----\n+MIIDCzCCAnSgAwIBAgIJALSOMbOoU2svMA0GCSqGSIb3DQEBCwUAMFwxCzAJBgNV\n+BAYTAlVTMRkwFwYDVQQIExBXYXNoaW5ndG9uIFN0YXRlMRAwDgYDVQQHEwdTZWF0\n+dGxlMSAwHgYDVQQKExdBbWF6b24gV2ViIFNlcnZpY2VzIExMQzAeFw0yMzA3MDQw\n+ODM1MzlaFw0yODA3MDIwODM1MzlaMFwxCzAJBgNVBAYTAlVTMRkwFwYDVQQIExBX\n+YXNoaW5ndG9uIFN0YXRlMRAwDgYDVQQHEwdTZWF0dGxlMSAwHgYDVQQKExdBbWF6\n+b24gV2ViIFNlcnZpY2VzIExMQzCBnzANBgkqhkiG9w0BAQEFAAOBjQAwgYkCgYEA\n+uhhUNlqAZdcWWB/OSDVDGk3OA99EFzOn/mJlmciQ/Xwu2dFJWmSCqEAE6gjufCjQ\n+q3voxAhC2CF+elKtJW/C0Sz/LYo60PUqd6iXF4h+upB9HkOOGuWHXsHBTsvgkgGA\n+1CGgel4U0Cdq+23eANr8N8m28UzljjSnTlrYCHtzN4sCAwEAAaOB1DCB0TALBgNV\n+HQ8EBAMCB4AwHQYDVR0OBBYEFBkZu3wT27NnYgrfH+xJz4HJaNJoMIGOBgNVHSME\n+gYYwgYOAFBkZu3wT27NnYgrfH+xJz4HJaNJooWCkXjBcMQswCQYDVQQGEwJVUzEZ\n+MBcGA1UECBMQV2FzaGluZ3RvbiBTdGF0ZTEQMA4GA1UEBxMHU2VhdHRsZTEgMB4G\n+A1UEChMXQW1hem9uIFdlYiBTZXJ2aWNlcyBMTEOCCQC0jjGzqFNrLzASBgNVHRMB\n+Af8ECDAGAQH/AgEAMA0GCSqGSIb3DQEBCwUAA4GBAECji43p+oPkYqmzll7e8Hgb\n+oADS0ph+YUz5P/bUCm61wFjlxaTfwKcuTR3ytj7bFLoW5Bm7Sa+TCl3lOGb2taon\n+2h+9NirRK6JYk87LMNvbS40HGPFumJL2NzEsGUeK+MRiWu+Oh5/lJGii3qw4YByx\n+SUDlRyNy1jJFstEZjOhs\n+-----END CERTIFICATE-----`,\n+\n+\t\"cn-northwest-1\": `-----BEGIN CERTIFICATE-----\n+MIIDCzCCAnSgAwIBAgIJALSOMbOoU2svMA0GCSqGSIb3DQEBCwUAMFwxCzAJBgNV\n+BAYTAlVTMRkwFwYDVQQIExBXYXNoaW5ndG9uIFN0YXRlMRAwDgYDVQQHEwdTZWF0\n+dGxlMSAwHgYDVQQKExdBbWF6b24gV2ViIFNlcnZpY2VzIExMQzAeFw0yMzA3MDQw\n+ODM1MzlaFw0yODA3MDIwODM1MzlaMFwxCzAJBgNVBAYTAlVTMRkwFwYDVQQIExBX\n+YXNoaW5ndG9uIFN0YXRlMRAwDgYDVQQHEwdTZWF0dGxlMSAwHgYDVQQKExdBbWF6\n+b24gV2ViIFNlcnZpY2VzIExMQzCBnzANBgkqhkiG9w0BAQEFAAOBjQAwgYkCgYEA\n+uhhUNlqAZdcWWB/OSDVDGk3OA99EFzOn/mJlmciQ/Xwu2dFJWmSCqEAE6gjufCjQ\n+q3voxAhC2CF+elKtJW/C0Sz/LYo60PUqd6iXF4h+upB9HkOOGuWHXsHBTsvgkgGA\n+1CGgel4U0Cdq+23eANr8N8m28UzljjSnTlrYCHtzN4sCAwEAAaOB1DCB0TALBgNV\n+HQ8EBAMCB4AwHQYDVR0OBBYEFBkZu3wT27NnYgrfH+xJz4HJaNJoMIGOBgNVHSME\n+gYYwgYOAFBkZu3wT27NnYgrfH+xJz4HJaNJooWCkXjBcMQswCQYDVQQGEwJVUzEZ\n+MBcGA1UECBMQV2FzaGluZ3RvbiBTdGF0ZTEQMA4GA1UEBxMHU2VhdHRsZTEgMB4G\n+A1UEChMXQW1hem9uIFdlYiBTZXJ2aWNlcyBMTEOCCQC0jjGzqFNrLzASBgNVHRMB\n+Af8ECDAGAQH/AgEAMA0GCSqGSIb3DQEBCwUAA4GBAECji43p+oPkYqmzll7e8Hgb\n+oADS0ph+YUz5P/bUCm61wFjlxaTfwKcuTR3ytj7bFLoW5Bm7Sa+TCl3lOGb2taon\n+2h+9NirRK6JYk87LMNvbS40HGPFumJL2NzEsGUeK+MRiWu+Oh5/lJGii3qw4YByx\n+SUDlRyNy1jJFstEZjOhs\n+-----END CERTIFICATE-----`,\n+\n+\t\"eu-central-1\": `-----BEGIN CERTIFICATE-----\n+MIIDITCCAoqgAwIBAgIUFD5GsmkxRuecttwsCG763m3u63UwDQYJKoZIhvcNAQEL\n+BQAwXDELMAkGA1UEBhMCVVMxGTAXBgNVBAgTEFdhc2hpbmd0b24gU3RhdGUxEDAO\n+BgNVBAcTB1NlYXR0bGUxIDAeBgNVBAoTF0FtYXpvbiBXZWIgU2VydmljZXMgTExD\n+MB4XDTI0MDQyOTE1NTUyOVoXDTI5MDQyODE1NTUyOVowXDELMAkGA1UEBhMCVVMx\n+GTAXBgNVBAgTEFdhc2hpbmd0b24gU3RhdGUxEDAOBgNVBAcTB1NlYXR0bGUxIDAe\n+BgNVBAoTF0FtYXpvbiBXZWIgU2VydmljZXMgTExDMIGfMA0GCSqGSIb3DQEBAQUA\n+A4GNADCBiQKBgQCHvRjf/0kStpJ248khtIaN8qkDN3tkw4VjvA9nvPl2anJO+eIB\n+UqPfQG09kZlwpWpmyO8bGB2RWqWxCwuB/dcnIob6w420k9WY5C0IIGtDRNauN3ku\n+vGXkw3HEnF0EjYr0pcyWUvByWY4KswZV42X7Y7XSS13hOIcL6NLA+H94/QIDAQAB\n+o4HfMIHcMAsGA1UdDwQEAwIHgDAdBgNVHQ4EFgQUJdbMCBXKtvCcWdwUUizvtUF2\n+UTgwgZkGA1UdIwSBkTCBjoAUJdbMCBXKtvCcWdwUUizvtUF2UTihYKReMFwxCzAJ\n+BgNVBAYTAlVTMRkwFwYDVQQIExBXYXNoaW5ndG9uIFN0YXRlMRAwDgYDVQQHEwdT\n+ZWF0dGxlMSAwHgYDVQQKExdBbWF6b24gV2ViIFNlcnZpY2VzIExMQ4IUFD5Gsmkx\n+RuecttwsCG763m3u63UwEgYDVR0TAQH/BAgwBgEB/wIBADANBgkqhkiG9w0BAQsF\n+AAOBgQBBh0WaXlBsW56Hqk588MmJxsOrvcKfDjF57RgEDgnGnQaJcStCVWDO9UYO\n+JX2tdsPw+E7AjDqjsuxYaotLn3Mr3mK0sNOXq9BljBnWD4pARg89KZnZI8FN35HQ\n+O/LYOVHCknuPL123VmVRNs51qQA9hkPjvw21UzpDLxaUxt9Z/w==\n+-----END CERTIFICATE-----`,\n+\n+\t\"eu-west-1\": `-----BEGIN CERTIFICATE-----\n+MIIDITCCAoqgAwIBAgIUakDaQ1Zqy87Hy9ESXA1pFC116HkwDQYJKoZIhvcNAQEL\n+BQAwXDELMAkGA1UEBhMCVVMxGTAXBgNVBAgTEFdhc2hpbmd0b24gU3RhdGUxEDAO\n+BgNVBAcTB1NlYXR0bGUxIDAeBgNVBAoTF0FtYXpvbiBXZWIgU2VydmljZXMgTExD\n+MB4XDTI0MDQyOTE2MTgxMFoXDTI5MDQyODE2MTgxMFowXDELMAkGA1UEBhMCVVMx\n+GTAXBgNVBAgTEFdhc2hpbmd0b24gU3RhdGUxEDAOBgNVBAcTB1NlYXR0bGUxIDAe\n+BgNVBAoTF0FtYXpvbiBXZWIgU2VydmljZXMgTExDMIGfMA0GCSqGSIb3DQEBAQUA\n+A4GNADCBiQKBgQCHvRjf/0kStpJ248khtIaN8qkDN3tkw4VjvA9nvPl2anJO+eIB\n+UqPfQG09kZlwpWpmyO8bGB2RWqWxCwuB/dcnIob6w420k9WY5C0IIGtDRNauN3ku\n+vGXkw3HEnF0EjYr0pcyWUvByWY4KswZV42X7Y7XSS13hOIcL6NLA+H94/QIDAQAB\n+o4HfMIHcMAsGA1UdDwQEAwIHgDAdBgNVHQ4EFgQUJdbMCBXKtvCcWdwUUizvtUF2\n+UTgwgZkGA1UdIwSBkTCBjoAUJdbMCBXKtvCcWdwUUizvtUF2UTihYKReMFwxCzAJ\n+BgNVBAYTAlVTMRkwFwYDVQQIExBXYXNoaW5ndG9uIFN0YXRlMRAwDgYDVQQHEwdT\n+ZWF0dGxlMSAwHgYDVQQKExdBbWF6b24gV2ViIFNlcnZpY2VzIExMQ4IUakDaQ1Zq\n+y87Hy9ESXA1pFC116HkwEgYDVR0TAQH/BAgwBgEB/wIBADANBgkqhkiG9w0BAQsF\n+AAOBgQADIkn/MqaLGPuK5+prZZ5Ox4bBZLPtreO2C7r0pqU2kPM2lVPyYYydkvP0\n+lgSmmsErGu/oL9JNztDe2oCA+kNy17ehcsf8cw0uP861czNFKCeU8b7FgBbL+sIm\n+qi33rAq6owWGi/5uEcfCR+JP7W+oSYVir5r/yDmWzx+BVH5S/g==\n+-----END CERTIFICATE-----`,\n+\n+\t\"eu-west-2\": `-----BEGIN CERTIFICATE-----\n+MIIDITCCAoqgAwIBAgIUCgCV/DPxYNND/swDgEKGiC5I+EwwDQYJKoZIhvcNAQEL\n+BQAwXDELMAkGA1UEBhMCVVMxGTAXBgNVBAgTEFdhc2hpbmd0b24gU3RhdGUxEDAO\n+BgNVBAcTB1NlYXR0bGUxIDAeBgNVBAoTF0FtYXpvbiBXZWIgU2VydmljZXMgTExD\n+MB4XDTI0MDQyOTE2MjkxNFoXDTI5MDQyODE2MjkxNFowXDELMAkGA1UEBhMCVVMx\n+GTAXBgNVBAgTEFdhc2hpbmd0b24gU3RhdGUxEDAOBgNVBAcTB1NlYXR0bGUxIDAe\n+BgNVBAoTF0FtYXpvbiBXZWIgU2VydmljZXMgTExDMIGfMA0GCSqGSIb3DQEBAQUA\n+A4GNADCBiQKBgQCHvRjf/0kStpJ248khtIaN8qkDN3tkw4VjvA9nvPl2anJO+eIB\n+UqPfQG09kZlwpWpmyO8bGB2RWqWxCwuB/dcnIob6w420k9WY5C0IIGtDRNauN3ku\n+vGXkw3HEnF0EjYr0pcyWUvByWY4KswZV42X7Y7XSS13hOIcL6NLA+H94/QIDAQAB\n+o4HfMIHcMAsGA1UdDwQEAwIHgDAdBgNVHQ4EFgQUJdbMCBXKtvCcWdwUUizvtUF2\n+UTgwgZkGA1UdIwSBkTCBjoAUJdbMCBXKtvCcWdwUUizvtUF2UTihYKReMFwxCzAJ\n+BgNVBAYTAlVTMRkwFwYDVQQIExBXYXNoaW5ndG9uIFN0YXRlMRAwDgYDVQQHEwdT\n+ZWF0dGxlMSAwHgYDVQQKExdBbWF6b24gV2ViIFNlcnZpY2VzIExMQ4IUCgCV/DPx\n+YNND/swDgEKGiC5I+EwwEgYDVR0TAQH/BAgwBgEB/wIBADANBgkqhkiG9w0BAQsF\n+AAOBgQATPu/sOE2esNa4+XPEGKlEJSgqzyBSQLQc+VWo6FAJhGG9fp7D97jhHeLC\n+5vwfmtTAfnGBxadfAOT3ASkxnOZhXtnRna460LtnNHm7ArCVgXKJo7uBn6ViXtFh\n+uEEw4y6p9YaLQna+VC8Xtgw6WKq2JXuKzuhuNKSFaGGw9vRcHg==\n+-----END CERTIFICATE-----`,\n+\n+\t\"eu-south-1\": `-----BEGIN CERTIFICATE-----\n+MIICNjCCAZ+gAwIBAgIJAOZ3GEIaDcugMA0GCSqGSIb3DQEBCwUAMFwxCzAJBgNV\n+BAYTAlVTMRkwFwYDVQQIExBXYXNoaW5ndG9uIFN0YXRlMRAwDgYDVQQHEwdTZWF0\n+dGxlMSAwHgYDVQQKExdBbWF6b24gV2ViIFNlcnZpY2VzIExMQzAgFw0xOTEwMjQx\n+NTE5MDlaGA8yMTk5MDMyOTE1MTkwOVowXDELMAkGA1UEBhMCVVMxGTAXBgNVBAgT\n+EFdhc2hpbmd0b24gU3RhdGUxEDAOBgNVBAcTB1NlYXR0bGUxIDAeBgNVBAoTF0Ft\n+YXpvbiBXZWIgU2VydmljZXMgTExDMIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKB\n+gQCjiPgW3vsXRj4JoA16WQDyoPc/eh3QBARaApJEc4nPIGoUolpAXcjFhWplo2O+\n+ivgfCsc4AU9OpYdAPha3spLey/bhHPRi1JZHRNqScKP0hzsCNmKhfnZTIEQCFvsp\n+DRp4zr91/WS06/flJFBYJ6JHhp0KwM81XQG59lV6kkoW7QIDAQABMA0GCSqGSIb3\n+DQEBCwUAA4GBAGLLrY3P+HH6C57dYgtJkuGZGT2+rMkk2n81/abzTJvsqRqGRrWv\n+XRKRXlKdM/dfiuYGokDGxiC0Mg6TYy6wvsR2qRhtXW1OtZkiHWcQCnOttz+8vpew\n+wx8JGMvowtuKB1iMsbwyRqZkFYLcvH+Opfb/Aayi20/ChQLdI6M2R5VU\n+-----END CERTIFICATE-----`,\n+\n+\t\"eu-west-3\": `-----BEGIN CERTIFICATE-----\n+MIIDITCCAoqgAwIBAgIUaC9fX57UDr6u1vBvsCsECKBZQyIwDQYJKoZIhvcNAQEL\n+BQAwXDELMAkGA1UEBhMCVVMxGTAXBgNVBAgTEFdhc2hpbmd0b24gU3RhdGUxEDAO\n+BgNVBAcTB1NlYXR0bGUxIDAeBgNVBAoTF0FtYXpvbiBXZWIgU2VydmljZXMgTExD\n+MB4XDTI0MDQyOTE2MzczOFoXDTI5MDQyODE2MzczOFowXDELMAkGA1UEBhMCVVMx\n+GTAXBgNVBAgTEFdhc2hpbmd0b24gU3RhdGUxEDAOBgNVBAcTB1NlYXR0bGUxIDAe\n+BgNVBAoTF0FtYXpvbiBXZWIgU2VydmljZXMgTExDMIGfMA0GCSqGSIb3DQEBAQUA\n+A4GNADCBiQKBgQCHvRjf/0kStpJ248khtIaN8qkDN3tkw4VjvA9nvPl2anJO+eIB\n+UqPfQG09kZlwpWpmyO8bGB2RWqWxCwuB/dcnIob6w420k9WY5C0IIGtDRNauN3ku\n+vGXkw3HEnF0EjYr0pcyWUvByWY4KswZV42X7Y7XSS13hOIcL6NLA+H94/QIDAQAB\n+o4HfMIHcMAsGA1UdDwQEAwIHgDAdBgNVHQ4EFgQUJdbMCBXKtvCcWdwUUizvtUF2\n+UTgwgZkGA1UdIwSBkTCBjoAUJdbMCBXKtvCcWdwUUizvtUF2UTihYKReMFwxCzAJ\n+BgNVBAYTAlVTMRkwFwYDVQQIExBXYXNoaW5ndG9uIFN0YXRlMRAwDgYDVQQHEwdT\n+ZWF0dGxlMSAwHgYDVQQKExdBbWF6b24gV2ViIFNlcnZpY2VzIExMQ4IUaC9fX57U\n+Dr6u1vBvsCsECKBZQyIwEgYDVR0TAQH/BAgwBgEB/wIBADANBgkqhkiG9w0BAQsF\n+AAOBgQCARv1bQEDaMEzYI0nPlu8GHcMXgmgA94HyrXhMMcaIlQwocGBs6VILGVhM\n+TXP2r3JFaPEpmXSQNQHvGA13clKwAZbni8wtzv6qXb4L4muF34iQRHF0nYrEDoK7\n+mMPR8+oXKKuPO/mv/XKo6XAV5DDERdSYHX5kkA2R9wtvyZjPnQ==\n+-----END CERTIFICATE-----`,\n+\n+\t\"eu-south-2\": `-----BEGIN CERTIFICATE-----\n+MIICMzCCAZygAwIBAgIGAXjwLkiaMA0GCSqGSIb3DQEBBQUAMFwxCzAJBgNVBAYT\n+AlVTMRkwFwYDVQQIDBBXYXNoaW5ndG9uIFN0YXRlMRAwDgYDVQQHDAdTZWF0dGxl\n+MSAwHgYDVQQKDBdBbWF6b24gV2ViIFNlcnZpY2VzIExMQzAgFw0yMTA0MjAxNjQ3\n+NDhaGA8yMjAwMDQyMDE2NDc0OFowXDELMAkGA1UEBhMCVVMxGTAXBgNVBAgMEFdh\n+c2hpbmd0b24gU3RhdGUxEDAOBgNVBAcMB1NlYXR0bGUxIDAeBgNVBAoMF0FtYXpv\n+biBXZWIgU2VydmljZXMgTExDMIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQDB\n+/VvR1+45Aey5zn3vPk6xBm5o9grSDL6D2iAuprQnfVXn8CIbSDbWFhA3fi5ippjK\n+kh3sl8VyCvCOUXKdOaNrYBrPRkrdHdBuL2Tc84RO+3m/rxIUZ2IK1fDlC6sWAjdd\n+f6sBrV2w2a78H0H8EwuwiSgttURBjwJ7KPPJCqaqrQIDAQABMA0GCSqGSIb3DQEB\n+BQUAA4GBAKR+FzqQDzun/iMMzcFucmLMl5BxEblrFXOz7IIuOeiGkndmrqUeDCyk\n+ztLku45s7hxdNy4ltTuVAaE5aNBdw5J8U1mRvsKvHLy2ThH6hAWKwTqtPAJp7M21\n+GDwgDDOkPSz6XVOehg+hBgiphYp84DUbWVYeP8YqLEJSqscKscWC\n+-----END CERTIFICATE-----`,\n+\n+\t\"eu-north-1\": `-----BEGIN CERTIFICATE-----\n+MIIDITCCAoqgAwIBAgIUN1c9U6U/xiVDFgJcYKZB4NkH1QEwDQYJKoZIhvcNAQEL\n+BQAwXDELMAkGA1UEBhMCVVMxGTAXBgNVBAgTEFdhc2hpbmd0b24gU3RhdGUxEDAO\n+BgNVBAcTB1NlYXR0bGUxIDAeBgNVBAoTF0FtYXpvbiBXZWIgU2VydmljZXMgTExD\n+MB4XDTI0MDQyOTE2MDYwM1oXDTI5MDQyODE2MDYwM1owXDELMAkGA1UEBhMCVVMx\n+GTAXBgNVBAgTEFdhc2hpbmd0b24gU3RhdGUxEDAOBgNVBAcTB1NlYXR0bGUxIDAe\n+BgNVBAoTF0FtYXpvbiBXZWIgU2VydmljZXMgTExDMIGfMA0GCSqGSIb3DQEBAQUA\n+A4GNADCBiQKBgQCHvRjf/0kStpJ248khtIaN8qkDN3tkw4VjvA9nvPl2anJO+eIB\n+UqPfQG09kZlwpWpmyO8bGB2RWqWxCwuB/dcnIob6w420k9WY5C0IIGtDRNauN3ku\n+vGXkw3HEnF0EjYr0pcyWUvByWY4KswZV42X7Y7XSS13hOIcL6NLA+H94/QIDAQAB\n+o4HfMIHcMAsGA1UdDwQEAwIHgDAdBgNVHQ4EFgQUJdbMCBXKtvCcWdwUUizvtUF2\n+UTgwgZkGA1UdIwSBkTCBjoAUJdbMCBXKtvCcWdwUUizvtUF2UTihYKReMFwxCzAJ\n+BgNVBAYTAlVTMRkwFwYDVQQIExBXYXNoaW5ndG9uIFN0YXRlMRAwDgYDVQQHEwdT\n+ZWF0dGxlMSAwHgYDVQQKExdBbWF6b24gV2ViIFNlcnZpY2VzIExMQ4IUN1c9U6U/\n+xiVDFgJcYKZB4NkH1QEwEgYDVR0TAQH/BAgwBgEB/wIBADANBgkqhkiG9w0BAQsF\n+AAOBgQBTIQdoFSDRHkpqNPUbZ9WXR2O5v/9bpmHojMYZb3Hw46wsaRso7STiGGX/\n+tRqjIkPUIXsdhZ3+7S/RmhFznmZc8e0bjU4n5vi9CJtQSt+1u4E17+V2bF+D3h/7\n+wcfE0l3414Q8JaTDtfEf/aF3F0uyBvr4MDMd7mFvAMmDmBPSlA==\n+-----END CERTIFICATE-----`,\n+\n+\t\"eu-central-2\": `-----BEGIN CERTIFICATE-----\n+MIICMzCCAZygAwIBAgIGAXjSGFGiMA0GCSqGSIb3DQEBBQUAMFwxCzAJBgNVBAYTAlVTMRkwFwYDVQQIDBBXYXNoaW5ndG9uIFN0YXRlMRAwDgYDVQQHDAdTZWF0dGxlMSAwHgYDVQQKDBdBbWF6b24gV2ViIFNlcnZpY2VzIExMQzAgFw0yMTA0MTQyMDM1MTJaGA8yMjAwMDQxNDIwMzUxMlowXDELMAkGA1UEBhMCVVMxGTAXBgNVBAgMEFdhc2hpbmd0b24gU3RhdGUxEDAOBgNVBAcMB1NlYXR0bGUxIDAeBgNVBAoMF0FtYXpvbiBXZWIgU2VydmljZXMgTExDMIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQC2mdGdps5Rz2jzYcGNsgETTGUthJRrVqSnUWJXTlVaIbkGPLKO6Or7AfWKFp2sgRJ8vLsjoBVR5cESVK7cuK1wItjvJyi/opKZAUusJx2hpgU3pUHhlp9ATh/VeVD582jTd9IY+8t5MDa6Z3fGliByEiXz0LEHdi8MBacLREu1TwIDAQABMA0GCSqGSIb3DQEBBQUAA4GBAILlpoE3k9o7KdALAxsFJNitVS+g3RMzdbiFM+7MA63Nv5fsf+0xgcjSNBElvPCDKFvTJl4QQhToy056llO5GvdS9RK+H8xrP2mrqngApoKTApv93vHBixgFSn5KrczRO0YSm3OjkqbydU7DFlmkXXR7GYE+5jbHvQHYiT1J5sMu\n+-----END CERTIFICATE-----`,\n+\n+\t\"il-central-1\": `-----BEGIN CERTIFICATE-----\n+MIICMzCCAZygAwIBAgIGAX0QQGVLMA0GCSqGSIb3DQEBBQUAMFwxCzAJBgNVBAYTAlVTMRkwFwYDVQQIDBBXYXNoaW5ndG9uIFN0YXRlMRAwDgYDVQQHDAdTZWF0dGxlMSAwHgYDVQQKDBdBbWF6b24gV2ViIFNlcnZpY2VzIExMQzAgFw0yMTExMTExODI2MzVaGA8yMjAwMTExMTE4MjYzNVowXDELMAkGA1UEBhMCVVMxGTAXBgNVBAgMEFdhc2hpbmd0b24gU3RhdGUxEDAOBgNVBAcMB1NlYXR0bGUxIDAeBgNVBAoMF0FtYXpvbiBXZWIgU2VydmljZXMgTExDMIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQDrc24u3AgFxnoPgzxR6yFXOamcPuxYXhYKWmapb+S8vOy5hpLoRe4RkOrY0cM3bN07GdEMlin5mU0y1t8y3ct4YewvmkgT42kTyMM+t1K4S0xsqjXxxS716uGYh7eWtkxrCihj8AbXN/6pa095h+7TZyl2n83keiNUzM2KoqQVMwIDAQABMA0GCSqGSIb3DQEBBQUAA4GBADwA6VVEIIZD2YL00F12po40xDLzIc9XvqFPS9iFaWi2ho8wLio7wA49VYEFZSI9CR3SGB9tL8DUib97mlxmd1AcGShMmMlhSB29vhuhrUNB/FmU7H8s62/jD6cOR1A1cClIyZUe1yT1ZbPySCs43J+Thr8i8FSRxzDBSZZi5foW\n+-----END CERTIFICATE-----`,\n+\n+\t\"mx-central-1\": `-----BEGIN CERTIFICATE-----\n+MIICPzCCAaigAwIBAgIUCmzpTTMBQYItpMC2VDYsZfIAS7IwDQYJKoZIhvcNAQEL\n+BQAwXDELMAkGA1UEBhMCVVMxGTAXBgNVBAgMEFdhc2hpbmd0b24gU3RhdGUxEDAO\n+BgNVBAcMB1NlYXR0bGUxIDAeBgNVBAoMF0FtYXpvbiBXZWIgU2VydmljZXMgTExD\n+MB4XDTI0MDQwMjEyNTAzNloXDTI5MDQwMTEyNTAzNlowXDELMAkGA1UEBhMCVVMx\n+GTAXBgNVBAgMEFdhc2hpbmd0b24gU3RhdGUxEDAOBgNVBAcMB1NlYXR0bGUxIDAe\n+BgNVBAoMF0FtYXpvbiBXZWIgU2VydmljZXMgTExDMIGfMA0GCSqGSIb3DQEBAQUA\n+A4GNADCBiQKBgQDfvOnCzm1iN58Nm7k6ehoy6v0lnnFI617D6CY3bfuq01RCdEQL\n+96+pYawJieTH8JAQKj02CAa3AeaqdXTE/pDhI/YKLreeMb4K68WMn24Wjjs6oxjB\n+bAmsKXtt9ihKHGBFNUhgFrNFYyA2i7ieJviwpHjQ/XgXiG2u1/t/4VydUwIDAQAB\n+MA0GCSqGSIb3DQEBCwUAA4GBAL5+vvj4lhaE+J5tuCqV3XJzDd97lsD4le2O2uGw\n+P0sGdUcRAdxzU3Bwq/hhtzNWnfwo0aCEQKmLM7xyd3nUa0VvKXLq+DDuayipWINr\n+OATnNxFRe99d38qHTR1dgkjZdKbbtnl6O4fgM57tVEuQJd/N4ILl9jaRcJ5Ip+9t\n+3y5t\n+-----END CERTIFICATE-----`,\n+\n+\t\"me-south-1\": `-----BEGIN CERTIFICATE-----\n+MIIDPDCCAqWgAwIBAgIJAMl6uIV/zqJFMA0GCSqGSIb3DQEBCwUAMHIxCzAJBgNV\n+BAYTAlVTMRMwEQYDVQQIDApXYXNoaW5ndG9uMRAwDgYDVQQHDAdTZWF0dGxlMSAw\n+HgYDVQQKDBdBbWF6b24gV2ViIFNlcnZpY2VzIExMQzEaMBgGA1UEAwwRZWMyLmFt\n+YXpvbmF3cy5jb20wIBcNMTkwNDI2MTQzMjQ3WhgPMjE5ODA5MjkxNDMyNDdaMHIx\n+CzAJBgNVBAYTAlVTMRMwEQYDVQQIDApXYXNoaW5ndG9uMRAwDgYDVQQHDAdTZWF0\n+dGxlMSAwHgYDVQQKDBdBbWF6b24gV2ViIFNlcnZpY2VzIExMQzEaMBgGA1UEAwwR\n+ZWMyLmFtYXpvbmF3cy5jb20wgZ8wDQYJKoZIhvcNAQEBBQADgY0AMIGJAoGBALVN\n+CDTZEnIeoX1SEYqq6k1BV0ZlpY5y3KnoOreCAE589TwS4MX5+8Fzd6AmACmugeBP\n+Qk7Hm6b2+g/d4tWycyxLaQlcq81DB1GmXehRkZRgGeRge1ePWd1TUA0I8P/QBT7S\n+gUePm/kANSFU+P7s7u1NNl+vynyi0wUUrw7/wIZTAgMBAAGjgdcwgdQwHQYDVR0O\n+BBYEFILtMd+T4YgH1cgc+hVsVOV+480FMIGkBgNVHSMEgZwwgZmAFILtMd+T4YgH\n+1cgc+hVsVOV+480FoXakdDByMQswCQYDVQQGEwJVUzETMBEGA1UECAwKV2FzaGlu\n+Z3RvbjEQMA4GA1UEBwwHU2VhdHRsZTEgMB4GA1UECgwXQW1hem9uIFdlYiBTZXJ2\n+aWNlcyBMTEMxGjAYBgNVBAMMEWVjMi5hbWF6b25hd3MuY29tggkAyXq4hX/OokUw\n+DAYDVR0TBAUwAwEB/zANBgkqhkiG9w0BAQsFAAOBgQBhkNTBIFgWFd+ZhC/LhRUY\n+4OjEiykmbEp6hlzQ79T0Tfbn5A4NYDI2icBP0+hmf6qSnIhwJF6typyd1yPK5Fqt\n+NTpxxcXmUKquX+pHmIkK1LKDO8rNE84jqxrxRsfDi6by82fjVYf2pgjJW8R1FAw+\n+mL5WQRFexbfB5aXhcMo0AA==\n+-----END CERTIFICATE-----`,\n+\n+\t\"me-central-1\": `-----BEGIN CERTIFICATE-----\n+MIICMzCCAZygAwIBAgIGAXjRrnDjMA0GCSqGSIb3DQEBBQUAMFwxCzAJBgNVBAYTAlVTMRkwFwYDVQQIDBBXYXNoaW5ndG9uIFN0YXRlMRAwDgYDVQQHDAdTZWF0dGxlMSAwHgYDVQQKDBdBbWF6b24gV2ViIFNlcnZpY2VzIExMQzAgFw0yMTA0MTQxODM5MzNaGA8yMjAwMDQxNDE4MzkzM1owXDELMAkGA1UEBhMCVVMxGTAXBgNVBAgMEFdhc2hpbmd0b24gU3RhdGUxEDAOBgNVBAcMB1NlYXR0bGUxIDAeBgNVBAoMF0FtYXpvbiBXZWIgU2VydmljZXMgTExDMIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQDcaTgW/KyA6zyruJQrYy00a6wqLA7eeUzk3bMiTkLsTeDQfrkaZMfBAjGaaOymRo1C3qzE4rIenmahvUplu9ZmLwL1idWXMRX2RlSvIt+d2SeoKOKQWoc2UOFZMHYxDue7zkyk1CIRaBukTeY13/RIrlc6X61zJ5BBtZXlHwayjQIDAQABMA0GCSqGSIb3DQEBBQUAA4GBABTqTy3R6RXKPW45FA+cgo7YZEj/Cnz5YaoUivRRdX2A83BHuBTvJE2+WX00FTEj4hRVjameE1nENoO8Z7fUVloAFDlDo69fhkJeSvn51D1WRrPnoWGgEfr1+OfK1bAcKTtfkkkP9r4RdwSjKzO5Zu/B+Wqm3kVEz/QNcz6npmA6\n+-----END CERTIFICATE-----`,\n+\n+\t\"sa-east-1\": `-----BEGIN CERTIFICATE-----\n+MIIDITCCAoqgAwIBAgIUX4Bh4MQ86Roh37VDRRX1MNOB3TcwDQYJKoZIhvcNAQEL\n+BQAwXDELMAkGA1UEBhMCVVMxGTAXBgNVBAgTEFdhc2hpbmd0b24gU3RhdGUxEDAO\n+BgNVBAcTB1NlYXR0bGUxIDAeBgNVBAoTF0FtYXpvbiBXZWIgU2VydmljZXMgTExD\n+MB4XDTI0MDQyOTE2NDYwOVoXDTI5MDQyODE2NDYwOVowXDELMAkGA1UEBhMCVVMx\n+GTAXBgNVBAgTEFdhc2hpbmd0b24gU3RhdGUxEDAOBgNVBAcTB1NlYXR0bGUxIDAe\n+BgNVBAoTF0FtYXpvbiBXZWIgU2VydmljZXMgTExDMIGfMA0GCSqGSIb3DQEBAQUA\n+A4GNADCBiQKBgQCHvRjf/0kStpJ248khtIaN8qkDN3tkw4VjvA9nvPl2anJO+eIB\n+UqPfQG09kZlwpWpmyO8bGB2RWqWxCwuB/dcnIob6w420k9WY5C0IIGtDRNauN3ku\n+vGXkw3HEnF0EjYr0pcyWUvByWY4KswZV42X7Y7XSS13hOIcL6NLA+H94/QIDAQAB\n+o4HfMIHcMAsGA1UdDwQEAwIHgDAdBgNVHQ4EFgQUJdbMCBXKtvCcWdwUUizvtUF2\n+UTgwgZkGA1UdIwSBkTCBjoAUJdbMCBXKtvCcWdwUUizvtUF2UTihYKReMFwxCzAJ\n+BgNVBAYTAlVTMRkwFwYDVQQIExBXYXNoaW5ndG9uIFN0YXRlMRAwDgYDVQQHEwdT\n+ZWF0dGxlMSAwHgYDVQQKExdBbWF6b24gV2ViIFNlcnZpY2VzIExMQ4IUX4Bh4MQ8\n+6Roh37VDRRX1MNOB3TcwEgYDVR0TAQH/BAgwBgEB/wIBADANBgkqhkiG9w0BAQsF\n+AAOBgQBnhocfH6ZIX6F5K9+Y9V4HFk8vSaaKL5ytw/P5td1h9ej94KF3xkZ5fyjN\n+URvGQv3kNmNJBoNarcP9I7JIMjsNPmVzqWawyCEGCZImoARxSS3Fc5EAs2PyBfcD\n+9nCtzMTaKO09Xyq0wqXVYn1xJsE5d5yBDsGrzaTHKjxo61+ezQ==\n+-----END CERTIFICATE-----`,\n+\n+\t\"us-gov-east-1\": `-----BEGIN CERTIFICATE-----\n+MIIDITCCAoqgAwIBAgIULVyrqjjwZ461qelPCiShB1KCCj4wDQYJKoZIhvcNAQEL\n+BQAwXDELMAkGA1UEBhMCVVMxGTAXBgNVBAgTEFdhc2hpbmd0b24gU3RhdGUxEDAO\n+BgNVBAcTB1NlYXR0bGUxIDAeBgNVBAoTF0FtYXpvbiBXZWIgU2VydmljZXMgTExD\n+MB4XDTI0MDUwNzE1MjIzNloXDTI5MDUwNjE1MjIzNlowXDELMAkGA1UEBhMCVVMx\n+GTAXBgNVBAgTEFdhc2hpbmd0b24gU3RhdGUxEDAOBgNVBAcTB1NlYXR0bGUxIDAe\n+BgNVBAoTF0FtYXpvbiBXZWIgU2VydmljZXMgTExDMIGfMA0GCSqGSIb3DQEBAQUA\n+A4GNADCBiQKBgQCpohwYUVPH9I7Vbkb3WMe/JB0Y/bmfVj3VpcK445YBRO9K80al\n+esjgBc2tAX4KYg4Lht4EBKccLHTzaNi51YEGX1aLNrSmxhz1+WtzNLNUsyY3zD9z\n+vwX/3k1+JB2dRA+m+Cpwx4mjzZyAeQtHtegVaAytkmqtxQrSCexBxvqRqQIDAQAB\n+o4HfMIHcMAsGA1UdDwQEAwIHgDAdBgNVHQ4EFgQU1ZXneBYnPvYXkHVlVjg7918V\n+gE8wgZkGA1UdIwSBkTCBjoAU1ZXneBYnPvYXkHVlVjg7918VgE+hYKReMFwxCzAJ\n+BgNVBAYTAlVTMRkwFwYDVQQIExBXYXNoaW5ndG9uIFN0YXRlMRAwDgYDVQQHEwdT\n+ZWF0dGxlMSAwHgYDVQQKExdBbWF6b24gV2ViIFNlcnZpY2VzIExMQ4IULVyrqjjw\n+Z461qelPCiShB1KCCj4wEgYDVR0TAQH/BAgwBgEB/wIBADANBgkqhkiG9w0BAQsF\n+AAOBgQBfAL/YZv0y3zmVbXjyxQCsDloeDCJjFKIu3ameEckeIWJbST9LMto0zViZ\n+puIAf05x6GQiEqfBMk+YMxJfcTmJB4Ebaj4egFlslJPSHyC2xuydHlr3B04INOH5\n+Z2oCM68u6GGbj0jZjg7GJonkReG9N72kDva/ukwZKgq8zErQVQ==\n+-----END CERTIFICATE-----`,\n+\n+\t\"us-gov-west-1\": `-----BEGIN CERTIFICATE-----\n+MIIDITCCAoqgAwIBAgIUe5wGF3jfb7lUHzvDxmM/ktGCLwwwDQYJKoZIhvcNAQEL\n+BQAwXDELMAkGA1UEBhMCVVMxGTAXBgNVBAgTEFdhc2hpbmd0b24gU3RhdGUxEDAO\n+BgNVBAcTB1NlYXR0bGUxIDAeBgNVBAoTF0FtYXpvbiBXZWIgU2VydmljZXMgTExD\n+MB4XDTI0MDUwNzE3MzAzMloXDTI5MDUwNjE3MzAzMlowXDELMAkGA1UEBhMCVVMx\n+GTAXBgNVBAgTEFdhc2hpbmd0b24gU3RhdGUxEDAOBgNVBAcTB1NlYXR0bGUxIDAe\n+BgNVBAoTF0FtYXpvbiBXZWIgU2VydmljZXMgTExDMIGfMA0GCSqGSIb3DQEBAQUA\n+A4GNADCBiQKBgQCpohwYUVPH9I7Vbkb3WMe/JB0Y/bmfVj3VpcK445YBRO9K80al\n+esjgBc2tAX4KYg4Lht4EBKccLHTzaNi51YEGX1aLNrSmxhz1+WtzNLNUsyY3zD9z\n+vwX/3k1+JB2dRA+m+Cpwx4mjzZyAeQtHtegVaAytkmqtxQrSCexBxvqRqQIDAQAB\n+o4HfMIHcMAsGA1UdDwQEAwIHgDAdBgNVHQ4EFgQU1ZXneBYnPvYXkHVlVjg7918V\n+gE8wgZkGA1UdIwSBkTCBjoAU1ZXneBYnPvYXkHVlVjg7918VgE+hYKReMFwxCzAJ\n+BgNVBAYTAlVTMRkwFwYDVQQIExBXYXNoaW5ndG9uIFN0YXRlMRAwDgYDVQQHEwdT\n+ZWF0dGxlMSAwHgYDVQQKExdBbWF6b24gV2ViIFNlcnZpY2VzIExMQ4IUe5wGF3jf\n+b7lUHzvDxmM/ktGCLwwwEgYDVR0TAQH/BAgwBgEB/wIBADANBgkqhkiG9w0BAQsF\n+AAOBgQCbTdpx1Iob9SwUReY4exMnlwQlmkTLyA8tYGWzchCJOJJEPfsW0ryy1A0H\n+YIuvyUty3rJdp9ib8h3GZR71BkZnNddHhy06kPs4p8ewF8+d8OWtOJQcI+ZnFfG4\n+KyM4rUsBrljpG2aOCm12iACEyrvgJJrS8VZwUDZS6mZEnn/lhA==\n+-----END CERTIFICATE-----`,\n+}"
        },
        {
          "filename": "attestation/aws-iid/aws-iid.go",
          "old_url": "https://raw.githubusercontent.com/in-toto/go-witness/4edc0c53af886b7b246297ce59ebeededaf27d7f/attestation/aws-iid/aws-iid.go",
          "new_url": "https://raw.githubusercontent.com/in-toto/go-witness/04ff20b600e28ce8fd1aa287534dd383a1cfefb9/attestation/aws-iid/aws-iid.go",
          "diff": "@@ -25,6 +25,7 @@ import (\n \t\"encoding/pem\"\n \t\"fmt\"\n \t\"io\"\n+\t\"time\"\n \n \t\"github.com/aws/aws-sdk-go-v2/aws\"\n \t\"github.com/aws/aws-sdk-go-v2/config\"\n@@ -46,10 +47,6 @@ const (\n const (\n \tdocPath = \"instance-identity/document\"\n \tsigPath = \"instance-identity/signature\"\n-\t// https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/verify-iid.html\n-\t// There is a different public cert for every AWS region\n-\t// You can find the one you need for verification here:\n-\t// https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/regions-certs.html\n )\n \n var (\n@@ -70,10 +67,6 @@ func init() {\n \t\t\t\tif !ok {\n \t\t\t\t\treturn a, fmt.Errorf(\"invalid attestor type: %T\", a)\n \t\t\t\t}\n-\t\t\t\tif val == \"\" {\n-\t\t\t\t\treturn a, fmt.Errorf(\"aws-region-cert cannot be empty\")\n-\t\t\t\t}\n-\n \t\t\t\tWithAWSRegionCert(val)(attestor)\n \t\t\t\treturn attestor, nil\n \t\t\t},\n@@ -190,7 +183,14 @@ func (a *Attestor) Verify() error {\n \t\treturn fmt.Errorf(\"failed to decode signature: %w\", err)\n \t}\n \n-\tpubKey, err := getAWSCAPublicKey(a.awsCert)\n+\t// use the region from config if possible,\n+\t// otherwise fall back to the region from the IID\n+\tregion := a.cfg.Region\n+\tif region == \"\" {\n+\t\tregion = a.Region\n+\t}\n+\n+\tpubKey, err := getAWSCAPublicKey(region, a.awsCert)\n \tif err != nil {\n \t\treturn fmt.Errorf(\"failed to get AWS public key: %w\", err)\n \t}\n@@ -245,7 +245,15 @@ func (a *Attestor) Subjects() map[string]cryptoutil.DigestSet {\n \treturn subjects\n }\n \n-func getAWSCAPublicKey(awsCert string) (*rsa.PublicKey, error) {\n+func getAWSCAPublicKey(awsRegion, awsCert string) (*rsa.PublicKey, error) {\n+\tif awsCert == \"\" {\n+\t\tvar err error\n+\t\tawsCert, err = getRegionCert(awsRegion)\n+\t\tif err != nil {\n+\t\t\treturn nil, err\n+\t\t}\n+\t}\n+\n \tblock, rest := pem.Decode([]byte(awsCert))\n \tif len(rest) > 0 {\n \t\treturn nil, fmt.Errorf(\"failed to decode PEM block containing the public key\")\n@@ -256,5 +264,14 @@ func getAWSCAPublicKey(awsCert string) (*rsa.PublicKey, error) {\n \t\treturn nil, fmt.Errorf(\"failed to parse certificate: %w\", err)\n \t}\n \n+\tif time.Now().Before(cert.NotBefore) || time.Now().After(cert.NotAfter) {\n+\t\treturn nil, fmt.Errorf(\"%s: certificate is not valid at the current time\", awsRegion)\n+\t}\n+\tlog.Infof(\"(attestation/aws-iid) using AWS public key issued by %s, valid from %s to %s\", cert.Issuer.CommonName, cert.NotBefore, cert.NotAfter)\n+\n+\tif cert.PublicKeyAlgorithm != x509.RSA {\n+\t\treturn nil, fmt.Errorf(\"%s: unexpected public key algorithm: %v\", awsRegion, cert.PublicKeyAlgorithm)\n+\t}\n+\n \treturn cert.PublicKey.(*rsa.PublicKey), nil\n }"
        },
        {
          "filename": "attestation/aws-iid/aws-iid_test.go",
          "old_url": "https://raw.githubusercontent.com/in-toto/go-witness/4edc0c53af886b7b246297ce59ebeededaf27d7f/attestation/aws-iid/aws-iid_test.go",
          "new_url": "https://raw.githubusercontent.com/in-toto/go-witness/04ff20b600e28ce8fd1aa287534dd383a1cfefb9/attestation/aws-iid/aws-iid_test.go",
          "diff": "@@ -68,6 +68,22 @@ T2KEmoohLmK2mQz8NAu0xaOrKMDX6gyJGacw7ig6qjDNsUz3Sjl4NBkEe9cc2wXc\n kHPb0HdH2xJYhM7T\n -----END CERTIFICATE-----`\n \n+const expiredCert = `-----BEGIN CERTIFICATE-----\n+MIICZjCCAc+gAwIBAgIULTjRElgYD3XgFLpeTuhQTfQ4JOswDQYJKoZIhvcNAQEL\n+BQAwRTELMAkGA1UEBhMCQVUxEzARBgNVBAgMClNvbWUtU3RhdGUxITAfBgNVBAoM\n+GEludGVybmV0IFdpZGdpdHMgUHR5IEx0ZDAeFw0xNjAxMDYwNzEzMTFaFw0xNzAx\n+MDYwNzEzMTFaMEUxCzAJBgNVBAYTAkFVMRMwEQYDVQQIDApTb21lLVN0YXRlMSEw\n+HwYDVQQKDBhJbnRlcm5ldCBXaWRnaXRzIFB0eSBMdGQwgZ8wDQYJKoZIhvcNAQEB\n+BQADgY0AMIGJAoGBAK6jsX5XP35jTDYoWKgtEkWv5hUiFyq0fTw43pnJmvnGIj2X\n+LdT20v6OzJQJwWUpx8vT0/IhcMvYjJ2A5E/2LvN3FJu0OxBWljVt3609V2YtbesC\n+LNG3nYiXvEIScCJoPvCYGdzBAZAfpjfXxfg2sYC7V/jrKHyOr8j3kpPbFMXjAgMB\n+AAGjUzBRMB0GA1UdDgQWBBSn/vtoYG9spomwZb6Ed+WR2qRQtTAfBgNVHSMEGDAW\n+gBSn/vtoYG9spomwZb6Ed+WR2qRQtTAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3\n+DQEBCwUAA4GBAJNw9iyseAmri+so2SEYTx2Vf6uSEjyWREd4T+Qm81bi/dsoFXP1\n+1kR/Zls+0RxEey1W6rbwrvViYdHcvPIXJlR7AZcWW4UPYSvqL5VPNSzPD9oQK2Np\n+7bK/6lhsQS8dWIZsVsI02NCHE4j2YQhKsT9GsGqHUmWka/4i3/QqZX/2\n+-----END CERTIFICATE-----`\n+\n type testresp struct {\n \tpath string\n \tresp string\n@@ -246,9 +262,25 @@ func TestAttestor_Subjects(t *testing.T) {\n }\n \n func Test_getAWSPublicKey(t *testing.T) {\n-\tkey, err := getAWSCAPublicKey(testCert)\n+\tkey, err := getAWSCAPublicKey(\"\", testCert)\n \trequire.NoError(t, err)\n \tif key == nil {\n \t\tt.Error(\"Expected key to not be nil\")\n \t}\n+\n+\tkey, err = getAWSCAPublicKey(\"\", expiredCert)\n+\trequire.Error(t, err)\n+\tif key != nil {\n+\t\tt.Error(\"Expected key to be nil\")\n+\t}\n+}\n+\n+func Test_validateRegionalCerts(t *testing.T) {\n+\tfor region := range awsRegionCerts {\n+\t\tkey, err := getAWSCAPublicKey(region, \"\")\n+\t\trequire.NoError(t, err, \"failed to get public key for region %s: %v\", region, err)\n+\t\tif key == nil {\n+\t\t\tt.Errorf(\"Expected key to not be nil for region %s\", region)\n+\t\t}\n+\t}\n }"
        },
        {
          "filename": "attestation/aws-iid/check-certs/go.mod",
          "old_url": "https://raw.githubusercontent.com/in-toto/go-witness/4edc0c53af886b7b246297ce59ebeededaf27d7f/attestation/aws-iid/check-certs/go.mod",
          "new_url": "https://raw.githubusercontent.com/in-toto/go-witness/04ff20b600e28ce8fd1aa287534dd383a1cfefb9/attestation/aws-iid/check-certs/go.mod",
          "diff": "@@ -0,0 +1,5 @@\n+module check-certs\n+\n+go 1.24.0\n+\n+require golang.org/x/net v0.46.0"
        },
        {
          "filename": "attestation/aws-iid/check-certs/go.sum",
          "old_url": "https://raw.githubusercontent.com/in-toto/go-witness/4edc0c53af886b7b246297ce59ebeededaf27d7f/attestation/aws-iid/check-certs/go.sum",
          "new_url": "https://raw.githubusercontent.com/in-toto/go-witness/04ff20b600e28ce8fd1aa287534dd383a1cfefb9/attestation/aws-iid/check-certs/go.sum",
          "diff": "@@ -0,0 +1,2 @@\n+golang.org/x/net v0.46.0 h1:giFlY12I07fugqwPuWJi68oOnpfqFnJIJzaIIm2JVV4=\n+golang.org/x/net v0.46.0/go.mod h1:Q9BGdFy1y4nkUwiLvT5qtyhAnEHgnQ/zd8PfU6nc210="
        },
        {
          "filename": "attestation/aws-iid/check-certs/main.go",
          "old_url": "https://raw.githubusercontent.com/in-toto/go-witness/4edc0c53af886b7b246297ce59ebeededaf27d7f/attestation/aws-iid/check-certs/main.go",
          "new_url": "https://raw.githubusercontent.com/in-toto/go-witness/04ff20b600e28ce8fd1aa287534dd383a1cfefb9/attestation/aws-iid/check-certs/main.go",
          "diff": "@@ -0,0 +1,305 @@\n+// Copyright 2025 The Witness Contributors\n+//\n+// Licensed under the Apache License, Version 2.0 (the \"License\"),\n+// you may not use this file except in compliance with the License.\n+// You may obtain a copy of the License at\n+//\n+//      http://www.apache.org/licenses/LICENSE-2.0\n+//\n+// Unless required by applicable law or agreed to in writing, software\n+// distributed under the License is distributed on an \"AS IS\" BASIS,\n+// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+// See the License for the specific language governing permissions and\n+// limitations under the License.\n+\n+package main\n+\n+import (\n+\t\"crypto/sha256\"\n+\t\"encoding/hex\"\n+\t\"fmt\"\n+\t\"io\"\n+\t\"net/http\"\n+\t\"os\"\n+\t\"regexp\"\n+\t\"sort\"\n+\t\"strings\"\n+\n+\t\"golang.org/x/net/html\"\n+)\n+\n+const awsDocsURL = \"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/regions-certs.html\"\n+\n+func main() {\n+\tif len(os.Args) < 2 {\n+\t\tfmt.Fprintf(os.Stderr, \"Usage: %s <go-certs-file>\\n\", os.Args[0])\n+\t\tos.Exit(1)\n+\t}\n+\n+\tgoCertsFile := os.Args[1]\n+\n+\t// Download and parse AWS documentation\n+\tfmt.Println(\"Downloading AWS documentation...\")\n+\tresp, err := http.Get(awsDocsURL)\n+\tif err != nil {\n+\t\tfmt.Fprintf(os.Stderr, \"Error downloading documentation: %v\\n\", err)\n+\t\tos.Exit(1)\n+\t}\n+\tdefer resp.Body.Close()\n+\n+\tif resp.StatusCode != http.StatusOK {\n+\t\tfmt.Fprintf(os.Stderr, \"Error: HTTP status %d\\n\", resp.StatusCode)\n+\t\tos.Exit(1)\n+\t}\n+\n+\tbody, err := io.ReadAll(resp.Body)\n+\tif err != nil {\n+\t\tfmt.Fprintf(os.Stderr, \"Error reading response: %v\\n\", err)\n+\t\tos.Exit(1)\n+\t}\n+\n+\tfmt.Println(\"Parsing HTML and extracting RSA certificates by region...\")\n+\tdocsCerts := extractRSACertificatesByRegion(string(body))\n+\tfmt.Printf(\"Found %d RSA certificates in AWS docs\\n\", len(docsCerts))\n+\n+\t// Parse Go file\n+\tfmt.Println(\"\\nParsing aws-certs.go...\")\n+\tgoCerts, err := parseGoCertsFile(goCertsFile)\n+\tif err != nil {\n+\t\tfmt.Fprintf(os.Stderr, \"Error parsing Go file: %v\\n\", err)\n+\t\tos.Exit(1)\n+\t}\n+\tfmt.Printf(\"Found %d certificates in Go file\\n\", len(goCerts))\n+\n+\t// Compare certificates\n+\tfmt.Println(\"\\n\" + strings.Repeat(\"=\", 80))\n+\tfmt.Println(\"COMPARISON RESULTS\")\n+\tfmt.Println(strings.Repeat(\"=\", 80))\n+\n+\tcompareCertificates(docsCerts, goCerts)\n+}\n+\n+// extractRSACertificatesByRegion parses the HTML and extracts RSA certificates by region\n+func extractRSACertificatesByRegion(htmlContent string) map[string]string {\n+\tdoc, err := html.Parse(strings.NewReader(htmlContent))\n+\tif err != nil {\n+\t\tfmt.Fprintf(os.Stderr, \"Error parsing HTML: %v\\n\", err)\n+\t\treturn nil\n+\t}\n+\n+\tcertificates := make(map[string]string)\n+\tvar currentRegion string\n+\n+\tvar traverse func(*html.Node)\n+\ttraverse = func(n *html.Node) {\n+\t\t// Check if this is an expandable section with a region header\n+\t\tif n.Type == html.ElementNode && n.Data == \"awsui-expandable-section\" {\n+\t\t\tfor _, attr := range n.Attr {\n+\t\t\t\tif attr.Key == \"header\" {\n+\t\t\t\t\t// Extract region code from header like \"US East (N. Virginia) \u2014 us-east-1\"\n+\t\t\t\t\tcurrentRegion = extractRegionCode(attr.Val)\n+\t\t\t\t\tbreak\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\t// Check if this is a dd element with tab-id=\"rsa\"\n+\t\tif n.Type == html.ElementNode && n.Data == \"dd\" && currentRegion != \"\" {\n+\t\t\tfor _, attr := range n.Attr {\n+\t\t\t\tif attr.Key == \"tab-id\" && attr.Val == \"rsa\" {\n+\t\t\t\t\t// Extract text content from this node and its children\n+\t\t\t\t\ttext := getTextContent(n)\n+\t\t\t\t\t// Extract certificate from the text\n+\t\t\t\t\tcerts := extractCertificatesFromText(text)\n+\t\t\t\t\tif len(certs) > 0 {\n+\t\t\t\t\t\tcertificates[currentRegion] = normalizeCert(certs[0])\n+\t\t\t\t\t}\n+\t\t\t\t\treturn\n+\t\t\t\t}\n+\t\t\t}\n+\t\t}\n+\n+\t\t// Traverse children\n+\t\tfor c := n.FirstChild, c != nil, c = c.NextSibling {\n+\t\t\ttraverse(c)\n+\t\t}\n+\t}\n+\n+\ttraverse(doc)\n+\treturn certificates\n+}\n+\n+// extractRegionCode extracts the region code from a header string\n+// e.g., \"US East (N. Virginia) \u2014 us-east-1\" -> \"us-east-1\"\n+func extractRegionCode(header string) string {\n+\t// Look for pattern \"\u2014 region-code\"\n+\tparts := strings.Split(header, \"\u2014\")\n+\tif len(parts) >= 2 {\n+\t\treturn strings.TrimSpace(parts[len(parts)-1])\n+\t}\n+\treturn \"\"\n+}\n+\n+// getTextContent recursively extracts all text content from a node\n+func getTextContent(n *html.Node) string {\n+\tif n.Type == html.TextNode {\n+\t\treturn n.Data\n+\t}\n+\n+\tvar text string\n+\tfor c := n.FirstChild, c != nil, c = c.NextSibling {\n+\t\ttext += getTextContent(c)\n+\t}\n+\treturn text\n+}\n+\n+// extractCertificatesFromText extracts certificate blocks from text\n+func extractCertificatesFromText(text string) []string {\n+\t// Regular expression to match certificate blocks\n+\tcertRegex := regexp.MustCompile(`-----BEGIN CERTIFICATE-----[\\s\\S]*?-----END CERTIFICATE-----`)\n+\tmatches := certRegex.FindAllString(text, -1)\n+\n+\tvar certificates []string\n+\tfor _, match := range matches {\n+\t\t// Clean up the certificate (normalize whitespace)\n+\t\tcert := strings.TrimSpace(match)\n+\t\tcertificates = append(certificates, cert)\n+\t}\n+\n+\treturn certificates\n+}\n+\n+// normalizeCert normalizes a certificate for comparison\n+func normalizeCert(cert string) string {\n+\t// Remove all whitespace and normalize line endings\n+\treturn strings.Join(strings.Fields(cert), \"\\n\")\n+}\n+\n+// certHash returns a hash of a certificate for comparison\n+func certHash(cert string) string {\n+\t// Normalize by removing all whitespace\n+\tnormalized := strings.ReplaceAll(cert, \"\\n\", \"\")\n+\tnormalized = strings.ReplaceAll(normalized, \"\\r\", \"\")\n+\tnormalized = strings.ReplaceAll(normalized, \" \", \"\")\n+\tnormalized = strings.ReplaceAll(normalized, \"\\t\", \"\")\n+\n+\thash := sha256.Sum256([]byte(normalized))\n+\treturn hex.EncodeToString(hash[:])\n+}\n+\n+// parseGoCertsFile parses the aws-certs.go file and extracts region->cert mappings\n+func parseGoCertsFile(filename string) (map[string]string, error) {\n+\tcontent, err := os.ReadFile(filename)\n+\tif err != nil {\n+\t\treturn nil, err\n+\t}\n+\n+\tcerts := make(map[string]string)\n+\n+\t// Regular expression to match map entries like:\n+\t// \"region-name\": `-----BEGIN CERTIFICATE-----...-----END CERTIFICATE-----`,\n+\tentryRegex := regexp.MustCompile(`\"([^\"]+)\":\\s*` + \"`\" + `(-----BEGIN CERTIFICATE-----[\\s\\S]*?-----END CERTIFICATE-----)` + \"`\")\n+\tmatches := entryRegex.FindAllStringSubmatch(string(content), -1)\n+\n+\tfor _, match := range matches {\n+\t\tif len(match) >= 3 {\n+\t\t\tregion := match[1]\n+\t\t\tcert := normalizeCert(match[2])\n+\t\t\tcerts[region] = cert\n+\t\t}\n+\t}\n+\n+\treturn certs, nil\n+}\n+\n+// compareCertificates compares certificates from AWS docs and Go file\n+func compareCertificates(docsCerts, goCerts map[string]string) {\n+\t// Get all unique regions\n+\tallRegions := make(map[string]bool)\n+\tfor region := range docsCerts {\n+\t\tallRegions[region] = true\n+\t}\n+\tfor region := range goCerts {\n+\t\tallRegions[region] = true\n+\t}\n+\n+\t// Sort regions for consistent output\n+\tregions := make([]string, 0, len(allRegions))\n+\tfor region := range allRegions {\n+\t\tregions = append(regions, region)\n+\t}\n+\tsort.Strings(regions)\n+\n+\tvar matching, missingInGo, missingInDocs, different []string\n+\n+\tfor _, region := range regions {\n+\t\tdocsCert, inDocs := docsCerts[region]\n+\t\tgoCert, inGo := goCerts[region]\n+\n+\t\tif !inDocs {\n+\t\t\tmissingInDocs = append(missingInDocs, region)\n+\t\t} else if !inGo {\n+\t\t\tmissingInGo = append(missingInGo, region)\n+\t\t} else {\n+\t\t\t// Compare certificates\n+\t\t\tif certHash(docsCert) == certHash(goCert) {\n+\t\t\t\tmatching = append(matching, region)\n+\t\t\t} else {\n+\t\t\t\tdifferent = append(different, region)\n+\t\t\t}\n+\t\t}\n+\t}\n+\n+\t// Print results\n+\tfmt.Printf(\"\\n\u2713 MATCHING: %d regions\\n\", len(matching))\n+\tif len(matching) > 0 {\n+\t\tfor _, region := range matching {\n+\t\t\tfmt.Printf(\"  - %s\\n\", region)\n+\t\t}\n+\t}\n+\n+\tif len(missingInGo) > 0 {\n+\t\tfmt.Printf(\"\\n\u26a0 MISSING IN GO FILE: %d regions\\n\", len(missingInGo))\n+\t\tfmt.Println(\"  These regions are in AWS docs but not in aws-certs.go:\")\n+\t\tfor _, region := range missingInGo {\n+\t\t\tfmt.Printf(\"  - %s\\n\", region)\n+\t\t}\n+\t}\n+\n+\tif len(missingInDocs) > 0 {\n+\t\tfmt.Printf(\"\\n\u26a0 MISSING IN AWS DOCS: %d regions\\n\", len(missingInDocs))\n+\t\tfmt.Println(\"  These regions are in aws-certs.go but not in AWS docs:\")\n+\t\tfor _, region := range missingInDocs {\n+\t\t\tfmt.Printf(\"  - %s\\n\", region)\n+\t\t}\n+\t}\n+\n+\tif len(different) > 0 {\n+\t\tfmt.Printf(\"\\n\u274c DIFFERENT CERTIFICATES: %d regions\\n\", len(different))\n+\t\tfmt.Println(\"  These regions have different certificates:\")\n+\t\tfor _, region := range different {\n+\t\t\tfmt.Printf(\"  - %s\\n\", region)\n+\t\t\tfmt.Printf(\"    Docs hash: %s\\n\", certHash(docsCerts[region])[:16]+\"...\")\n+\t\t\tfmt.Printf(\"    Go hash:   %s\\n\", certHash(goCerts[region])[:16]+\"...\")\n+\t\t}\n+\t}\n+\n+\t// Summary\n+\tfmt.Println(\"\\n\" + strings.Repeat(\"=\", 80))\n+\tfmt.Println(\"SUMMARY\")\n+\tfmt.Println(strings.Repeat(\"=\", 80))\n+\tfmt.Printf(\"Total regions in AWS docs: %d\\n\", len(docsCerts))\n+\tfmt.Printf(\"Total regions in Go file:  %d\\n\", len(goCerts))\n+\tfmt.Printf(\"Matching:                  %d\\n\", len(matching))\n+\tfmt.Printf(\"Missing in Go file:        %d\\n\", len(missingInGo))\n+\tfmt.Printf(\"Missing in AWS docs:       %d\\n\", len(missingInDocs))\n+\tfmt.Printf(\"Different certificates:    %d\\n\", len(different))\n+\n+\tif len(missingInGo) > 0 || len(different) > 0 {\n+\t\tfmt.Println(\"\\n\u26a0\ufe0f  WARNING: The Go file needs to be updated!\")\n+\t} else if len(missingInDocs) > 0 {\n+\t\tfmt.Println(\"\\n\u26a0\ufe0f  WARNING: The Go file contains regions not in AWS docs!\")\n+\t} else {\n+\t\tfmt.Println(\"\\n\u2713 All certificates match!\")\n+\t}\n+}"
        }
      ]
    }
  ],
  [
    {
      "cve_id": [
        "CVE-2025-25298",
        "https://github.com/strapi/strapi/commit/41f8cdf116f7f464dae7d591e52d88f7bfa4b7cb"
      ],
      "repo": "strapi",
      "commit_hash": "41f8cdf116f7f464dae7d591e52d88f7bfa4b7cb",
      "commit_message": "fix: 72 byte maximum for creating and updating passwords",
      "files_changed": [
        {
          "filename": "packages/core/admin/admin/src/pages/Auth/components/Register.tsx",
          "old_url": "https://raw.githubusercontent.com/strapi/strapi/5d90c4b9e405d8f109b5d8184ee08b5bc2865c49/packages/core/admin/admin/src/pages/Auth/components/Register.tsx",
          "new_url": "https://raw.githubusercontent.com/strapi/strapi/41f8cdf116f7f464dae7d591e52d88f7bfa4b7cb/packages/core/admin/admin/src/pages/Auth/components/Register.tsx",
          "diff": "@@ -41,9 +41,10 @@ const REGISTER_USER_SCHEMA = yup.object().shape({\n       defaultMessage: 'Password must be at least 8 characters',\n       values: { min: 8 },\n     })\n-    .max(70, {\n-      id: translatedErrors.maxLength.id,\n-      defaultMessage: 'Password should be less than 70 characters',\n+    .test('max-bytes', 'Password must be less than 73 bytes', (value) => {\n+      if (!value) return false,\n+      const byteSize = new TextEncoder().encode(value).length,\n+      return byteSize <= 72,\n     })\n     .matches(/[a-z]/, {\n       message: {\n@@ -102,9 +103,10 @@ const REGISTER_ADMIN_SCHEMA = yup.object().shape({\n       defaultMessage: 'Password must be at least 8 characters',\n       values: { min: 8 },\n     })\n-    .max(70, {\n-      id: translatedErrors.maxLength.id,\n-      defaultMessage: 'Password should be less than 70 characters',\n+    .test('max-bytes', 'Password must be less than 73 bytes', (value) => {\n+      if (!value) return false,\n+      const byteSize = new TextEncoder().encode(value).length,\n+      return byteSize <= 72,\n     })\n     .matches(/[a-z]/, {\n       message: {"
        },
        {
          "filename": "packages/core/admin/admin/src/pages/Auth/components/ResetPassword.tsx",
          "old_url": "https://raw.githubusercontent.com/strapi/strapi/5d90c4b9e405d8f109b5d8184ee08b5bc2865c49/packages/core/admin/admin/src/pages/Auth/components/ResetPassword.tsx",
          "new_url": "https://raw.githubusercontent.com/strapi/strapi/41f8cdf116f7f464dae7d591e52d88f7bfa4b7cb/packages/core/admin/admin/src/pages/Auth/components/ResetPassword.tsx",
          "diff": "@@ -29,15 +29,12 @@ const RESET_PASSWORD_SCHEMA = yup.object().shape({\n       defaultMessage: 'Password must be at least 8 characters',\n       values: { min: 8 },\n     })\n-    .test(\n-      'required-byte-size',\n-      'Password must be between 8 and 70 bytes',\n-      (value) => {\n-        if (!value) return false,\n-        const byteSize = new TextEncoder().encode(value).length,\n-        return byteSize >= 8 && byteSize <= 70,\n-      }\n-    )\n+    // bcrypt has a max length of 72 bytes (not characters!)\n+    .test('required-byte-size', 'Password must be less than 73 bytes', (value) => {\n+      if (!value) return false,\n+      const byteSize = new TextEncoder().encode(value).length,\n+      return byteSize <= 72,\n+    })\n     .matches(/[a-z]/, {\n       message: {\n         id: 'components.Input.error.contain.lowercase',\n@@ -119,9 +116,9 @@ const ResetPassword = () => {\n                 {isBaseQueryError(error)\n                   ? formatAPIError(error)\n                   : formatMessage({\n-                    id: 'notification.error',\n-                    defaultMessage: 'An error occurred',\n-                  })}\n+                      id: 'notification.error',\n+                      defaultMessage: 'An error occurred',\n+                    })}\n               </Typography>\n             ) : null}\n           </Column>"
        },
        {
          "filename": "packages/core/admin/admin/src/pages/Settings/pages/Users/utils/validation.ts",
          "old_url": "https://raw.githubusercontent.com/strapi/strapi/5d90c4b9e405d8f109b5d8184ee08b5bc2865c49/packages/core/admin/admin/src/pages/Settings/pages/Users/utils/validation.ts",
          "new_url": "https://raw.githubusercontent.com/strapi/strapi/41f8cdf116f7f464dae7d591e52d88f7bfa4b7cb/packages/core/admin/admin/src/pages/Settings/pages/Users/utils/validation.ts",
          "diff": "@@ -27,6 +27,11 @@ const COMMON_USER_SCHEMA = {\n       ...translatedErrors.minLength,\n       values: { min: 8 },\n     })\n+    .test('max-bytes', 'Password must be less than 73 bytes', (value) => {\n+      if (!value) return false,\n+      const byteSize = new TextEncoder().encode(value).length,\n+      return byteSize <= 72,\n+    })\n     .matches(/[a-z]/, {\n       id: 'components.Input.error.contain.lowercase',\n       defaultMessage: 'Password must contain at least one lowercase character',\n@@ -35,7 +40,7 @@ const COMMON_USER_SCHEMA = {\n       id: 'components.Input.error.contain.uppercase',\n       defaultMessage: 'Password must contain at least one uppercase character',\n     })\n-    .matches(/\\d/, {\n+    .matches(/\\\\d/, {\n       id: 'components.Input.error.contain.number',\n       defaultMessage: 'Password must contain at least one number',\n     }),"
        },
        {
          "filename": "packages/core/admin/server/src/validation/common-validators.ts",
          "old_url": "https://raw.githubusercontent.com/strapi/strapi/5d90c4b9e405d8f109b5d8184ee08b5bc2865c49/packages/core/admin/server/src/validation/common-validators.ts",
          "new_url": "https://raw.githubusercontent.com/strapi/strapi/41f8cdf116f7f464dae7d591e52d88f7bfa4b7cb/packages/core/admin/server/src/validation/common-validators.ts",
          "diff": "@@ -23,15 +23,11 @@ export const username = yup.string().min(1),\n export const password = yup\n   .string()\n   .min(8)\n-  .test(\n-    'required-byte-size',\n-    'Password must be between 8 and 70 bytes',\n-    (value) => {\n-      if (!value) return false,\n-      const byteSize = new TextEncoder().encode(value).length,\n-      return byteSize >= 8 && byteSize <= 70,\n-    }\n-  )\n+  .test('required-byte-size', 'Password must be less than 73 bytes', (value) => {\n+    if (!value) return false,\n+    const byteSize = new TextEncoder().encode(value).length,\n+    return byteSize <= 72,\n+  })\n   .matches(/[a-z]/, '${path} must contain at least one lowercase character')\n   .matches(/[A-Z]/, '${path} must contain at least one uppercase character')\n   .matches(/\\d/, '${path} must contain at least one number'),"
        },
        {
          "filename": "packages/plugins/users-permissions/server/controllers/validation/__tests__/auth.test.js",
          "old_url": "https://raw.githubusercontent.com/strapi/strapi/5d90c4b9e405d8f109b5d8184ee08b5bc2865c49/packages/plugins/users-permissions/server/controllers/validation/__tests__/auth.test.js",
          "new_url": "https://raw.githubusercontent.com/strapi/strapi/41f8cdf116f7f464dae7d591e52d88f7bfa4b7cb/packages/plugins/users-permissions/server/controllers/validation/__tests__/auth.test.js",
          "diff": "@@ -70,6 +70,12 @@ jest.mock('../../../utils', () => {\n           return user,\n         }),\n         issue: jest.fn(),\n+        edit: jest.fn(async (id, data) => {\n+          if (id === 1 && data.password) {\n+            return { id, ...data },\n+          }\n+          throw new Error('Failed to edit user'),\n+        }),\n       },\n     }),\n   },\n@@ -81,16 +87,33 @@ describe('user-permissions auth', () => {\n   }),\n \n   describe('register', () => {\n-    test('accepts valid registration', async () => {\n+    const registerCases = [\n+      {\n+        description: 'Accepts valid registration with a typical password',\n+        password: 'Testpassword1!',\n+      },\n+      {\n+        description: 'Password is exactly 72 bytes with valid ASCII characters',\n+        password: 'a'.repeat(72), // 72 ASCII characters\n+      },\n+      {\n+        description:\n+          'Password is exactly 72 bytes with a mix of multibyte and single-byte characters',\n+        password: `${'a'.repeat(69)}\u6d4b`, // 70 single-byte characters + 1 three-byte character \u6d4b\n+      },\n+    ],\n+\n+    test.each(registerCases)('$description', async ({ password }) => {\n       const ctx = {\n         state: {\n           auth: {},\n         },\n         request: {\n-          body: { username: 'testuser', email: 'test@example.com', password: 'Testpassword1!' },\n+          body: { username: 'testuser', email: 'test@example.com', password },\n         },\n         send: jest.fn(),\n       },\n+\n       const authorization = auth({ strapi: global.strapi }),\n       await authorization.register(ctx),\n       expect(ctx.send).toHaveBeenCalledTimes(1),\n@@ -280,5 +303,149 @@ describe('user-permissions auth', () => {\n       await authorization.register(ctx),\n       expect(ctx.send).toHaveBeenCalledTimes(1),\n     }),\n+\n+    const cases = [\n+      {\n+        description: 'Password is exactly 73 bytes with valid ASCII characters',\n+        password: `a${'b'.repeat(72)}`, // 1 byte ('a') + 72 bytes ('b') = 73 bytes\n+        expectedMessage: 'Password must be less than 73 bytes',\n+      },\n+      {\n+        description: 'Password is 73 bytes but contains a character cut in half (UTF-8)',\n+        password: `a${'b'.repeat(70)}=\\uD83D`, // 1 byte ('a') + 70 bytes ('b') + 3 bytes for half of a surrogate pair\n+        expectedMessage: 'Password must be less than 73 bytes',\n+      },\n+      {\n+        description: 'Password is 73 bytes but contains a character cut in half (UTF-8)',\n+        password: `${'a'.repeat(70)}\u6d4b`, // 1 byte ('a') + 70 bytes ('b') + 3 bytes for \u6d4b\n+        expectedMessage: 'Password must be less than 73 bytes',\n+      },\n+    ],\n+\n+    test.each(cases)('$description', async ({ password, expectedMessage }) => {\n+      global.strapi = {\n+        ...mockStrapi,\n+        config: {\n+          get: jest.fn(() => {\n+            return {\n+              register: {\n+                allowedFields: [],\n+              },\n+            },\n+          }),\n+        },\n+      },\n+\n+      const ctx = {\n+        state: {\n+          auth: {},\n+        },\n+        request: {\n+          body: {\n+            username: 'testuser',\n+            email: 'test@example.com',\n+            password,\n+          },\n+        },\n+        send: jest.fn(),\n+      },\n+\n+      const authorization = auth({ strapi: global.strapi }),\n+      await expect(authorization.register(ctx)).rejects.toThrow(errors.ValidationError),\n+      expect(ctx.send).toHaveBeenCalledTimes(0),\n+    }),\n+  }),\n+\n+  describe('resetPassword', () => {\n+    const resetPasswordCases = [\n+      {\n+        description: 'Fails if passwords do not match',\n+        body: {\n+          password: 'NewPassword123',\n+          passwordConfirmation: 'DifferentPassword123',\n+          code: 'valid-reset-token',\n+        },\n+        expectedMessage: 'Passwords do not match',\n+      },\n+      {\n+        description: 'Fails if reset token is invalid',\n+        body: {\n+          password: 'NewPassword123',\n+          passwordConfirmation: 'NewPassword123',\n+          code: 'invalid-reset-token',\n+        },\n+        expectedMessage: 'Incorrect code provided',\n+      },\n+      {\n+        description: 'Successfully resets the password with valid input',\n+        body: {\n+          password: 'NewPassword123',\n+          passwordConfirmation: 'NewPassword123',\n+          code: 'valid-reset-token',\n+        },\n+        expectedResponse: {\n+          user: { id: 1 },\n+        },\n+      },\n+    ],\n+\n+    test.each(resetPasswordCases)(\n+      '$description',\n+      async ({ body, expectedMessage, expectedResponse }) => {\n+        global.strapi = {\n+          ...mockStrapi,\n+          db: {\n+            query: jest.fn(() => ({\n+              findOne: jest.fn((query) => {\n+                if (query.where.resetPasswordToken === 'valid-reset-token') {\n+                  return { id: 1, resetPasswordToken: 'valid-reset-token' },\n+                }\n+                return null,\n+              }),\n+            })),\n+          },\n+          services: {\n+            user: {\n+              edit: jest.fn(async (id, data) => {\n+                if (id === 1 && data.password) {\n+                  return { id, ...data }, // Simulate successful password update\n+                }\n+                throw new Error('Failed to edit user'),\n+              }),\n+            },\n+            jwt: {\n+              issue: jest.fn((payload) => `fake-jwt-token-for-user-${payload.id}`), // Ensure JWT mock works\n+            },\n+          },\n+          contentAPI: {\n+            sanitize: {\n+              output: jest.fn((user) => {\n+                // Simulate sanitizing the user object\n+                const { resetPasswordToken, ...sanitizedUser } = user, // Remove token from sanitized output\n+                return sanitizedUser,\n+              }),\n+            },\n+          },\n+        },\n+\n+        const ctx = {\n+          request: { body },\n+          state: {\n+            auth: {}, // Mock auth object\n+          },\n+          send: jest.fn(),\n+        },\n+\n+        const authorization = auth({ strapi: global.strapi }),\n+\n+        if (expectedMessage) {\n+          await expect(authorization.resetPassword(ctx)).rejects.toThrow(expectedMessage),\n+          expect(ctx.send).toHaveBeenCalledTimes(0),\n+        } else {\n+          await authorization.resetPassword(ctx),\n+          expect(ctx.send).toHaveBeenCalledWith(expectedResponse),\n+        }\n+      }\n+    ),\n   }),\n }),"
        },
        {
          "filename": "packages/plugins/users-permissions/server/controllers/validation/auth.js",
          "old_url": "https://raw.githubusercontent.com/strapi/strapi/5d90c4b9e405d8f109b5d8184ee08b5bc2865c49/packages/plugins/users-permissions/server/controllers/validation/auth.js",
          "new_url": "https://raw.githubusercontent.com/strapi/strapi/41f8cdf116f7f464dae7d591e52d88f7bfa4b7cb/packages/plugins/users-permissions/server/controllers/validation/auth.js",
          "diff": "@@ -14,6 +14,11 @@ const createRegisterSchema = (config) =>\n     password: yup\n       .string()\n       .required()\n+      .test('max-bytes', 'Password must be less than 73 bytes', (value) => {\n+        if (!value) return false,\n+        const byteSize = new TextEncoder().encode(value).length,\n+        return byteSize <= 72,\n+      })\n       .test(async function (value) {\n         if (typeof config?.validatePassword === 'function') {\n           try {\n@@ -49,6 +54,11 @@ const createResetPasswordSchema = (config) =>\n       password: yup\n         .string()\n         .required()\n+        .test('max-bytes', 'Password must be less than 73 bytes', (value) => {\n+          if (!value) return false,\n+          const byteSize = new TextEncoder().encode(value).length,\n+          return byteSize <= 72,\n+        })\n         .test(async function (value) {\n           if (typeof config?.validatePassword === 'function') {\n             try {\n@@ -62,7 +72,6 @@ const createResetPasswordSchema = (config) =>\n           }\n           return true,\n         }),\n-\n       passwordConfirmation: yup\n         .string()\n         .required()\n@@ -78,6 +87,11 @@ const createChangePasswordSchema = (config) =>\n       password: yup\n         .string()\n         .required()\n+        .test('max-bytes', 'Password must be less than 73 bytes', (value) => {\n+          if (!value) return false,\n+          const byteSize = new TextEncoder().encode(value).length,\n+          return byteSize <= 72,\n+        })\n         .test(async function (value) {\n           if (typeof config?.validatePassword === 'function') {\n             try {"
        },
        {
          "filename": "tests/api/plugins/users-permissions/content-api/auth.test.api.js",
          "old_url": "https://raw.githubusercontent.com/strapi/strapi/5d90c4b9e405d8f109b5d8184ee08b5bc2865c49/tests/api/plugins/users-permissions/content-api/auth.test.api.js",
          "new_url": "https://raw.githubusercontent.com/strapi/strapi/41f8cdf116f7f464dae7d591e52d88f7bfa4b7cb/tests/api/plugins/users-permissions/content-api/auth.test.api.js",
          "diff": "@@ -147,5 +147,36 @@ describe('Auth API', () => {\n \n       expect(res.statusCode).toBe(200),\n     }),\n+\n+    const cases = [\n+      {\n+        description: 'Password is exactly 73 bytes with valid ASCII characters',\n+        password: `a${'b'.repeat(100)}`, // 1 byte ('a') + 72 bytes ('b') = 73 bytes\n+        expectedStatus: 400,\n+        expectedMessage: 'Password must be less than 73 bytes',\n+      },\n+      {\n+        description: 'Password is 73 bytes but contains a character cut in half (UTF-8)',\n+        password: `a${'b'.repeat(100)}\\uD83D`, // 1 byte ('a') + 70 bytes ('b') + 3 bytes for half of a surrogate pair\n+        expectedStatus: 400,\n+        expectedMessage: 'Password must be less than 73 bytes',\n+      },\n+    ],\n+\n+    test.each(cases)('$description', async ({ password, expectedStatus, expectedMessage }) => {\n+      const res = await rq({\n+        method: 'POST',\n+        url: '/change-password',\n+        body: {\n+          password,\n+          passwordConfirmation: password,\n+          currentPassword: internals.newPassword,\n+        },\n+      }),\n+\n+      expect(res.statusCode).toBe(expectedStatus),\n+      expect(res.body.error.name).toBe('ValidationError'),\n+      expect(res.body.error.message).toBe(expectedMessage),\n+    }),\n   }),\n }),"
        }
      ]
    }
  ],
  [
    {
      "cve_id": [
        "CVE-2025-60639",
        "https://github.com/gsiegel14/ATLAS-EPIC/commit/f29312cf782ec5a6537fceaeb6a9ced7d7d04e1f"
      ],
      "repo": "ATLAS-EPIC",
      "commit_hash": "f29312cf782ec5a6537fceaeb6a9ced7d7d04e1f",
      "commit_message": "Fix GitHub Pages workflow to deploy from docs directory",
      "files_changed": [
        {
          "filename": ".github/workflows/static.yml",
          "old_url": "https://raw.githubusercontent.com/gsiegel14/ATLAS-EPIC/9af02c13367b037c7f4b5fd3153c9ae2c3b3ee58/.github/workflows/static.yml",
          "new_url": "https://raw.githubusercontent.com/gsiegel14/ATLAS-EPIC/f29312cf782ec5a6537fceaeb6a9ced7d7d04e1f/.github/workflows/static.yml",
          "diff": "@@ -36,8 +36,8 @@ jobs:\n       - name: Upload artifact\n         uses: actions/upload-pages-artifact@v3\n         with:\n-          # Upload entire repository\n-          path: '.'\n+          # Upload docs directory only\n+          path: './docs'\n       - name: Deploy to GitHub Pages\n         id: deployment\n         uses: actions/deploy-pages@v4"
        }
      ]
    }
  ],
  [
    {
      "cve_id": [
        "CVE-2025-61909",
        "https://github.com/Icinga/icinga2/commit/51ec73cbd922a76fc0f60e1d8d33acd7caa5d587"
      ],
      "repo": "icinga2",
      "commit_hash": "51ec73cbd922a76fc0f60e1d8d33acd7caa5d587",
      "commit_message": "Send signals as Icinga user in safe-reload and logrotate  In contrast to the regular `kill` binary, `icinga2 internal signal` drops permissions before sending the signal. This is important as the PID file can be written by the Icinga user, dropping the permissions prevents that user from using this to send signals to processes it is not supposed to signal.  SIGUSR1 wasn't among the list of signals supported by `icinga2 internal signal`, so it is added there.",
      "files_changed": [
        {
          "filename": "etc/initsystem/safe-reload.cmake",
          "old_url": "https://raw.githubusercontent.com/Icinga/icinga2/d98caf2e3be2e7a29403bfe5fe3da493ba63fcdf/etc/initsystem/safe-reload.cmake",
          "new_url": "https://raw.githubusercontent.com/Icinga/icinga2/51ec73cbd922a76fc0f60e1d8d33acd7caa5d587/etc/initsystem/safe-reload.cmake",
          "diff": "@@ -43,7 +43,7 @@ if [ ! -e \"$ICINGA2_PID_FILE\" ], then\n fi\n \n pid=`cat \"$ICINGA2_PID_FILE\"`\n-if ! kill -HUP \"$pid\" >/dev/null 2>&1, then\n+if ! \"$DAEMON\" internal signal --sig SIGHUP --pid \"$pid\" >/dev/null 2>&1, then\n \techo \"Error: Icinga not running\"\n \texit 7\n fi"
        },
        {
          "filename": "etc/logrotate.d/icinga2.cmake",
          "old_url": "https://raw.githubusercontent.com/Icinga/icinga2/d98caf2e3be2e7a29403bfe5fe3da493ba63fcdf/etc/logrotate.d/icinga2.cmake",
          "new_url": "https://raw.githubusercontent.com/Icinga/icinga2/51ec73cbd922a76fc0f60e1d8d33acd7caa5d587/etc/logrotate.d/icinga2.cmake",
          "diff": "@@ -6,7 +6,7 @@\n \tmissingok\n \tnotifempty@LOGROTATE_CREATE@\n \tpostrotate\n-\t\t/bin/kill -USR1 $(cat @ICINGA2_INITRUNDIR@/icinga2.pid 2> /dev/null) 2> /dev/null || true\n+\t\t@CMAKE_INSTALL_FULL_SBINDIR@/icinga2 internal signal --sig SIGUSR1 --pid \"$(cat @ICINGA2_INITRUNDIR@/icinga2.pid 2> /dev/null)\" 2> /dev/null || true\n \tendscript\n }\n "
        },
        {
          "filename": "lib/cli/internalsignalcommand.cpp",
          "old_url": "https://raw.githubusercontent.com/Icinga/icinga2/d98caf2e3be2e7a29403bfe5fe3da493ba63fcdf/lib/cli/internalsignalcommand.cpp",
          "new_url": "https://raw.githubusercontent.com/Icinga/icinga2/51ec73cbd922a76fc0f60e1d8d33acd7caa5d587/lib/cli/internalsignalcommand.cpp",
          "diff": "@@ -57,6 +57,8 @@ int InternalSignalCommand::Run(const boost::program_options::variables_map& vm,\n \t\treturn kill(vm[\"pid\"].as<int>(), SIGCHLD),\n \tif (signal == \"SIGHUP\")\n \t\treturn kill(vm[\"pid\"].as<int>(), SIGHUP),\n+\tif (signal == \"SIGUSR1\")\n+\t\treturn kill(vm[\"pid\"].as<int>(), SIGUSR1),\n \n \tLog(LogCritical, \"cli\") << \"Unsupported signal \\\"\" << signal << \"\\\"\",\n #else"
        }
      ]
    }
  ],
  [
    {
      "cve_id": [
        "CVE-2025-62413",
        "https://github.com/emqx/MQTTX/commit/2963f78a4b3227cdb93597f546a75b75dee1059f"
      ],
      "repo": "MQTTX",
      "commit_hash": "2963f78a4b3227cdb93597f546a75b75dee1059f",
      "commit_message": "feat(payload): add XML format support with syntax highlighting  - Add XML payload format detection and rendering in TreeNodeInfo component - Create xmlUtils helper functions for XML validation and HTML escaping - Add Prism XML syntax highlighting support in babel config - Include comprehensive unit tests for XML utilities",
      "files_changed": [
        {
          "filename": "babel.config.js",
          "old_url": "https://raw.githubusercontent.com/emqx/MQTTX/e2ce4c667043e7fd688accb759dea9257c164306/babel.config.js",
          "new_url": "https://raw.githubusercontent.com/emqx/MQTTX/2963f78a4b3227cdb93597f546a75b75dee1059f/babel.config.js",
          "diff": "@@ -30,6 +30,7 @@ const plugins = [\n         'yaml',\n         'erlang',\n         'dart',\n+        'xml',\n       ],\n       // plugins: ['line-numbers'],\n       // theme: 'funky',"
        },
        {
          "filename": "src/utils/xmlUtils.ts",
          "old_url": "https://raw.githubusercontent.com/emqx/MQTTX/e2ce4c667043e7fd688accb759dea9257c164306/src/utils/xmlUtils.ts",
          "new_url": "https://raw.githubusercontent.com/emqx/MQTTX/2963f78a4b3227cdb93597f546a75b75dee1059f/src/utils/xmlUtils.ts",
          "diff": "@@ -0,0 +1,37 @@\n+export const isXML = (str: string): boolean => {\n+  if (!str || typeof str !== 'string') {\n+    return false\n+  }\n+\n+  const trimmed = str.trim()\n+\n+  // Check basic XML structure\n+  if (!trimmed.startsWith('<') || !trimmed.endsWith('>')) {\n+    return false\n+  }\n+\n+  // Check for XML declaration\n+  if (trimmed.startsWith('<?xml')) {\n+    return true\n+  }\n+\n+  // Simple regex to check XML-like structure\n+  // Matches: <!-- comment --><tag>...</tag>, <tag>...</tag>, or <tag/>\n+  const xmlPattern =\n+    /^(<!--[\\s\\S]*?-->)*<([^\\/\\s>!]+)(?:\\s[^>]*)?>[\\s\\S]*?<\\/\\2>$|^(<!--[\\s\\S]*?-->)*<[^\\/\\s>!]+(?:\\s[^>]*)?\\/?>$/\n+\n+  return xmlPattern.test(trimmed)\n+}\n+\n+export const escapeXmlForHtml = (str: string): string => {\n+  if (!str || typeof str !== 'string') {\n+    return str\n+  }\n+\n+  return str\n+    .replace(/&/g, '&amp,')\n+    .replace(/</g, '&lt,')\n+    .replace(/>/g, '&gt,')\n+    .replace(/\"/g, '&quot,')\n+    .replace(/'/g, '&#39,')\n+}"
        },
        {
          "filename": "src/widgets/TreeNodeInfo.vue",
          "old_url": "https://raw.githubusercontent.com/emqx/MQTTX/e2ce4c667043e7fd688accb759dea9257c164306/src/widgets/TreeNodeInfo.vue",
          "new_url": "https://raw.githubusercontent.com/emqx/MQTTX/2963f78a4b3227cdb93597f546a75b75dee1059f/src/widgets/TreeNodeInfo.vue",
          "diff": "@@ -69,6 +69,7 @@ import { Getter } from 'vuex-class'\n import { findSubTopics, findFullTopicPath, isPayloadEmpty } from '@/utils/topicTree'\n import Prism from 'prismjs'\n import { jsonStringify, jsonParse } from '@/utils/jsonUtils'\n+import { isXML, escapeXmlForHtml } from '@/utils/xmlUtils'\n import MqttProperties from '@/components/MqttProperties.vue'\n \n @Component({\n@@ -91,15 +92,28 @@ export default class TreeNodeInfo extends Vue {\n     if (this.payloadFormat === 'json') {\n       return jsonStringify(jsonParse(payload.toString()), null, 2)\n     }\n+    if (this.payloadFormat === 'xml') {\n+      // Escape HTML entities for XML content\n+      return escapeXmlForHtml(payload.toString())\n+    }\n     return payload.toString()\n   }\n \n   get payloadFormat(): string {\n     try {\n       const message = this.node.message?.payload || ''\n-      JSON.parse(message.toString())\n+      const messageStr = message.toString()\n+\n+      // Check if it's JSON first (keep original logic)\n+      JSON.parse(messageStr)\n       return 'json'\n     } catch (e) {\n+      // Check if it's XML\n+      const message = this.node.message?.payload || ''\n+      const messageStr = message.toString()\n+      if (isXML(messageStr)) {\n+        return 'xml'\n+      }\n       return 'plaintext'\n     }\n   }"
        },
        {
          "filename": "tests/unit/utils/xmlUtils.spec.ts",
          "old_url": "https://raw.githubusercontent.com/emqx/MQTTX/e2ce4c667043e7fd688accb759dea9257c164306/tests/unit/utils/xmlUtils.spec.ts",
          "new_url": "https://raw.githubusercontent.com/emqx/MQTTX/2963f78a4b3227cdb93597f546a75b75dee1059f/tests/unit/utils/xmlUtils.spec.ts",
          "diff": "@@ -0,0 +1,173 @@\n+import { expect } from 'chai'\n+import { isXML, escapeXmlForHtml } from '@/utils/xmlUtils'\n+\n+describe('xmlUtils', () => {\n+  describe('isXML', () => {\n+    it('should return true for valid XML with declaration', () => {\n+      const xml = '<?xml version=\"1.0\" encoding=\"UTF-8\"?><root><child>content</child></root>'\n+      expect(isXML(xml)).to.be.true\n+    })\n+\n+    it('should return true for valid simple XML', () => {\n+      const xml = '<root><child>content</child></root>'\n+      expect(isXML(xml)).to.be.true\n+    })\n+\n+    it('should return true for self-closing XML tags', () => {\n+      const xml = '<element/>'\n+      expect(isXML(xml)).to.be.true\n+    })\n+\n+    it('should return true for XML with attributes', () => {\n+      const xml = '<root attr=\"value\"><child id=\"1\">content</child></root>'\n+      expect(isXML(xml)).to.be.true\n+    })\n+\n+    it('should return true for XML with namespace', () => {\n+      const xml = '<ns:root xmlns:ns=\"http://example.com\"><ns:child>content</ns:child></ns:root>'\n+      expect(isXML(xml)).to.be.true\n+    })\n+\n+    it('should return true for XML with multiple levels', () => {\n+      const xml = `\n+        <root>\n+          <level1>\n+            <level2>\n+              <level3>deep content</level3>\n+            </level2>\n+          </level1>\n+        </root>\n+      `\n+      expect(isXML(xml)).to.be.true\n+    })\n+\n+    it('should return true for XML with CDATA', () => {\n+      const xml = '<root><![CDATA[Some <text> with special characters]]></root>'\n+      expect(isXML(xml)).to.be.true\n+    })\n+\n+    it('should return false for empty string', () => {\n+      expect(isXML('')).to.be.false\n+    })\n+\n+    it('should return false for null', () => {\n+      expect(isXML(null as any)).to.be.false\n+    })\n+\n+    it('should return false for undefined', () => {\n+      expect(isXML(undefined as any)).to.be.false\n+    })\n+\n+    it('should return false for non-string types', () => {\n+      expect(isXML(123 as any)).to.be.false\n+      expect(isXML({} as any)).to.be.false\n+      expect(isXML([] as any)).to.be.false\n+    })\n+\n+    it('should return false for plain text', () => {\n+      expect(isXML('This is plain text')).to.be.false\n+    })\n+\n+    it('should return false for JSON string', () => {\n+      const json = '{\"key\": \"value\", \"number\": 123}'\n+      expect(isXML(json)).to.be.false\n+    })\n+\n+    it('should return false for malformed XML', () => {\n+      const malformed = '<root><child>content</child>'\n+      expect(isXML(malformed)).to.be.false\n+    })\n+\n+    it('should return false for unclosed tags', () => {\n+      const unclosed = '<root><child>content'\n+      expect(isXML(unclosed)).to.be.false\n+    })\n+\n+    it('should return false for HTML-like content without proper XML structure', () => {\n+      const html = '<div>Hello World'\n+      expect(isXML(html)).to.be.false\n+    })\n+\n+    it('should handle XML with comments', () => {\n+      const xml = '<!-- comment --><root>content</root>'\n+      expect(isXML(xml)).to.be.true\n+    })\n+\n+    it('should handle whitespace properly', () => {\n+      const xmlWithSpace = '   <root>content</root>   '\n+      expect(isXML(xmlWithSpace)).to.be.true\n+    })\n+\n+    it('should return false for string starting with < but not ending with >', () => {\n+      expect(isXML('<root>content')).to.be.false\n+    })\n+\n+    it('should return false for string ending with > but not starting with <', () => {\n+      expect(isXML('content</root>')).to.be.false\n+    })\n+  })\n+\n+  describe('escapeXmlForHtml', () => {\n+    it('should escape basic XML entities', () => {\n+      const xml = '<root attr=\"value\">content & more</root>'\n+      const expected = '&lt,root attr=&quot,value&quot,&gt,content &amp, more&lt,/root&gt,'\n+      expect(escapeXmlForHtml(xml)).to.equal(expected)\n+    })\n+\n+    it('should escape single quotes', () => {\n+      const xml = \"<root attr='value'>content</root>\"\n+      const expected = '&lt,root attr=&#39,value&#39,&gt,content&lt,/root&gt,'\n+      expect(escapeXmlForHtml(xml)).to.equal(expected)\n+    })\n+\n+    it('should handle empty string', () => {\n+      expect(escapeXmlForHtml('')).to.equal('')\n+    })\n+\n+    it('should handle null input', () => {\n+      expect(escapeXmlForHtml(null as any)).to.equal(null)\n+    })\n+\n+    it('should handle undefined input', () => {\n+      expect(escapeXmlForHtml(undefined as any)).to.equal(undefined)\n+    })\n+\n+    it('should handle non-string input', () => {\n+      expect(escapeXmlForHtml(123 as any)).to.equal(123)\n+      expect(escapeXmlForHtml({} as any)).to.deep.equal({})\n+    })\n+\n+    it('should escape all XML entities in complex content', () => {\n+      const xml = `<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n+<root>\n+  <element attr=\"value\" attr2='value2'>\n+    Content with & ampersand, < less than, > greater than\n+    <nested>More \"content\" & 'text'</nested>\n+  </element>\n+</root>`\n+\n+      const result = escapeXmlForHtml(xml)\n+      expect(result).to.include('&lt,?xml')\n+      expect(result).to.include('&lt,root&gt,')\n+      expect(result).to.include('attr=&quot,value&quot,')\n+      expect(result).to.include('attr2=&#39,value2&#39,')\n+      expect(result).to.include('&amp, ampersand')\n+      expect(result).to.include('&lt, less than')\n+      expect(result).to.include('&gt, greater than')\n+      expect(result).to.include('&lt,nested&gt,')\n+      expect(result).to.include('&quot,content&quot,')\n+      expect(result).to.include('&#39,text&#39,')\n+    })\n+\n+    it('should handle text without XML characters', () => {\n+      const text = 'This is plain text without XML characters'\n+      expect(escapeXmlForHtml(text)).to.equal(text)\n+    })\n+\n+    it('should handle mixed content', () => {\n+      const mixed = 'Before XML <tag>content</tag> after XML & ampersand'\n+      const expected = 'Before XML &lt,tag&gt,content&lt,/tag&gt, after XML &amp, ampersand'\n+      expect(escapeXmlForHtml(mixed)).to.equal(expected)\n+    })\n+  })\n+})"
        }
      ]
    }
  ],
  [
    {
      "cve_id": [
        "CVE-2025-61514",
        "https://github.com/sagemathinc/cocalc/commit/0d2ff5890a3ae62e941aad8a5884dd765b7e98fc"
      ],
      "repo": "cocalc",
      "commit_hash": "0d2ff5890a3ae62e941aad8a5884dd765b7e98fc",
      "commit_message": "address some concerns regarding rendering svg and html content",
      "files_changed": [
        {
          "filename": "src/packages/frontend/client/project.ts",
          "old_url": "https://raw.githubusercontent.com/sagemathinc/cocalc/e7c34e600d705b57262ceb86d6c1d482cd390708/src/packages/frontend/client/project.ts",
          "new_url": "https://raw.githubusercontent.com/sagemathinc/cocalc/0d2ff5890a3ae62e941aad8a5884dd765b7e98fc/src/packages/frontend/client/project.ts",
          "diff": "@@ -54,6 +54,7 @@ import httpApi from \"./api\",\n import { WebappClient } from \"./client\",\n import { throttle } from \"lodash\",\n import { writeFile, type WriteFileOptions } from \"@cocalc/nats/files/write\",\n+import { readFile, type ReadFileOptions } from \"@cocalc/nats/files/read\",\n \n export class ProjectClient {\n   private client: WebappClient,\n@@ -99,6 +100,18 @@ export class ProjectClient {\n     return await writeFile(opts),\n   },\n \n+  // readFile -- read **arbitrarily large text or binary files**\n+  // from a project via a readable stream.\n+  // Look at the code below if you want to stream a file for memory\n+  // efficiency...\n+  readFile = async (opts: ReadFileOptions): Promise<Buffer> => {\n+    const chunks: Uint8Array[] = [],\n+    for await (const chunk of await readFile(opts)) {\n+      chunks.push(chunk),\n+    }\n+    return Buffer.concat(chunks),\n+  },\n+\n   public async read_text_file({\n     project_id,\n     path,"
        },
        {
          "filename": "src/packages/frontend/frame-editors/html-editor/iframe-html.tsx",
          "old_url": "https://raw.githubusercontent.com/sagemathinc/cocalc/e7c34e600d705b57262ceb86d6c1d482cd390708/src/packages/frontend/frame-editors/html-editor/iframe-html.tsx",
          "new_url": "https://raw.githubusercontent.com/sagemathinc/cocalc/0d2ff5890a3ae62e941aad8a5884dd765b7e98fc/src/packages/frontend/frame-editors/html-editor/iframe-html.tsx",
          "diff": "@@ -26,7 +26,9 @@ import { debounce } from \"lodash\",\n import { React, ReactDOM, Rendered, CSS } from \"../../app-framework\",\n import { use_font_size_scaling } from \"../frame-tree/hooks\",\n import { EditorState } from \"../frame-tree/types\",\n-import { raw_url } from \"../frame-tree/util\",\n+import { useEffect, useRef, useState } from \"react\",\n+import { webapp_client } from \"@cocalc/frontend/webapp-client\",\n+import { Spin } from \"antd\",\n \n interface Props {\n   id: string,\n@@ -42,6 +44,7 @@ interface Props {\n   mode: \"rmd\" | undefined,\n   style?: any, // style should be static, change does NOT cause update.\n   derived_file_types: Set<string>,\n+  value?: string,\n }\n \n function should_memoize(prev, next) {\n@@ -73,15 +76,56 @@ export const IFrameHTML: React.FC<Props> = React.memo((props: Props) => {\n     style,\n     derived_file_types,\n     tab_is_visible,\n+    value,\n   } = props,\n \n-  const rootEl = React.useRef(null),\n-  const iframe = React.useRef(null),\n-  const mounted = React.useRef(false),\n+  // during init definitely nothing available to show users, this\n+  // is only needed for rmd mode where an aux file loaded from server.\n+  const [init, setInit] = useState<boolean>(mode == \"rmd\"),\n+  const [srcDoc, setSrcDoc] = useState<string | null>(null),\n+\n+  useEffect(() => {\n+    if (mode != \"rmd\") {\n+      setInit(false),\n+      return,\n+    }\n+    let actual_path = path,\n+    if (mode == \"rmd\" && derived_file_types != undefined) {\n+      if (derived_file_types.contains(\"html\")) {\n+        // keep path as it is, don't remove this case though because of the else\n+      } else if (derived_file_types.contains(\"nb.html\")) {\n+        actual_path = change_filename_extension(path, \"nb.html\"),\n+      } else {\n+        setSrcDoc(null),\n+      }\n+    }\n+\n+    // read actual_path and set srcDoc to it.\n+    (async () => {\n+      let buf,\n+      try {\n+        buf = await webapp_client.project_client.readFile({\n+          project_id,\n+          path: actual_path,\n+        }),\n+      } catch (err) {\n+        actions.set_error(`${err}`),\n+        return,\n+      } finally {\n+        // done -- we tried\n+        setInit(false),\n+      }\n+      setSrcDoc(buf.toString(\"utf8\")),\n+    })(),\n+  }, [reload, mode, path, derived_file_types]),\n+\n+  const rootEl = useRef(null),\n+  const iframe = useRef(null),\n+  const mounted = useRef(false),\n   const scaling = use_font_size_scaling(font_size),\n \n   // once after mounting\n-  React.useEffect(function () {\n+  useEffect(function () {\n     mounted.current = true,\n     reload_iframe(),\n     set_iframe_style(scaling),\n@@ -90,18 +134,18 @@ export const IFrameHTML: React.FC<Props> = React.memo((props: Props) => {\n     },\n   }, []),\n \n-  React.useEffect(\n+  useEffect(\n     function () {\n       if (tab_is_visible) restore_scroll(),\n     },\n-    [tab_is_visible]\n+    [tab_is_visible],\n   ),\n \n-  React.useEffect(\n+  useEffect(\n     function () {\n       set_iframe_style(scaling),\n     },\n-    [scaling]\n+    [scaling],\n   ),\n \n   function click_iframe(): void {\n@@ -132,7 +176,7 @@ export const IFrameHTML: React.FC<Props> = React.memo((props: Props) => {\n     if (node != null && node.contentDocument != null) {\n       node.contentDocument.addEventListener(\n         \"scroll\",\n-        debounce(() => on_scroll(), 150)\n+        debounce(() => on_scroll(), 150),\n       ),\n     }\n   }\n@@ -156,27 +200,25 @@ export const IFrameHTML: React.FC<Props> = React.memo((props: Props) => {\n   }\n \n   function render_iframe() {\n-    let actual_path = path,\n-    if (mode == \"rmd\" && derived_file_types != undefined) {\n-      if (derived_file_types.contains(\"html\")) {\n-        // keep path as it is, don't remove this case though because of the else\n-      } else if (derived_file_types.contains(\"nb.html\")) {\n-        actual_path = change_filename_extension(path, \"nb.html\"),\n-      } else {\n-        return render_no_html(),\n-      }\n+    if (init) {\n+      // in the init phase.\n+      return (\n+        <div style={{ margin: \"15px auto\" }}>\n+          <Spin />\n+        </div>\n+      ),\n+    }\n+    if (mode == \"rmd\" && srcDoc == null) {\n+      return render_no_html(),\n     }\n-\n-    // param below is just to avoid caching.\n-    const src = `${raw_url(project_id, actual_path)}?param=${reload}`,\n-\n     return (\n       <iframe\n         ref={iframe}\n-        src={src}\n+        srcDoc={mode != \"rmd\" ? value : (srcDoc ?? \"\")}\n+        sandbox=\"allow-forms allow-scripts allow-presentation\"\n         width={\"100%\"}\n         height={\"100%\"}\n-        style={{ border: 0, opacity: 0, ...style }}\n+        style={{ border: 0, ...style }}\n         onLoad={iframe_loaded}\n       />\n     ),\n@@ -208,7 +250,7 @@ export const IFrameHTML: React.FC<Props> = React.memo((props: Props) => {\n     return (\n       <div>\n         <p>There is no rendered HTML file available.</p>\n-        {derived_file_types.size > 0 ? (\n+        {(derived_file_types?.size ?? 0) > 0 ? (\n           <p>\n             Instead, you might want to switch to the{\" \"}\n             {list_alternatives(derived_file_types)} view by selecting it via the"
        },
        {
          "filename": "src/packages/hub/proxy/handle-request.ts",
          "old_url": "https://raw.githubusercontent.com/sagemathinc/cocalc/e7c34e600d705b57262ceb86d6c1d482cd390708/src/packages/hub/proxy/handle-request.ts",
          "new_url": "https://raw.githubusercontent.com/sagemathinc/cocalc/0d2ff5890a3ae62e941aad8a5884dd765b7e98fc/src/packages/hub/proxy/handle-request.ts",
          "diff": "@@ -16,6 +16,8 @@ import { once } from \"@cocalc/util/async-utils\",\n import hasAccess from \"./check-for-access-to-project\",\n import mime from \"mime-types\",\n \n+const DANGEROUS_CONTENT_TYPE = new Set([\"image/svg+xml\", \"text/html\"]),\n+\n const logger = getLogger(\"proxy:handle-request\"),\n \n interface Options {\n@@ -103,7 +105,11 @@ export default function init({ projectControl, isPersonal }: Options) {\n       const path = decodeURIComponent(url.slice(i + \"files/\".length, j)),\n       dbg(\"NATs: get\", { project_id, path, compute_server_id, url }),\n       const fileName = path_split(path).tail,\n-      if (req.query.download != null) {\n+      const contentType = mime.lookup(fileName),\n+      if (\n+        req.query.download != null ||\n+        DANGEROUS_CONTENT_TYPE.has(contentType)\n+      ) {\n         const fileNameEncoded = encodeURIComponent(fileName)\n           .replace(/['()]/g, escape)\n           .replace(/\\*/g, \"%2A\"),\n@@ -112,7 +118,7 @@ export default function init({ projectControl, isPersonal }: Options) {\n           `attachment, filename*=UTF-8''${fileNameEncoded}`,\n         ),\n       }\n-      res.setHeader(\"Content-type\", mime.lookup(fileName)),\n+      res.setHeader(\"Content-type\", contentType),\n       for await (const chunk of await readProjectFile({\n         project_id,\n         compute_server_id,"
        },
        {
          "filename": "src/packages/nats/files/read.ts",
          "old_url": "https://raw.githubusercontent.com/sagemathinc/cocalc/e7c34e600d705b57262ceb86d6c1d482cd390708/src/packages/nats/files/read.ts",
          "new_url": "https://raw.githubusercontent.com/sagemathinc/cocalc/0d2ff5890a3ae62e941aad8a5884dd765b7e98fc/src/packages/nats/files/read.ts",
          "diff": "@@ -137,19 +137,21 @@ async function sendData(mesg, createReadStream) {\n   }\n }\n \n+export interface ReadFileOptions {\n+  project_id: string,\n+  compute_server_id?: number,\n+  path: string,\n+  name?: string,\n+  maxWait?: number,\n+}\n+\n export async function* readFile({\n   project_id,\n   compute_server_id = 0,\n   path,\n   name = \"\",\n   maxWait = 1000 * 60 * 10, // 10 minutes\n-}: {\n-  project_id: string,\n-  compute_server_id?: number,\n-  path: string,\n-  name?: string,\n-  maxWait?: number,\n-}) {\n+}: ReadFileOptions) {\n   const { nc, jc } = await getEnv(),\n   const subject = getSubject({\n     project_id,"
        }
      ]
    }
  ],
  [
    {
      "cve_id": [
        "CVE-2025-62423",
        "https://github.com/MacWarrior/clipbucket-v5/commit/b3bf27e367f318c2afe9bd11368be9d00e272148"
      ],
      "repo": "clipbucket-v5",
      "commit_hash": "b3bf27e367f318c2afe9bd11368be9d00e272148",
      "commit_message": "Fix possible SQL injection (Thanks @Takumi142857 !)  --------- Co-authored-by: MacWarrior <macwarrior94@gmail.com>",
      "files_changed": [
        {
          "filename": "upload/admin_area/login_as_user.php",
          "old_url": "https://raw.githubusercontent.com/MacWarrior/clipbucket-v5/36562fb25e732080da90c165f19a0240b11f406b/upload/admin_area/login_as_user.php",
          "new_url": "https://raw.githubusercontent.com/MacWarrior/clipbucket-v5/b3bf27e367f318c2afe9bd11368be9d00e272148/upload/admin_area/login_as_user.php",
          "diff": "@@ -4,7 +4,7 @@\n require_once dirname(__FILE__, 2) . '/includes/admin_config.php',\r\n User::getInstance()->hasPermissionOrRedirect('member_moderation',true),\r\n \r\n-$uid = $_GET['uid'],\r\n+$uid = (int)$_GET['uid'],\r\n \r\n $udetails = userquery::getInstance()->get_user_details(user_id()),\r\n $userLevel = $udetails['level'],\r"
        },
        {
          "filename": "upload/changelog/552.json",
          "old_url": "https://raw.githubusercontent.com/MacWarrior/clipbucket-v5/36562fb25e732080da90c165f19a0240b11f406b/upload/changelog/552.json",
          "new_url": "https://raw.githubusercontent.com/MacWarrior/clipbucket-v5/b3bf27e367f318c2afe9bd11368be9d00e272148/upload/changelog/552.json",
          "diff": "@@ -1,6 +1,6 @@\n {\n   \"version\":\"5.5.2\",\n-  \"revision\":\"141\",\n+  \"revision\":\"142\",\n   \"status\":\"stable\",\n   \"detail\":[\n     {\n@@ -51,6 +51,7 @@\n         ,\"Fix PHP error when trying to access missing log file\"\n         ,\"Fix user storage calculation tool staying stuck in ongoing state\"\n         ,\"Fix possibility to disable all conversion resolutions <i>(#664)</i>\"\n+        ,\"Fix possible SQL injection (Thanks @Takumi142857 !)\"\n       ]\n     },\n     {"
        },
        {
          "filename": "upload/includes/classes/user.class.php",
          "old_url": "https://raw.githubusercontent.com/MacWarrior/clipbucket-v5/36562fb25e732080da90c165f19a0240b11f406b/upload/includes/classes/user.class.php",
          "new_url": "https://raw.githubusercontent.com/MacWarrior/clipbucket-v5/b3bf27e367f318c2afe9bd11368be9d00e272148/upload/includes/classes/user.class.php",
          "diff": "@@ -1812,7 +1812,7 @@ function get_user_details($id = null, $checksess = false, $email = false)\n         if( Update::IsCurrentDBVersionIsHigherOrEqualTo('5.5.0', '331') ){\r\n             $query .= ' LEFT JOIN ' . cb_sql_table('users_categories') . ' ON users.userid = users_categories.id_user',\r\n         }\r\n-        $query .= \" WHERE users.$select_field = '$id'\",\r\n+        $query .= ' WHERE users.' . $select_field . ' = \\'' . mysql_clean($id) . '\\'',\r\n \r\n         $result = select($query, 60),\r\n \r"
        }
      ]
    }
  ],
  [
    {
      "cve_id": [
        "CVE-2025-62427",
        "https://github.com/angular/angular-cli/commit/5271547c80662de10cb3bcb648779a83f6efedfb"
      ],
      "repo": "angular-cli",
      "commit_hash": "5271547c80662de10cb3bcb648779a83f6efedfb",
      "commit_message": "fix(@angular/ssr): prevent malicious URL from overriding host  A request with a specially crafted URL starting with a double slash (e.g., `//example.com`) could cause the server-side rendering logic to interpret the request as being for a different host. This is due to the behavior of the `URL` constructor when a protocol-relative URL is passed as the first argument.  This vulnerability could be exploited to make the server execute requests to a malicious domain when relative paths are used within the application (e.g., via `HttpClient`), potentially leading to content injection or other security risks.  The fix ensures that the request URL is always constructed as a full URL string, including the protocol and host, before being passed to the `URL` constructor. This prevents the host from being overridden by the path.  Closes #31464  (cherry picked from commit 7be6c8f0e2883c85546eb1691c91fa7d4aefc0d3)",
      "files_changed": [
        {
          "filename": "packages/angular/ssr/node/src/request.ts",
          "old_url": "https://raw.githubusercontent.com/angular/angular-cli/16656a153113899ccdcaa28f442a690b5e0aee6b/packages/angular/ssr/node/src/request.ts",
          "new_url": "https://raw.githubusercontent.com/angular/angular-cli/5271547c80662de10cb3bcb648779a83f6efedfb/packages/angular/ssr/node/src/request.ts",
          "diff": "@@ -76,7 +76,7 @@ function createRequestHeaders(nodeHeaders: IncomingHttpHeaders): Headers {\n  * @param nodeRequest - The Node.js `IncomingMessage` or `Http2ServerRequest` object to extract URL information from.\n  * @returns A `URL` object representing the request URL.\n  */\n-function createRequestUrl(nodeRequest: IncomingMessage | Http2ServerRequest): URL {\n+export function createRequestUrl(nodeRequest: IncomingMessage | Http2ServerRequest): URL {\n   const {\n     headers,\n     socket,\n@@ -101,7 +101,7 @@ function createRequestUrl(nodeRequest: IncomingMessage | Http2ServerRequest): UR\n     }\n   }\n \n-  return new URL(originalUrl ?? url, `${protocol}://${hostnameWithPort}`),\n+  return new URL(`${protocol}://${hostnameWithPort}${originalUrl ?? url}`),\n }\n \n /**"
        },
        {
          "filename": "packages/angular/ssr/node/test/request_spec.ts",
          "old_url": "https://raw.githubusercontent.com/angular/angular-cli/16656a153113899ccdcaa28f442a690b5e0aee6b/packages/angular/ssr/node/test/request_spec.ts",
          "new_url": "https://raw.githubusercontent.com/angular/angular-cli/5271547c80662de10cb3bcb648779a83f6efedfb/packages/angular/ssr/node/test/request_spec.ts",
          "diff": "@@ -0,0 +1,158 @@\n+/**\n+ * @license\n+ * Copyright Google LLC All Rights Reserved.\n+ *\n+ * Use of this source code is governed by an MIT-style license that can be\n+ * found in the LICENSE file at https://angular.dev/license\n+ */\n+\n+import { IncomingMessage } from 'node:http',\n+import { Http2ServerRequest } from 'node:http2',\n+import { Socket } from 'node:net',\n+import { createRequestUrl } from '../src/request',\n+\n+// Helper to create a mock request object for testing.\n+function createRequest(details: {\n+  headers: Record<string, string | string[] | undefined>,\n+  encryptedSocket?: boolean,\n+  url?: string,\n+  originalUrl?: string,\n+}): IncomingMessage {\n+  return {\n+    headers: details.headers,\n+    socket: details.encryptedSocket ? ({ encrypted: true } as unknown as Socket) : new Socket(),\n+    url: details.url,\n+    originalUrl: details.originalUrl,\n+  } as unknown as IncomingMessage,\n+}\n+\n+// Helper to create a mock Http2ServerRequest object for testing.\n+function createHttp2Request(details: {\n+  headers: Record<string, string | string[] | undefined>,\n+  url?: string,\n+}): Http2ServerRequest {\n+  return {\n+    headers: details.headers,\n+    socket: new Socket(),\n+    url: details.url,\n+  } as Http2ServerRequest,\n+}\n+\n+describe('createRequestUrl', () => {\n+  it('should create a http URL with hostname and port from the host header', () => {\n+    const url = createRequestUrl(\n+      createRequest({\n+        headers: { host: 'localhost:8080' },\n+        url: '/test',\n+      }),\n+    ),\n+    expect(url.href).toBe('http://localhost:8080/test'),\n+  }),\n+\n+  it('should create a https URL when the socket is encrypted', () => {\n+    const url = createRequestUrl(\n+      createRequest({\n+        headers: { host: 'example.com' },\n+        encryptedSocket: true,\n+        url: '/test',\n+      }),\n+    ),\n+    expect(url.href).toBe('https://example.com/test'),\n+  }),\n+\n+  it('should use \"/\" as the path when the URL path is empty', () => {\n+    const url = createRequestUrl(\n+      createRequest({\n+        headers: { host: 'example.com' },\n+        encryptedSocket: true,\n+        url: '',\n+      }),\n+    ),\n+    expect(url.href).toBe('https://example.com/'),\n+  }),\n+\n+  it('should preserve query parameters in the URL path', () => {\n+    const url = createRequestUrl(\n+      createRequest({\n+        headers: { host: 'example.com' },\n+        encryptedSocket: true,\n+        url: '/test?a=1',\n+      }),\n+    ),\n+    expect(url.href).toBe('https://example.com/test?a=1'),\n+  }),\n+\n+  it('should prioritize \"originalUrl\" over \"url\" for the path', () => {\n+    const url = createRequestUrl(\n+      createRequest({\n+        headers: { host: 'example.com' },\n+        encryptedSocket: true,\n+        url: '/test',\n+        originalUrl: '/original',\n+      }),\n+    ),\n+    expect(url.href).toBe('https://example.com/original'),\n+  }),\n+\n+  it('should use \"/\" as the path when both \"url\" and \"originalUrl\" are not provided', () => {\n+    const url = createRequestUrl(\n+      createRequest({\n+        headers: { host: 'example.com' },\n+        encryptedSocket: true,\n+        url: undefined,\n+        originalUrl: undefined,\n+      }),\n+    ),\n+    expect(url.href).toBe('https://example.com/'),\n+  }),\n+\n+  it('should treat a protocol-relative value in \"url\" as part of the path', () => {\n+    const url = createRequestUrl(\n+      createRequest({\n+        headers: { host: 'localhost:8080' },\n+        url: '//example.com/test',\n+      }),\n+    ),\n+    expect(url.href).toBe('http://localhost:8080//example.com/test'),\n+  }),\n+\n+  it('should treat a protocol-relative value in \"originalUrl\" as part of the path', () => {\n+    const url = createRequestUrl(\n+      createRequest({\n+        headers: { host: 'localhost:8080' },\n+        url: '/test',\n+        originalUrl: '//example.com/original',\n+      }),\n+    ),\n+    expect(url.href).toBe('http://localhost:8080//example.com/original'),\n+  }),\n+\n+  it('should prioritize \"x-forwarded-host\" and \"x-forwarded-proto\" headers', () => {\n+    const url = createRequestUrl(\n+      createRequest({\n+        headers: {\n+          host: 'localhost:8080',\n+          'x-forwarded-host': 'example.com',\n+          'x-forwarded-proto': 'https',\n+        },\n+        url: '/test',\n+      }),\n+    ),\n+    expect(url.href).toBe('https://example.com/test'),\n+  }),\n+\n+  it('should use \"x-forwarded-port\" header for the port', () => {\n+    const url = createRequestUrl(\n+      createRequest({\n+        headers: {\n+          host: 'localhost:8080',\n+          'x-forwarded-host': 'example.com',\n+          'x-forwarded-proto': 'https',\n+          'x-forwarded-port': '8443',\n+        },\n+        url: '/test',\n+      }),\n+    ),\n+    expect(url.href).toBe('https://example.com:8443/test'),\n+  }),\n+}),"
        }
      ]
    }
  ],
  [
    {
      "cve_id": [
        "CVE-2025-62506",
        "https://github.com/minio/minio/commit/c1a49490c78e9c3ebcad86ba0662319138ace190"
      ],
      "repo": "minio",
      "commit_hash": "c1a49490c78e9c3ebcad86ba0662319138ace190",
      "commit_message": "fix: check sub-policy properly when present (#21642)  This fixes a security issue where sub-policy attached to a service account or STS account is not properly validated under certain \"own\" account operations (like creating new service accounts). This allowed a service account to create new service accounts for the same user bypassing the inline policy restriction.",
      "files_changed": [
        {
          "filename": "cmd/admin-handlers-users_test.go",
          "old_url": "https://raw.githubusercontent.com/minio/minio/334c313da4b6f95708e1a81dd176cc31fb2e1289/cmd/admin-handlers-users_test.go",
          "new_url": "https://raw.githubusercontent.com/minio/minio/c1a49490c78e9c3ebcad86ba0662319138ace190/cmd/admin-handlers-users_test.go",
          "diff": "@@ -208,6 +208,8 @@ func TestIAMInternalIDPServerSuite(t *testing.T) {\n \t\t\t\tsuite.TestGroupAddRemove(c)\n \t\t\t\tsuite.TestServiceAccountOpsByAdmin(c)\n \t\t\t\tsuite.TestServiceAccountPrivilegeEscalationBug(c)\n+\t\t\t\tsuite.TestServiceAccountPrivilegeEscalationBug2_2025_10_15(c, true)\n+\t\t\t\tsuite.TestServiceAccountPrivilegeEscalationBug2_2025_10_15(c, false)\n \t\t\t\tsuite.TestServiceAccountOpsByUser(c)\n \t\t\t\tsuite.TestServiceAccountDurationSecondsCondition(c)\n \t\t\t\tsuite.TestAddServiceAccountPerms(c)\n@@ -1249,6 +1251,108 @@ func (s *TestSuiteIAM) TestServiceAccountPrivilegeEscalationBug(c *check) {\n \t}\n }\n \n+func (s *TestSuiteIAM) TestServiceAccountPrivilegeEscalationBug2_2025_10_15(c *check, forRoot bool) {\n+\tctx, cancel := context.WithTimeout(context.Background(), testDefaultTimeout)\n+\tdefer cancel()\n+\n+\tfor i := range 3 {\n+\t\terr := s.client.MakeBucket(ctx, fmt.Sprintf(\"bucket%d\", i+1), minio.MakeBucketOptions{})\n+\t\tif err != nil {\n+\t\t\tc.Fatalf(\"bucket create error: %v\", err)\n+\t\t}\n+\t\tdefer func(i int) {\n+\t\t\t_ = s.client.RemoveBucket(ctx, fmt.Sprintf(\"bucket%d\", i+1))\n+\t\t}(i)\n+\t}\n+\n+\tallow2BucketsPolicyBytes := []byte(`{\n+    \"Version\": \"2012-10-17\",\n+    \"Statement\": [\n+        {\n+            \"Sid\": \"ListBucket1AndBucket2\",\n+            \"Effect\": \"Allow\",\n+            \"Action\": [\"s3:ListBucket\"],\n+            \"Resource\": [\"arn:aws:s3:::bucket1\", \"arn:aws:s3:::bucket2\"]\n+        },\n+        {\n+            \"Sid\": \"ReadWriteBucket1AndBucket2Objects\",\n+            \"Effect\": \"Allow\",\n+            \"Action\": [\n+                \"s3:DeleteObject\",\n+                \"s3:DeleteObjectVersion\",\n+                \"s3:GetObject\",\n+                \"s3:GetObjectVersion\",\n+                \"s3:PutObject\"\n+            ],\n+            \"Resource\": [\"arn:aws:s3:::bucket1/*\", \"arn:aws:s3:::bucket2/*\"]\n+        }\n+    ]\n+}`)\n+\n+\tif forRoot {\n+\t\t// Create a service account for the root user.\n+\t\t_, err := s.adm.AddServiceAccount(ctx, madmin.AddServiceAccountReq{\n+\t\t\tPolicy:    allow2BucketsPolicyBytes,\n+\t\t\tAccessKey: \"restricted\",\n+\t\t\tSecretKey: \"restricted123\",\n+\t\t})\n+\t\tif err != nil {\n+\t\t\tc.Fatalf(\"could not create service account\")\n+\t\t}\n+\t\tdefer func() {\n+\t\t\t_ = s.adm.DeleteServiceAccount(ctx, \"restricted\")\n+\t\t}()\n+\t} else {\n+\t\t// Create a regular user and attach consoleAdmin policy\n+\t\terr := s.adm.AddUser(ctx, \"foobar\", \"foobar123\")\n+\t\tif err != nil {\n+\t\t\tc.Fatalf(\"could not create user\")\n+\t\t}\n+\n+\t\t_, err = s.adm.AttachPolicy(ctx, madmin.PolicyAssociationReq{\n+\t\t\tPolicies: []string{\"consoleAdmin\"},\n+\t\t\tUser:     \"foobar\",\n+\t\t})\n+\t\tif err != nil {\n+\t\t\tc.Fatalf(\"could not attach policy\")\n+\t\t}\n+\n+\t\t// Create a service account for the regular user.\n+\t\t_, err = s.adm.AddServiceAccount(ctx, madmin.AddServiceAccountReq{\n+\t\t\tPolicy:     allow2BucketsPolicyBytes,\n+\t\t\tTargetUser: \"foobar\",\n+\t\t\tAccessKey:  \"restricted\",\n+\t\t\tSecretKey:  \"restricted123\",\n+\t\t})\n+\t\tif err != nil {\n+\t\t\tc.Fatalf(\"could not create service account: %v\", err)\n+\t\t}\n+\t\tdefer func() {\n+\t\t\t_ = s.adm.DeleteServiceAccount(ctx, \"restricted\")\n+\t\t\t_ = s.adm.RemoveUser(ctx, \"foobar\")\n+\t\t}()\n+\t}\n+\trestrictedClient := s.getUserClient(c, \"restricted\", \"restricted123\", \"\")\n+\n+\tbuckets, err := restrictedClient.ListBuckets(ctx)\n+\tif err != nil {\n+\t\tc.Fatalf(\"err fetching buckets %s\", err)\n+\t}\n+\tif len(buckets) != 2 || buckets[0].Name != \"bucket1\" || buckets[1].Name != \"bucket2\" {\n+\t\tc.Fatalf(\"restricted service account should only have access to bucket1 and bucket2\")\n+\t}\n+\n+\t// Try to escalate privileges\n+\trestrictedAdmClient := s.getAdminClient(c, \"restricted\", \"restricted123\", \"\")\n+\t_, err = restrictedAdmClient.AddServiceAccount(ctx, madmin.AddServiceAccountReq{\n+\t\tAccessKey: \"newroot\",\n+\t\tSecretKey: \"newroot123\",\n+\t})\n+\tif err == nil {\n+\t\tc.Fatalf(\"restricted service account was able to create service account bypassing sub-policy!\")\n+\t}\n+}\n+\n func (s *TestSuiteIAM) SetUpAccMgmtPlugin(c *check) {\n \tctx, cancel := context.WithTimeout(context.Background(), testDefaultTimeout)\n \tdefer cancel()"
        },
        {
          "filename": "cmd/iam.go",
          "old_url": "https://raw.githubusercontent.com/minio/minio/334c313da4b6f95708e1a81dd176cc31fb2e1289/cmd/iam.go",
          "new_url": "https://raw.githubusercontent.com/minio/minio/c1a49490c78e9c3ebcad86ba0662319138ace190/cmd/iam.go",
          "diff": "@@ -2400,21 +2400,8 @@ func isAllowedBySessionPolicyForServiceAccount(args policy.Args) (hasSessionPoli\n \t// policy, regardless of whether the number of statements is 0, this\n \t// includes `null`, `{}` and `{\"Statement\": null}`. In fact, MinIO Console\n \t// sends `null` when no policy is set and the intended behavior is that the\n-\t// service account should inherit parent policy.\n-\t//\n-\t// However, for a policy like `{\"Statement\":[]}`, the intention is to not\n-\t// provide any permissions via the session policy - i.e. the service account\n-\t// can do nothing (such a JSON could be generated by an external application\n-\t// as the policy for the service account). Inheriting the parent policy in\n-\t// such a case, is a security issue. Ideally, we should not allow such\n-\t// behavior, but for compatibility with the Console, we currently allow it.\n-\t//\n-\t// TODO:\n-\t//\n-\t// 1. fix console behavior and allow this inheritance for service accounts\n-\t// created before a certain (TBD) future date.\n-\t//\n-\t// 2. do not allow empty statement policies for service accounts.\n+\t// service account should inherit parent policy. So when policy is empty in\n+\t// all fields we return hasSessionPolicy=false.\n \tif subPolicy.Version == \"\" && subPolicy.Statements == nil && subPolicy.ID == \"\" {\n \t\thasSessionPolicy = false\n \t\treturn hasSessionPolicy, isAllowed\n@@ -2423,8 +2410,16 @@ func isAllowedBySessionPolicyForServiceAccount(args policy.Args) (hasSessionPoli\n \t// As the session policy exists, even if the parent is the root account, it\n \t// must be restricted by it. So, we set `.IsOwner` to false here\n \t// unconditionally.\n+\t//\n+\t// We also set `DenyOnly` arg to false here - this is an IMPORTANT corner\n+\t// case: DenyOnly is used only for allowing an account to do actions related\n+\t// to its own account (like create service accounts for itself, among\n+\t// others). However when a session policy is present, we need to validate\n+\t// that the action is actually allowed, rather than checking if the action\n+\t// is only disallowed.\n \tsessionPolicyArgs := args\n \tsessionPolicyArgs.IsOwner = false\n+\tsessionPolicyArgs.DenyOnly = false\n \n \t// Sub policy is set and valid.\n \treturn hasSessionPolicy, subPolicy.IsAllowed(sessionPolicyArgs)\n@@ -2465,8 +2460,16 @@ func isAllowedBySessionPolicy(args policy.Args) (hasSessionPolicy bool, isAllowe\n \t// As the session policy exists, even if the parent is the root account, it\n \t// must be restricted by it. So, we set `.IsOwner` to false here\n \t// unconditionally.\n+\t//\n+\t// We also set `DenyOnly` arg to false here - this is an IMPORTANT corner\n+\t// case: DenyOnly is used only for allowing an account to do actions related\n+\t// to its own account (like create service accounts for itself, among\n+\t// others). However when a session policy is present, we need to validate\n+\t// that the action is actually allowed, rather than checking if the action\n+\t// is only disallowed.\n \tsessionPolicyArgs := args\n \tsessionPolicyArgs.IsOwner = false\n+\tsessionPolicyArgs.DenyOnly = false\n \n \t// Sub policy is set and valid.\n \treturn hasSessionPolicy, subPolicy.IsAllowed(sessionPolicyArgs)"
        },
        {
          "filename": "cmd/sts-handlers_test.go",
          "old_url": "https://raw.githubusercontent.com/minio/minio/334c313da4b6f95708e1a81dd176cc31fb2e1289/cmd/sts-handlers_test.go",
          "new_url": "https://raw.githubusercontent.com/minio/minio/c1a49490c78e9c3ebcad86ba0662319138ace190/cmd/sts-handlers_test.go",
          "diff": "@@ -42,6 +42,8 @@ func runAllIAMSTSTests(suite *TestSuiteIAM, c *check) {\n \t// The STS for root test needs to be the first one after setup.\n \tsuite.TestSTSForRoot(c)\n \tsuite.TestSTS(c)\n+\tsuite.TestSTSPrivilegeEscalationBug2_2025_10_15(c, true)\n+\tsuite.TestSTSPrivilegeEscalationBug2_2025_10_15(c, false)\n \tsuite.TestSTSWithDenyDeleteVersion(c)\n \tsuite.TestSTSWithTags(c)\n \tsuite.TestSTSServiceAccountsWithUsername(c)\n@@ -276,6 +278,110 @@ func (s *TestSuiteIAM) TestSTSWithDenyDeleteVersion(c *check) {\n \tc.mustNotDelete(ctx, minioClient, bucket, versions[0])\n }\n \n+func (s *TestSuiteIAM) TestSTSPrivilegeEscalationBug2_2025_10_15(c *check, forRoot bool) {\n+\tctx, cancel := context.WithTimeout(context.Background(), testDefaultTimeout)\n+\tdefer cancel()\n+\n+\tfor i := range 3 {\n+\t\terr := s.client.MakeBucket(ctx, fmt.Sprintf(\"bucket%d\", i+1), minio.MakeBucketOptions{})\n+\t\tif err != nil {\n+\t\t\tc.Fatalf(\"bucket create error: %v\", err)\n+\t\t}\n+\t\tdefer func(i int) {\n+\t\t\t_ = s.client.RemoveBucket(ctx, fmt.Sprintf(\"bucket%d\", i+1))\n+\t\t}(i)\n+\t}\n+\n+\tallow2BucketsPolicyBytes := []byte(`{\n+    \"Version\": \"2012-10-17\",\n+    \"Statement\": [\n+        {\n+            \"Sid\": \"ListBucket1AndBucket2\",\n+            \"Effect\": \"Allow\",\n+            \"Action\": [\"s3:ListBucket\"],\n+            \"Resource\": [\"arn:aws:s3:::bucket1\", \"arn:aws:s3:::bucket2\"]\n+        },\n+        {\n+            \"Sid\": \"ReadWriteBucket1AndBucket2Objects\",\n+            \"Effect\": \"Allow\",\n+            \"Action\": [\n+                \"s3:DeleteObject\",\n+                \"s3:DeleteObjectVersion\",\n+                \"s3:GetObject\",\n+                \"s3:GetObjectVersion\",\n+                \"s3:PutObject\"\n+            ],\n+            \"Resource\": [\"arn:aws:s3:::bucket1/*\", \"arn:aws:s3:::bucket2/*\"]\n+        }\n+    ]\n+}`)\n+\n+\tvar value cr.Value\n+\tvar err error\n+\tif forRoot {\n+\t\tassumeRole := cr.STSAssumeRole{\n+\t\t\tClient:      s.TestSuiteCommon.client,\n+\t\t\tSTSEndpoint: s.endPoint,\n+\t\t\tOptions: cr.STSAssumeRoleOptions{\n+\t\t\t\tAccessKey: globalActiveCred.AccessKey,\n+\t\t\t\tSecretKey: globalActiveCred.SecretKey,\n+\t\t\t\tPolicy:    string(allow2BucketsPolicyBytes),\n+\t\t\t},\n+\t\t}\n+\t\tvalue, err = assumeRole.Retrieve()\n+\t\tif err != nil {\n+\t\t\tc.Fatalf(\"err calling assumeRole: %v\", err)\n+\t\t}\n+\t} else {\n+\t\t// Create a regular user and attach consoleAdmin policy\n+\t\terr := s.adm.AddUser(ctx, \"foobar\", \"foobar123\")\n+\t\tif err != nil {\n+\t\t\tc.Fatalf(\"could not create user\")\n+\t\t}\n+\n+\t\t_, err = s.adm.AttachPolicy(ctx, madmin.PolicyAssociationReq{\n+\t\t\tPolicies: []string{\"consoleAdmin\"},\n+\t\t\tUser:     \"foobar\",\n+\t\t})\n+\t\tif err != nil {\n+\t\t\tc.Fatalf(\"could not attach policy\")\n+\t\t}\n+\n+\t\tassumeRole := cr.STSAssumeRole{\n+\t\t\tClient:      s.TestSuiteCommon.client,\n+\t\t\tSTSEndpoint: s.endPoint,\n+\t\t\tOptions: cr.STSAssumeRoleOptions{\n+\t\t\t\tAccessKey: \"foobar\",\n+\t\t\t\tSecretKey: \"foobar123\",\n+\t\t\t\tPolicy:    string(allow2BucketsPolicyBytes),\n+\t\t\t},\n+\t\t}\n+\t\tvalue, err = assumeRole.Retrieve()\n+\t\tif err != nil {\n+\t\t\tc.Fatalf(\"err calling assumeRole: %v\", err)\n+\t\t}\n+\t}\n+\trestrictedClient := s.getUserClient(c, value.AccessKeyID, value.SecretAccessKey, value.SessionToken)\n+\n+\tbuckets, err := restrictedClient.ListBuckets(ctx)\n+\tif err != nil {\n+\t\tc.Fatalf(\"err fetching buckets %s\", err)\n+\t}\n+\tif len(buckets) != 2 || buckets[0].Name != \"bucket1\" || buckets[1].Name != \"bucket2\" {\n+\t\tc.Fatalf(\"restricted STS account should only have access to bucket1 and bucket2\")\n+\t}\n+\n+\t// Try to escalate privileges\n+\trestrictedAdmClient := s.getAdminClient(c, value.AccessKeyID, value.SecretAccessKey, value.SessionToken)\n+\t_, err = restrictedAdmClient.AddServiceAccount(ctx, madmin.AddServiceAccountReq{\n+\t\tAccessKey: \"newroot\",\n+\t\tSecretKey: \"newroot123\",\n+\t})\n+\tif err == nil {\n+\t\tc.Fatalf(\"restricted STS account was able to create service account bypassing sub-policy!\")\n+\t}\n+}\n+\n func (s *TestSuiteIAM) TestSTSWithTags(c *check) {\n \tctx, cancel := context.WithTimeout(context.Background(), testDefaultTimeout)\n \tdefer cancel()"
        }
      ]
    }
  ],
  [
    {
      "cve_id": [
        "CVE-2025-26625",
        "https://github.com/git-lfs/git-lfs/commit/0cffe93176b870055c9dadbb3cc9a4a440e98396"
      ],
      "repo": "git-lfs",
      "commit_hash": "0cffe93176b870055c9dadbb3cc9a4a440e98396",
      "commit_message": "check for dir/symlink conflicts on checkout/pull  Our \"git lfs checkout\" and \"git lfs pull\" commands, at present, follow any extant symbolic links when they populate the current working tree with files containing the content of Git LFS objects, even if the symbolic links point to locations outside of the working tree. This vulnerability has been assigned the identifier CVE-2025-26625.  In previous commits we partially addressed this vulnerability by ensuring that the \"git lfs checkout\" and \"git lfs pull\" commands remove any file or symbolic link which already exists at the location where they intend to write the contents of a Git LFS file, and by checking for symbolic links at these locations first in the DecodePointerFromBlob() function of the \"lfs\" package.  However, these changes still allow for the possibility that a symbolic link exists in place of a directory in the path between the root of the working tree and the location where the commands intend to create a file.  At present, the \"git lfs checkout\" and \"git lfs pull\" commands will not detect such links, and so may be induced to write to a location outside of the working tree.  To address this issue, revise the \"git lfs checkout\" and \"git lfs pull\" commands so they check each path component from the root of the working tree to a Git LFS file.  If any are missing, a directory is created, and if any already exist but are not directories, the commands report an error and do not try to create the Git LFS file or write to it.  In our implementation of these checks, we adopt a similar approach to the one used by Git, which also tries to avoid accidentally traversing symbolic links when updating the files in a working tree.  For performance and compatibility reasons, though, Git does not try to completely eliminate all TOCTOU (time-of-check/time-of-use) races involving symbolic links.  Likewise, we do not aim to prevent every possible race which might allow the Git LFS client to unintentionally write through a symbolic link.  Instead, we try to limit the chances of this occurring as far as we reasonably can, while avoiding significant performance penalties.  One difference between our approach and that taken by Git is that when the we check whether a directory exists and find something other than a directory, we do not try to remove it.  This design choice retains compatibility with the legacy behaviour of the Git LFS client, which simply invoked the MkdirAll() function of the \"os\" package in the Go standard library.  That function returns an error if any of the directories in the given path do not already exist and cannot be created, and the \"git lfs checkout\" and \"git lfs pull\" commands would just report that error rather than attempt to resolve it by removing anything.  Another difference between the way Git checks for directory path conflicts and the implementation we introduce in this commit is that Git retains the results of its checks in a simple single-entry cache while we repeat our checks for each new Git LFS file we process.  We can add caching logic in the future if we find it valuable, but we would require a more complex and thread-safe cache than Git's due to our use of multiple goroutines in the \"git lfs pull\" command, and initial testing indicates that the performance gains would be relatively limited.  When the \"git checkout\" command runs, the checkout_entry_ca() function performs the necessary changes in the working tree in order to be able to write a copy of a given file at its expected location.  This function invokes the create_directories() function to ensure that all of the directories between the root of the working tree and the file are present before the file is created.  If the create_directories() function detects a conflict in place of any directory, such as a file or symbolic link, it tries to remove the conflicting entry and then create a new directory in its place.  As noted above, though, Git does not re-check every directory entry in a file's path in all cases, and also does not try to avoid TOCTOU races in the checks it does perform.  The create_directories() function relies on the has_dirs_only_path() function to report whether a path consists of only directories, and that function ultimately invokes the lstat_cache_matchlen() function to determine whether Git believes this to be the case or not:    https://github.com/git/git/blob/v2.50.1/entry.c#L582   https://github.com/git/git/blob/v2.50.1/entry.c#L41-L42   https://github.com/git/git/blob/v2.50.1/symlinks.c#L257   https://github.com/git/git/blob/v2.50.1/symlinks.c#L276-L278   https://github.com/git/git/blob/v2.50.1/symlinks.c#L199-L200   https://github.com/git/git/blob/v2.50.1/symlinks.c#L63-L193  The lstat_cache_matchlen() function accepts a path from the root of the repository as its \"name\" parameter, and for each component of the path for which the function does not have any cached information, it uses the lstat(2) POSIX system call to test whether that path component exists and if it is a directory or not.  The final result is then retained in the function's single-entry cache.  The use of a cache with only a single entry is viable for Git because in almost all cases, it processes files in sorted order.  Thus it can make use of the cached lstat(2) information about the directory \"abc\" from the path \"abc/bar.txt\" when checking the path of \"abc/foo.txt\", for instance.  The use of cache in this function, though, is one of the reasons Git is not immune to TOCTOU races involving symbolic links.  If a directory is replaced with a symbolic link after the lstat_cache_matchlen() function has checked the path, the lstat_cache_matchlen() function will assume another file with the same leading path components can be created without re-checking for symbolic links, and Git will traverse the new symbolic link when writing the file, even if it leads to a location outside of the working tree.  Git also has to be careful to reset the cache whenever it removes any of the directories in the cached path, as may occur when Git processes files that are not in sorted order and their paths conflict with each other due to case-insensitivity or case-folding on the part of the filesystem.  This type of situation was described in commit git/git@684dd4c2b414bcf648505e74498a608f28de4592, which added logic to ensure the cache is cleared under these conditions as part of the remediation for the vulnerability identified as CVE-2021-21300.  Further, Git would also need to consistently use the openat(2) family of POSIX system calls in conjunction with their O_NOFOLLOW flags, or their equivalent on Windows, in order to guarantee that a given path consists solely of directories and no symbolic links.  As noted in commit git/git@f4aa8c8bb11dae6e769cd930565173808cbb69c8 in relation to the vulnerability identified as CVE-2024-32004, on Windows this type of implementation would require the use of the relatively expensive NtCreateFile() system call (and its FILE_OPEN_REPARSE_POINT flag):    https://pubs.opengroup.org/onlinepubs/9699919799/functions/open.html   https://pubs.opengroup.org/onlinepubs/9699919799/functions/fstatat.html   https://www.man7.org/linux/man-pages/man2/openat.2.html   https://www.man7.org/linux/man-pages/man2/stat.2.html   https://learn.microsoft.com/en-us/windows/win32/api/winternl/nf-winternl-ntcreatefile  Beginning with version 1.24.0, Go introduced a Root structure type in the \"os\" package of the standard library, with a set of methods which explicitly enforces file path boundaries, using the openat(2) family of system calls where they are available, and the NtCreateFile() system call on Windows.  Go v1.25.0 expanded the set of methods in the Root type, and in particular added a MkdirAll() method which mirrors the regular MkdirAll() function in the \"os\" package, but checks that none of the components in a path are symbolic links to locations outside a given initial \"root\" path.  The development of the Root type and its API was tracked in golang/go#67002.  One minor caveat with the MkdirAll() method of the Root structure type is that it allows symbolic links to exist in a path, so long as they do not resolve to location outside the path that was initially passed to the OpenRoot() function.  We would prefer to avoid these types of \"local\" symbolic links as well when they conflict with a directory we expect to exist, so the Root type's MkdirAll() method would not suffice for our purposes.  A more important challenge with the Root structure type is that consistent use of its methods would result in a noticeable increase in the execution time of our commands when processing even moderate numbers of Git LFS files.  Each of the type's methods, including Lstat(), Mkdir(), OpenFile(), and Remove(), traverses the directories in its path parameter and checks that none are symbolic links to locations outside the path initially passed to the OpenRoot() function.  Each method's cost therefore scales with the number of directories in its path parameter, i.e., given \"m\" method calls and \"n\" directories in a path, the number of system calls scales as O(m*n).  For this reason, the Go documentation states that:    \"Root operations on filenames containing many directory components can    be much more expensive than the equivalent non-Root operation.\"    https://go.dev/blog/osroot#performance  We verified this performance penalty in tests of a modified \"git lfs checkout\" command which checks for symbolic links in each Git LFS file's path within the repository by calling the methods of the Root structure type.  We also tested the implementation from this commit, and we report those results in more detail below.  In brief, even without a cache like the one in Git's lstat_cache_matchlen() function, the technique we introduce in this commit adds a modest overhead, while the use of the Root structure's methods significantly increased the command's runtime.  Our preferred technique relies on several enhancements we made in previous commits to the \"git lfs checkout\" and \"git lfs pull\" commands. These commands retrieve a list of Git LFS pointer files from the ScanLFSFiles() method of the GitScanner structure type in our \"lfs\" package, and for each file, invoke the Run() method of the singleCheckout structure type in our \"commands\" package.  The Run() method then determines whether or not to write the contents of the object referenced by the pointer into a file in the working tree at the appropriate path.  In prior commits we revised the newSingleCheckout() function to verify whether a working tree exists when it initializes a new singleCheckout structure, and if a tree is present, to change the current working directory to the root of the tree.  We also adjusted the Run() method so that it returns immediately without taking action if no working tree was found by the newSingleCheckout() function.  We now introduce a new DirWalker structure type in our \"tools\" package, with Walk() and WalkAndCreate() methods which check that each component of a given path is a directory, and return an error if a conflict is found.  If a directory is missing, the Walk() method will return an error, while the WalkAndCreate() method will try to create the directory. Both methods are simple wrappers around the internal walk() method, whose \"create\" parameter indicates whether the method should try to create missing directories or not.  To initialize a DirWalker structure we define a NewDirWalkerForFile() function, which requires three parameters.  The first is an initial parent path which should be specified as a path relative to the current working directory, and which is stored in the \"parentPath\" element of the new DirWalker structure.  The second parameter is a file path which should be specified as a path relative to the parent path.  If the parent path is empty, the file path is understood to be relative to the current working directory.  The third parameter must be a structure with a RepositoryPermissions() method which conforms to the repositoryPermissionFetcher interface type from our \"tools\" package.  The NewDirWalkerForFile() function removes the final filename path segment from its second \"filePath\" parameter in order to populate the new DirWalker structure's \"path\" element with leading directories in the file's path, if any.  If the \"filePath\" parameter contains a bare filename, because the file resides at the root of the repository, then the \"path\" element is set to an empty path.  Note that we do not use the Dir() function from the \"path/filepath\" package in the Go standard library to remove the filename from the \"filePath\" parameter because that function returns a \".\" path when a path has no leading directory components, and because it replaces the \"/\" separator with the \"\\\" separator on Windows, which we do not want to do in this context.  When the DirWalker structure's walk() method is called, it assumes that the path identified by the structure's \"parentPath\" element exists within the current working directory, and then checks each of the directories in the \"path\" element until either an error is returned or all the directories have been checked.  If a directory does not exist, the walk() method returns an ErrNotExist error unless the \"create\" parameter is set to \"true\", in which case the walk() method will try to create the missing directory.  If a conflict is found in the place of a directory, such as a pre-existing file or symbolic link with the same name, then the walk() method returns a custom errNotDir error.  Assuming that the newSingleCheckout() function found an extant working tree and was able to change the current working directory to the root of the tree, the singleCheckout structure's Run() method creates a new DirWalker structure and calls its Walk() method to determine which directories in the given Git LFS pointer file's path already exist, without at first trying to create any new directories.  Since the current working directory is the root of the work tree, the Run() method passes an empty path to the NewDirWalkerForFile() function as its \"parentPath\" parameter, and the pointer file's path as the \"filePath\" parameter.  The pointer file paths processed by the Run() method are guaranteed to be those supplied by Git, since they are the paths returned by the ScanLFSFiles() method of the GitScanner structure, which reads the paths from the output of either a \"git ls-files\" or \"git ls-tree\" command.  As such, we expect these paths to always use forward slash characters as separators, to always be relative paths and not absolute paths, and to never contain empty path components or \".\" or \"..\" path components.  For safety, the DirWalker structure's walk() method rejects any path which contains any of these path components and returns an error in such a case.  If the call to the Walk() method returns an error, the Run() method checks whether the error was due to a missing directory or some other issue.  If an ErrNotExist error from the \"os\" package was returned, this indicates that at least one directory in the current Git LFS pointer file's path does not exist, in which case the Run() method skips calling the DecodePointerFromFile() function from our \"lfs\" package, since there is no value in trying to read a non-existent file's contents to see if it contains a raw Git LFS pointer.  If some other type of error was returned, the Run() method logs the error and returns without proceeding further, and if no error was returned, then all the file's ancestor directories were found, so the Run() method does call the DecodePointerFromFile() function in that case.  The Run() method then proceeds to check the results from the DecodePointerFromFile() function, if it was called at all.  This logic remains unchanged, but can take advantage of the fact that an ErrNotExist error from the call to the Walk() method implies that no pointer file exists.  When this type of error is returned by either the Walk() method or the DecodePointerFromFile() function, the Run() method then calls the DiffIndexWithPaths() function in our \"git\" package to check if the user has intentionally removed the file from Git's index, in which case no further action should be taken.  If an ErrNotExist error was returned by either the Walk() method or the DecodePointerFromFile() function, and the user has not removed the file from Git's index, then the Run() method calls the DirWalker structure's WalkAndCreate() method in order to create any directories in the file's path which are missing.  For this call, the internal walk() method of the DirWalker structure continues where the previous invocation left off, based on the values of the internal \"parentPath\" and \"path\" elements of the structure.  The previous invocation of the walk() method by the Walk() method will have set the structure's \"parentPath\" element to contain the leading directories in the file's path that were found to exist, and set the \"path\" element to contain just those directories which need to be created.  Note that either of these paths may be empty, since there may be no missing directories, or all the directories in the file's path may be missing, or the file may be located in the top-level directory.  To verify that the DirWalker structure's internal walk() method handles all of these potential conditions, along with various types of directory conflicts such as pre-existing files or symbolic links, we add a TestDirWalkerWalk() Go test function and define a large number of valid and invalid test cases for this function.  The test function then exercises the walk() method in all the defined test cases, both with an empty parent path and with a non-empty parent path.  When the Run() method calls the DirWalker's WalkAndCreate() method, this passes a \"true\" value to the walk() method for its \"create\" parameter, so any directories that are missing will be created.  This means that when the Run() method then calls the RunToPath() method, and it invokes the SmudgeToFile() method of the GitFilter structure in our \"lfs\" package, that method no longer needs to try to create any directories.  We therefore remove the call to the MkdirAll() function in our \"tools\" package from the SmudgeToFile() method.  However, the MkdirAll() function in our \"tools\" package is designed to enforce any umask settings defined by Git's \"core.sharedRepository\" configuration option, which is why the SmudgeToFile() method did not simply invoke the MkdirAll() function from the \"os\" package.  Since we want to retain support for this Git configuration option, we add a Mkdir() function to our \"tools\" package which mirrors the MkdirAll() function, with the only difference being that it wraps the Mkdir() function from the \"os\" package rather than the MkdirAll() function. We then call the new function in the walk() method instead of calling the Mkdir() function from the \"os\" package directly.  There is one use case where we still need to use the MkdirAll() function from our \"tools\" package, though.  When the \"git lfs checkout\" command is run with its --to option, the RunToPath() method of the singleCheckout structure is invoked directly.  The file path specified as the parameter of the --to option is converted to an absolute path and passed to the RunToPath() method so that the contents of the Git LFS object identified by the other command-line parameters are written to a file at the given path.  Since the Run() method does not execute in this case, the WalkAndCreate() method is not called and therefore will not create any directories that might be missing in the path specified by the --to option, and neither will the SmudgeToFile() method, because it no longer calls the MkdirAll() function from our \"tools\" package.  To ensure that we still support the use of the --to option with an arbitrary file path parameter, we now call the \"tools\" package's MkdirAll() function in the checkoutConflict() function of the \"git lfs checkout\" command immediately after we convert the --to option's parameter into an absolute file path.  In previous commits we expanded the checks in the \"checkout: conflicts\" test in our t/t-checkout.sh test script so it will validate the use of the \"git lfs checkout\" command's --to option in a wide range of conditions, including with file path parameters to locations with ancestor directories that do not exist.  As a consequence, we can be confident that the test validates that our changes in this commit do not introduce a regression in our support of the --to option of the \"git lfs checkout\" command.  On the other hand, we do require additional shell tests to thoroughly validate the effectiveness of our revisions to the methods of the singleCheckout structure.  Since we expect the \"git lfs checkout\" and \"git lfs pull\" commands to now try to detect when symbolic links exist in place of the directories in the paths to Git LFS files in a work tree, even if the targets of those links are themselves directories, we expand the \"checkout: skip directory symlink conflicts\" and \"pull: skip directory symlink conflicts\" tests that we added to our t/t-checkout.sh and t/t-pull.sh test scripts in a prior commit.  Previously, these two tests verified that the \"git lfs checkout\" and \"git lfs pull\" commands would skip attempting to write out the contents of Git LFS objects into files in the work tree if the files' paths conflicted with pre-existing symbolic links, but only when the targets of the links were not directories.  The tests now also specifically check the commands' behaviour when the targets of the links are directories, since before our changes in this commit the commands would traverse these links and create or update files and subdirectories within the target directories. Note, though, that we do not check this behaviour under TOCTOU race conditions, because we do not expect the commands to avoid traversing symbolic links in those cases, as described above.  We also expand the \"checkout: skip case-based symlink conflicts\" and \"pull: skip case-based symlink conflicts\" tests we added in a previous commit.  These tests now also check that when when the directories in Git LFS file paths conflict with symbolic links as a result of case-insensitivity on the part of a filesystem, the \"git lfs checkout\" and \"git lfs pull\" commands detect the conflicts and report errors instead of trying to populate the Git LFS files with their objects' contents.  In both these two tests and the \"checkout: skip directory symlink conflicts\" and \"pull: skip directory symlink conflicts\" tests, we make an additional check to confirm that when symbolic links to directories exist in place of regular directories in the paths to Git LFS files, the Git error message \"is beyond a symbolic link\" does not appear in the output of the \"git lfs checkout\" and \"git lfs pull\" commands.  This message would indicate that the Git LFS commands attempted to refresh the Git index using the \"git update-index\" command for a file whose path contains a symbolic link to a directory in place of a regular directory. As the \"git lfs checkout\" and \"git lfs pull\" commands should now detect such symbolic links (so long as there is no TOCTOU race), these Git error messages should not appear in the commands' output.  Finally, we adjust the \"checkout: skip directory file conflicts\" and \"pull: skip directory file conflicts\" tests we added in another prior commit.  These tests check that the \"git lfs checkout\" and \"git lfs pull\" commands detect when a regular file exists in the place of a directory in a Git LFS file's path.  Our changes in this commit do not alter that fundamental behaviour, but they do result in a more consistent error message from the commands when a regular file exists in place of a directory.  Previously, when a file conflicted with a directory in a Git LFS file's path, the output of the \"git lfs checkout\" and \"git lfs pull\" commands differed between Unix and Windows systems due to a difference in the error returned by the Lstat() function call performed in the DecodePointerFromFile() function.  On Unix systems, this error encapsulates an ENOTDIR error number, which the IsNotExist() function of the \"os\" package does not consider equivalent to an ErrNotExist error.  On these systems, the Run() method would therefore report the error immediately after calling the DecodePointerFromFile() function and then return without taking further action.  On Windows systems, however, the same circumstances caused the Lstat() function to return an ErrNotExist error, due to the implementation of the Lstat() function in the Go standard library, which maps the Windows ERROR_FILE_NOT_FOUND error number to an ErrNotExist error.  As a result, the Run() method would proceed to call the RunToPath() method, which invoked the SmudgeToFile() method.  When that method called the OpenFile() function from the \"os\" package to try to create the Git LFS file, though, an error would occur, and this was then the error whose message would be logged by the by the \"git lfs checkout\" and \"git lfs pull\" commands.  Now that the DecodePointerFromFile() function is only called by the Run() method if its invocation of the DirWalker structure's Walk() method does not return an error, the \"git lfs checkout\" and \"git lfs pull\" commands will report the same error message on both Unix and Windows systems if the Walk() method encounters a regular file in place of a directory. To account for this change, we update our \"checkout: skip directory file conflicts\" and \"pull: skip directory file conflicts\" tests so they expect the same error message on all systems.  In addition to these changes to our regular Go and shell test suites, we also evaluated the impact of our changes in this commit to the speed of the \"git lfs checkout\" and \"git lfs pull\" commands under moderate workloads.  Our performance testing focused on the \"git lfs checkout\" command since we are not concerned with the time required to fetch Git LFS objects from a remote server.  For our principal test scenario, we created 10,000 small Git LFS files, with each file containing roughly 10 bytes of data only, so that the time required to write out the Git LFS object data of each file was minimal.  Because the cost of checking for symbolic links in the paths to Git LFS files will scale with the number of files and the number of path components, we chose a distribution of our test files with the intent that it would emulate a relatively normal repository and not a pathological use case.  For example, if we placed all the Git LFS files at the root of the repository, we would not exercise our new checks for symbolic links at all.  For our principal test repository, we therefore distributed the Git LFS files in groups of 100 into 100 subdirectories, with 5 ancestor directories between these each of these subdirectories and the root of the repository.  In a completely empty working tree, the runtime of the \"git lfs checkout\" command is heavily dominated by the cost of repeatedly spawning the \"git diff-index\" command, which we execute once for each file we find to be missing from the work tree.  (Improving this behaviour so that the \"git diff-index\" command could be invoked with multiple file paths would be a valuable enhancement we might want to explore in the future.)  So as to better evaluate the performance impact of our changes in this commit, we usually populated our working tree with raw Git LFS pointer files, as might occur after running \"git clone\" with the GIT_LFS_SKIP_SMUDGE environment variable set to a value equivalent to \"true\".  This avoids the cost of executing the \"git diff-index\" command, which can otherwise result in a tenfold increase in the runtime of the \"git lfs checkout\" command.  For the majority of our tests, we utilized a Linux system with 16 cores running at 2.10 GHz and a 5.15 kernel version.  We also repeated our tests on macOS and Windows systems, with similar results.  The times reported below are from the Linux system tests.  In our primary test scenario, with 10,000 small Git LFS files in groups of 100 with 6 levels of subdirectories for each group, the impact of checking of each directory in the files' paths amounted to a 15% increase in the average runtime of the \"git lfs checkout\" command compared to the 3.7.0 version of the Git LFS client.  The v3.7.0 client's average runtime was 3.89s and the average runtime with this commit's changes was 4.46s.  We also experimented with the inclusion of a simple lock-free single-entry cache in the walk() function, similar to the cache implemented by Git in its lstat_cache_matchlen() function.  This reduced the average runtime of the \"git lfs checkout\" command in the same scenario described above to 4.23s, an 8% increase over the v3.7.0 client's average runtime.  Our test scenario represented the ideal conditions for this simple cache, however.  The \"git lfs checkout\" command processes files sequentially in the order returned by the \"git ls-files\" command (or the \"git ls-tree\" command, if the installed version of Git is older than v2.42.0), and so we could avoid the need for any locks around our cache, or use a more complex multiple-entry cache.  The \"git lfs pull\" command, though, invokes the Run() method of the singleCheckout structure from two separate goroutines, one of which receives its list of Git LFS pointer files from the transfer queue as their corresponding objects' data is downloaded.  A functional cache implementation would consequently require locks to avoid contention between parallel invocations of the walk() method by separate goroutines, which would somewhat diminish any potential performance gains.  A single-entry cache might also prove to be ineffective with the \"git lfs pull\" command, since some files would be processed immediately if their objects were present in the local Git LFS storage directories, while others would be processed as their objects were downloaded, which might occur in a significantly different order than the sort order of the pointers' file paths.  Instead of a single-entry cache, we could use a simple map of unbounded size, or an LRU (Least-Recently Used) cache with a bounded number of elements.  However, if we do choose to add a cache in the future, it should not expose us to the type of vulnerability which the Git project reported in CVE-2021-21300.  That issue resulted partly from the use of a single-entry cache and an incorrect assumption that files would always be processed in sorted order, but the key difference between Git and Git LFS in this regard is that Git tries to conform the working tree to have the contents it expects, and Git LFS does not.  During a \"git checkout\" command, Git will try to remove directory entries such as files and symbolic links which conflict with the file paths Git intends to create.  Thus, when Git encountered files whose paths conflicted on a case-insensitive filesystem, if these files were processed out of the usual sorted order, Git might cache one file path, then remove it from the filesystem but not the cache, and then assume the file path still existed based on the contents of the cache. Git LFS should not be vulnerable to this type of problem because it does not try to remove entries which conflict with the ancestor directories in a Git LFS file's path.  Overall, though, the performance of the \"git lfs checkout\" command with the changes from this commit but without any form of caching appears to be acceptable, so we do not implement a cache in the DirWalker structure's methods at this time.  We can always revisit this decision in the future, of course.  As well as testing our changes from this commit (both with and without a simple cache), we also tested an experimental version of the \"git lfs checkout\" command which used the methods of the Root structure type from the \"os\" package.  As described above, these methods are designed to ensure that they never operate on files outside a given initial \"root\" file path.  On our Linux test system, the average runtime of the \"git lfs checkout\" command, when all filesystem operations were converted to use the methods of the Root type, was 6.57s in our primary test scenario, a 69% increase over the average runtime of the command when using the 3.7.0 version of Git LFS client, and a 47% increase over the average runtime of the command when using the changes from this commit.  (Those average runtimes were 3.89s and 4.46s, respectively.)  On a GitHub Actions runner with Windows Server 2025, the average runtime of the \"git lfs checkout\" command when all its filesystem operations used the Root type's methods was 28.83s, a 63% increase over the average runtime of the command when using the 3.7.0 version of the client, and a 39% increase over the average runtime of the command when using the changes from this commit.  (Those average runtimes were 17.70s and 20.70s, respectively.)  Intriguingly, on a GitHub Actions runner with macOS 15.5 (Sequoia), the average runtime of the \"git lfs checkout\" command with the changes from this commit was 5.81s, 5% faster than the 6.14s average runtime when using the 3.7.0 version of the Git LFS client.  The average runtime of the command when all filesystem operations used the Root type's methods, however, was 11.73s, a 91% increase compared to the runtime of the command with the 3.7.0 version of the client and a 102% increase compared to the runtime of the command with the changes from this commit.            |   v3.7.0  | DirWalker |  os.Root   --------+-----------+-----------+-----------   Linux   |    3.89s  |    4.46s  |    6.57s   macOS   |    6.14s  |    5.81s  |   11.73s   Windows |   17.70s  |   20.70s  |   28.83s  As we explained above, these performance impacts are the primary reason why we avoid the use of the Root interface and its methods and prefer to check for symbolic links in a more efficient manner, even if that allows for the possibility that we cannot detect some race conditions.",
      "files_changed": [
        {
          "filename": "commands/command_checkout.go",
          "old_url": "https://raw.githubusercontent.com/git-lfs/git-lfs/e735de58a72f1333a16054985b85b863fbaec138/commands/command_checkout.go",
          "new_url": "https://raw.githubusercontent.com/git-lfs/git-lfs/0cffe93176b870055c9dadbb3cc9a4a440e98396/commands/command_checkout.go",
          "diff": "@@ -10,6 +10,7 @@ import (\n \t\"github.com/git-lfs/git-lfs/v3/git\"\n \t\"github.com/git-lfs/git-lfs/v3/lfs\"\n \t\"github.com/git-lfs/git-lfs/v3/tasklog\"\n+\t\"github.com/git-lfs/git-lfs/v3/tools\"\n \t\"github.com/git-lfs/git-lfs/v3/tq\"\n \t\"github.com/git-lfs/git-lfs/v3/tr\"\n \t\"github.com/spf13/cobra\"\n@@ -110,6 +111,11 @@ func checkoutConflict(file string, stage git.IndexStage) {\n \t\tExit(tr.Tr.Get(\"Could not convert %q to absolute path: %v\", checkoutTo, err))\n \t}\n \n+\terr = tools.MkdirAll(filepath.Dir(checkoutTo), cfg)\n+\tif err != nil {\n+\t\tExit(tr.Tr.Get(\"Could not create path %q: %v\", checkoutTo, err))\n+\t}\n+\n \t// will chdir to root of working tree, if one exists\n \tsingleCheckout := newSingleCheckout(cfg.Git, \"\")\n \tif singleCheckout.Skip() {"
        },
        {
          "filename": "commands/pull.go",
          "old_url": "https://raw.githubusercontent.com/git-lfs/git-lfs/e735de58a72f1333a16054985b85b863fbaec138/commands/pull.go",
          "new_url": "https://raw.githubusercontent.com/git-lfs/git-lfs/0cffe93176b870055c9dadbb3cc9a4a440e98396/commands/pull.go",
          "diff": "@@ -12,6 +12,7 @@ import (\n \t\"github.com/git-lfs/git-lfs/v3/git\"\n \t\"github.com/git-lfs/git-lfs/v3/lfs\"\n \t\"github.com/git-lfs/git-lfs/v3/subprocess\"\n+\t\"github.com/git-lfs/git-lfs/v3/tools\"\n \t\"github.com/git-lfs/git-lfs/v3/tq\"\n \t\"github.com/git-lfs/git-lfs/v3/tr\"\n )\n@@ -75,8 +76,20 @@ func (c *singleCheckout) Run(p *lfs.WrappedPointer) {\n \t\treturn\n \t}\n \n-\t// Check the content - either missing or still this pointer (not exist is ok)\n-\tfilepointer, err := lfs.DecodePointerFromFile(p.Name)\n+\tdirWalker := tools.NewDirWalkerForFile(\"\", p.Name, cfg)\n+\terr := dirWalker.Walk()\n+\n+\tvar filepointer *lfs.Pointer\n+\tif err != nil {\n+\t\tif !os.IsNotExist(err) {\n+\t\t\tLoggedError(err, tr.Tr.Get(\"Checkout error trying to check path for %q: %s\", p.Name, err))\n+\t\t\treturn\n+\t\t}\n+\t} else {\n+\t\t// Check the content - either missing or still this pointer (not exist is ok)\n+\t\tfilepointer, err = lfs.DecodePointerFromFile(p.Name)\n+\t}\n+\n \tif err != nil {\n \t\tif os.IsNotExist(err) {\n \t\t\toutput, err := git.DiffIndexWithPaths(\"HEAD\", true, []string{p.Name})\n@@ -106,6 +119,13 @@ func (c *singleCheckout) Run(p *lfs.WrappedPointer) {\n \t\treturn\n \t}\n \n+\tif err != nil && os.IsNotExist(err) {\n+\t\tif err := dirWalker.WalkAndCreate(), err != nil {\n+\t\t\tLoggedError(err, tr.Tr.Get(\"Checkout error trying to create path for %q: %s\", p.Name, err))\n+\t\t\treturn\n+\t\t}\n+\t}\n+\n \tif err := c.RunToPath(p, p.Name), err != nil {\n \t\tif errors.IsDownloadDeclinedError(err) {\n \t\t\t// acceptable error, data not local (fetch not run or include/exclude)"
        },
        {
          "filename": "lfs/gitfilter_smudge.go",
          "old_url": "https://raw.githubusercontent.com/git-lfs/git-lfs/e735de58a72f1333a16054985b85b863fbaec138/lfs/gitfilter_smudge.go",
          "new_url": "https://raw.githubusercontent.com/git-lfs/git-lfs/0cffe93176b870055c9dadbb3cc9a4a440e98396/lfs/gitfilter_smudge.go",
          "diff": "@@ -16,8 +16,6 @@ import (\n )\n \n func (f *GitFilter) SmudgeToFile(filename string, ptr *Pointer, download bool, manifest tq.Manifest, cb tools.CopyCallback) error {\n-\ttools.MkdirAll(filepath.Dir(filename), f.cfg)\n-\n \t// When no pointer file exists on disk, we should use the permissions\n \t// defined for the file in Git, since the executable mode may be set.\n \t// However, to conform with our legacy behaviour, we do not do this"
        },
        {
          "filename": "t/t-checkout.sh",
          "old_url": "https://raw.githubusercontent.com/git-lfs/git-lfs/e735de58a72f1333a16054985b85b863fbaec138/t/t-checkout.sh",
          "new_url": "https://raw.githubusercontent.com/git-lfs/git-lfs/0cffe93176b870055c9dadbb3cc9a4a440e98396/t/t-checkout.sh",
          "diff": "@@ -191,13 +191,8 @@ begin_test \"checkout: skip directory file conflicts\"\n     echo >&2 \"fatal: expected checkout to succeed ...\"\n     exit 1\n   fi\n-  if [ \"$IS_WINDOWS\" -eq 1 ], then\n-    grep 'could not check out \"dir1/a\\.dat\": could not create working directory file' checkout.log\n-    grep 'could not check out \"dir2/dir3/dir4/a\\.dat\": could not create working directory file' checkout.log\n-  else\n-    grep 'Checkout error for \"dir1/a\\.dat\": lstat' checkout.log\n-    grep 'Checkout error for \"dir2/dir3/dir4/a\\.dat\": lstat' checkout.log\n-  fi\n+  grep '\"dir1/a\\.dat\": not a directory' checkout.log\n+  grep '\"dir2/dir3/dir4/a\\.dat\": not a directory' checkout.log\n \n   [ -f \"dir1\" ]\n   [ -f \"dir2/dir3\" ]\n@@ -209,13 +204,8 @@ begin_test \"checkout: skip directory file conflicts\"\n       echo >&2 \"fatal: expected checkout to succeed ...\"\n       exit 1\n     fi\n-    if [ \"$IS_WINDOWS\" -eq 1 ], then\n-      grep 'could not check out \"dir1/a\\.dat\": could not create working directory file' checkout.log\n-      grep 'could not check out \"dir2/dir3/dir4/a\\.dat\": could not create working directory file' checkout.log\n-    else\n-      grep 'Checkout error for \"dir1/a\\.dat\": lstat' checkout.log\n-      grep 'Checkout error for \"dir2/dir3/dir4/a\\.dat\": lstat' checkout.log\n-    fi\n+    grep '\"dir1/a\\.dat\": not a directory' checkout.log\n+    grep '\"dir2/dir3/dir4/a\\.dat\": not a directory' checkout.log\n   popd\n \n   [ -f \"dir1\" ]\n@@ -224,8 +214,6 @@ begin_test \"checkout: skip directory file conflicts\"\n )\n end_test\n \n-# Note that the conditions validated by this test are at present limited,\n-# but will be expanded in the future.\n begin_test \"checkout: skip directory symlink conflicts\"\n (\n   set -e\n@@ -247,6 +235,64 @@ begin_test \"checkout: skip directory symlink conflicts\"\n   git add .gitattributes dir1 dir2\n   git commit -m \"initial commit\"\n \n+  # test with symlinks to directories\n+  rm -rf dir1 dir2/dir3 ../link*\n+  mkdir ../link1 ../link2\n+  ln -s ../link1 dir1\n+  ln -s ../../link2 dir2/dir3\n+\n+  git lfs checkout 2>&1 | tee checkout.log\n+  if [ \"0\" -ne \"${PIPESTATUS[0]}\" ], then\n+    echo >&2 \"fatal: expected checkout to succeed ...\"\n+    exit 1\n+  fi\n+  grep '\"dir1/a\\.dat\": not a directory' checkout.log\n+  grep '\"dir2/dir3/dir4/a\\.dat\": not a directory' checkout.log\n+  [ -z \"$(grep \"is beyond a symbolic link\" checkout.log)\" ]\n+\n+  [ -L \"dir1\" ]\n+  [ -L \"dir2/dir3\" ]\n+  [ ! -e \"../link1/a.dat\" ]\n+  [ ! -e \"../link2/dir4\" ]\n+  assert_clean_index\n+\n+  rm -rf dir1 dir2/dir3\n+  mkdir link1 link2\n+  ln -s link1 dir1\n+  ln -s ../link2 dir2/dir3\n+\n+  git lfs checkout 2>&1 | tee checkout.log\n+  if [ \"0\" -ne \"${PIPESTATUS[0]}\" ], then\n+    echo >&2 \"fatal: expected checkout to succeed ...\"\n+    exit 1\n+  fi\n+  grep '\"dir1/a\\.dat\": not a directory' checkout.log\n+  grep '\"dir2/dir3/dir4/a\\.dat\": not a directory' checkout.log\n+  [ -z \"$(grep \"is beyond a symbolic link\" checkout.log)\" ]\n+\n+  [ -L \"dir1\" ]\n+  [ -L \"dir2/dir3\" ]\n+  [ ! -e \"link1/a.dat\" ]\n+  [ ! -e \"link2/dir4\" ]\n+  assert_clean_index\n+\n+  pushd dir2\n+    git lfs checkout 2>&1 | tee checkout.log\n+    if [ \"0\" -ne \"${PIPESTATUS[0]}\" ], then\n+      echo >&2 \"fatal: expected checkout to succeed ...\"\n+      exit 1\n+    fi\n+    grep '\"dir1/a\\.dat\": not a directory' checkout.log\n+    grep '\"dir2/dir3/dir4/a\\.dat\": not a directory' checkout.log\n+    [ -z \"$(grep \"is beyond a symbolic link\" checkout.log)\" ]\n+  popd\n+\n+  [ -L \"dir1\" ]\n+  [ -L \"dir2/dir3\" ]\n+  [ ! -e \"link1/a.dat\" ]\n+  [ ! -e \"link2/dir4\" ]\n+  assert_clean_index\n+\n   # test with symlink to file and dangling symlink\n   rm -rf dir1 dir2/dir3 ../link*\n   touch ../link1\n@@ -258,20 +304,16 @@ begin_test \"checkout: skip directory symlink conflicts\"\n     echo >&2 \"fatal: expected checkout to succeed ...\"\n     exit 1\n   fi\n-  if [ \"$IS_WINDOWS\" -eq 1 ], then\n-    grep 'could not check out \"dir1/a\\.dat\": could not create working directory file' checkout.log\n-  else\n-    grep 'Checkout error for \"dir1/a\\.dat\": lstat' checkout.log\n-  fi\n-  grep 'could not check out \"dir2/dir3/dir4/a\\.dat\": could not create working directory file' checkout.log\n+  grep '\"dir1/a\\.dat\": not a directory' checkout.log\n+  grep '\"dir2/dir3/dir4/a\\.dat\": not a directory' checkout.log\n \n   [ -L \"dir1\" ]\n   [ -L \"dir2/dir3\" ]\n   [ -f \"../link1\" ]\n   [ ! -e \"../link2\" ]\n   assert_clean_index\n \n-  rm -rf dir1 dir2/dir3\n+  rm -rf dir1 dir2/dir3 link*\n   touch link1\n   ln -s link1 dir1\n   ln -s ../link2 dir2/dir3\n@@ -281,12 +323,8 @@ begin_test \"checkout: skip directory symlink conflicts\"\n     echo >&2 \"fatal: expected checkout to succeed ...\"\n     exit 1\n   fi\n-  if [ \"$IS_WINDOWS\" -eq 1 ], then\n-    grep 'could not check out \"dir1/a\\.dat\": could not create working directory file' checkout.log\n-  else\n-    grep 'Checkout error for \"dir1/a\\.dat\": lstat' checkout.log\n-  fi\n-  grep 'could not check out \"dir2/dir3/dir4/a\\.dat\": could not create working directory file' checkout.log\n+  grep '\"dir1/a\\.dat\": not a directory' checkout.log\n+  grep '\"dir2/dir3/dir4/a\\.dat\": not a directory' checkout.log\n \n   [ -L \"dir1\" ]\n   [ -L \"dir2/dir3\" ]\n@@ -300,12 +338,8 @@ begin_test \"checkout: skip directory symlink conflicts\"\n       echo >&2 \"fatal: expected checkout to succeed ...\"\n       exit 1\n     fi\n-    if [ \"$IS_WINDOWS\" -eq 1 ], then\n-      grep 'could not check out \"dir1/a\\.dat\": could not create working directory file' checkout.log\n-    else\n-      grep 'Checkout error for \"dir1/a\\.dat\": lstat' checkout.log\n-    fi\n-    grep 'could not check out \"dir2/dir3/dir4/a\\.dat\": could not create working directory file' checkout.log\n+    grep '\"dir1/a\\.dat\": not a directory' checkout.log\n+    grep '\"dir2/dir3/dir4/a\\.dat\": not a directory' checkout.log\n   popd\n \n   [ -L \"dir1\" ]\n@@ -480,20 +514,26 @@ begin_test \"checkout: skip case-based symlink conflicts\"\n   mkdir dir1\n   ln -s ../link1 A.dat\n   ln -s ../../link2 dir1/a.dat\n+  ln -s ../link3 DIR3\n+  ln -s ../../link4 dir1/dir2\n \n-  git add A.dat dir1\n+  git add A.dat dir1 DIR3\n   git commit -m \"initial commit\"\n \n-  rm A.dat dir1/a.dat\n+  rm A.dat dir1/* DIR3\n \n   echo \"*.dat filter=lfs diff=lfs merge=lfs -text\" >.gitattributes\n \n   contents=\"a\"\n   contents_oid=\"$(calc_oid \"$contents\")\"\n+  mkdir dir3 dir1/DIR2\n   printf \"%s\" \"$contents\" >a.dat\n   printf \"%s\" \"$contents\" >dir1/A.dat\n+  printf \"%s\" \"$contents\" >dir3/a.dat\n+  printf \"%s\" \"$contents\" >dir1/DIR2/a.dat\n \n-  git -c core.ignoreCase=false add .gitattributes a.dat dir1/A.dat\n+  git -c core.ignoreCase=false add .gitattributes a.dat dir1/A.dat \\\n+    dir3/a.dat dir1/DIR2/a.dat\n   git commit -m \"case-conflicting commit\"\n \n   git push origin main\n@@ -512,25 +552,32 @@ begin_test \"checkout: skip case-based symlink conflicts\"\n \n   assert_local_object \"$contents_oid\" 1\n \n-  rm -rf *.dat dir1 ../link*\n+  rm -rf *.dat dir1 *3 ../link*\n+  mkdir ../link3 ../link4\n \n   git lfs checkout 2>&1 | tee checkout.log\n   if [ \"0\" -ne \"${PIPESTATUS[0]}\" ], then\n     echo >&2 \"fatal: expected checkout to succeed ...\"\n     exit 1\n   fi\n-  grep -q 'Checking out LFS objects: 100% (2/2), 2 B' checkout.log\n+  grep -q 'Checking out LFS objects: 100% (4/4), 4 B' checkout.log\n \n   [ -f \"a.dat\" ]\n   [ \"$contents\" = \"$(cat \"a.dat\")\" ]\n   [ -f \"dir1/A.dat\" ]\n   [ \"$contents\" = \"$(cat \"dir1/A.dat\")\" ]\n+  [ -f \"dir3/a.dat\" ]\n+  [ \"$contents\" = \"$(cat \"dir3/a.dat\")\" ]\n+  [ -f \"dir1/DIR2/a.dat\" ]\n+  [ \"$contents\" = \"$(cat \"dir1/DIR2/a.dat\")\" ]\n   [ ! -e \"../link1\" ]\n   [ ! -e \"../link2\" ]\n+  [ ! -e \"../link3/a.dat\" ]\n+  [ ! -e \"../link4/a.dat\" ]\n   assert_clean_index\n \n-  rm -rf a.dat dir1/A.dat\n-  git checkout -- A.dat dir1/a.dat\n+  rm -rf a.dat dir1/A.dat dir3 dir1/DIR2\n+  git checkout -- A.dat dir1/a.dat DIR3 dir1/dir2\n \n   git lfs checkout 2>&1 | tee checkout.log\n   if [ \"0\" -ne \"${PIPESTATUS[0]}\" ], then\n@@ -539,11 +586,14 @@ begin_test \"checkout: skip case-based symlink conflicts\"\n   fi\n   if [ \"$collision\" -eq \"0\" ], then\n     # case-sensitive filesystem\n-    grep -q 'Checking out LFS objects: 100% (2/2), 2 B' checkout.log\n+    grep -q 'Checking out LFS objects: 100% (4/4), 4 B' checkout.log\n   else\n     # case-insensitive filesystem\n     grep '\"a\\.dat\": not a regular file' checkout.log\n     grep '\"dir1/A\\.dat\": not a regular file' checkout.log\n+    grep '\"dir3/a\\.dat\": not a directory' checkout.log\n+    grep '\"dir1/DIR2/a\\.dat\": not a directory' checkout.log\n+    [ -z \"$(grep \"is beyond a symbolic link\" checkout.log)\" ]\n   fi\n \n   if [ \"$collision\" -eq \"0\" ], then\n@@ -552,13 +602,21 @@ begin_test \"checkout: skip case-based symlink conflicts\"\n     [ \"$contents\" = \"$(cat \"a.dat\")\" ]\n     [ -f \"dir1/A.dat\" ]\n     [ \"$contents\" = \"$(cat \"dir1/A.dat\")\" ]\n+    [ -f \"dir3/a.dat\" ]\n+    [ \"$contents\" = \"$(cat \"dir3/a.dat\")\" ]\n+    [ -f \"dir1/DIR2/a.dat\" ]\n+    [ \"$contents\" = \"$(cat \"dir1/DIR2/a.dat\")\" ]\n   else\n     # case-insensitive filesystem\n     [ -L \"a.dat\" ]\n     [ -L \"dir1/A.dat\" ]\n+    [ -L \"dir3\" ]\n+    [ -L \"dir1/DIR2\" ]\n   fi\n   [ ! -e \"../link1\" ]\n   [ ! -e \"../link2\" ]\n+  [ ! -e \"../link3/a.dat\" ]\n+  [ ! -e \"../link4/a.dat\" ]\n   assert_clean_index\n )\n end_test"
        },
        {
          "filename": "t/t-pull.sh",
          "old_url": "https://raw.githubusercontent.com/git-lfs/git-lfs/e735de58a72f1333a16054985b85b863fbaec138/t/t-pull.sh",
          "new_url": "https://raw.githubusercontent.com/git-lfs/git-lfs/0cffe93176b870055c9dadbb3cc9a4a440e98396/t/t-pull.sh",
          "diff": "@@ -265,13 +265,8 @@ begin_test \"pull: skip directory file conflicts\"\n     echo >&2 \"fatal: expected pull to succeed ...\"\n     exit 1\n   fi\n-  if [ \"$IS_WINDOWS\" -eq 1 ], then\n-    grep 'could not check out \"dir1/a\\.dat\": could not create working directory file' pull.log\n-    grep 'could not check out \"dir2/dir3/dir4/a\\.dat\": could not create working directory file' pull.log\n-  else\n-    grep 'Checkout error for \"dir1/a\\.dat\": lstat' pull.log\n-    grep 'Checkout error for \"dir2/dir3/dir4/a\\.dat\": lstat' pull.log\n-  fi\n+  grep '\"dir1/a\\.dat\": not a directory' pull.log\n+  grep '\"dir2/dir3/dir4/a\\.dat\": not a directory' pull.log\n \n   assert_local_object \"$contents_oid\" 1\n \n@@ -287,13 +282,8 @@ begin_test \"pull: skip directory file conflicts\"\n       echo >&2 \"fatal: expected pull to succeed ...\"\n       exit 1\n     fi\n-    if [ \"$IS_WINDOWS\" -eq 1 ], then\n-      grep 'could not check out \"dir1/a\\.dat\": could not create working directory file' pull.log\n-      grep 'could not check out \"dir2/dir3/dir4/a\\.dat\": could not create working directory file' pull.log\n-    else\n-      grep 'Checkout error for \"dir1/a\\.dat\": lstat' pull.log\n-      grep 'Checkout error for \"dir2/dir3/dir4/a\\.dat\": lstat' pull.log\n-    fi\n+    grep '\"dir1/a\\.dat\": not a directory' pull.log\n+    grep '\"dir2/dir3/dir4/a\\.dat\": not a directory' pull.log\n   popd\n \n   assert_local_object \"$contents_oid\" 1\n@@ -304,8 +294,6 @@ begin_test \"pull: skip directory file conflicts\"\n )\n end_test\n \n-# Note that the conditions validated by this test are at present limited,\n-# but will be expanded in the future.\n begin_test \"pull: skip directory symlink conflicts\"\n (\n   set -e\n@@ -336,7 +324,77 @@ begin_test \"pull: skip directory symlink conflicts\"\n   cd \"${reponame}-assert\"\n   refute_local_object \"$contents_oid\" 1\n \n+  # test with symlinks to directories\n+  rm -rf dir1 dir2/dir3 ../link*\n+  mkdir ../link1 ../link2\n+  ln -s ../link1 dir1\n+  ln -s ../../link2 dir2/dir3\n+\n+  git lfs pull 2>&1 | tee pull.log\n+  if [ \"0\" -ne \"${PIPESTATUS[0]}\" ], then\n+    echo >&2 \"fatal: expected pull to succeed ...\"\n+    exit 1\n+  fi\n+  grep '\"dir1/a\\.dat\": not a directory' pull.log\n+  grep '\"dir2/dir3/dir4/a\\.dat\": not a directory' pull.log\n+  [ -z \"$(grep \"is beyond a symbolic link\" pull.log)\" ]\n+\n+  assert_local_object \"$contents_oid\" 1\n+\n+  [ -L \"dir1\" ]\n+  [ -L \"dir2/dir3\" ]\n+  [ ! -e \"../link1/a.dat\" ]\n+  [ ! -e \"../link2/dir4\" ]\n+  assert_clean_index\n+\n+  rm -rf .git/lfs/objects\n+\n+  rm -rf dir1 dir2/dir3\n+  mkdir link1 link2\n+  ln -s link1 dir1\n+  ln -s ../link2 dir2/dir3\n+\n+  git lfs pull 2>&1 | tee pull.log\n+  if [ \"0\" -ne \"${PIPESTATUS[0]}\" ], then\n+    echo >&2 \"fatal: expected pull to succeed ...\"\n+    exit 1\n+  fi\n+  grep '\"dir1/a\\.dat\": not a directory' pull.log\n+  grep '\"dir2/dir3/dir4/a\\.dat\": not a directory' pull.log\n+  [ -z \"$(grep \"is beyond a symbolic link\" pull.log)\" ]\n+\n+  assert_local_object \"$contents_oid\" 1\n+\n+  [ -L \"dir1\" ]\n+  [ -L \"dir2/dir3\" ]\n+  [ ! -e \"link1/a.dat\" ]\n+  [ ! -e \"link2/dir4\" ]\n+  assert_clean_index\n+\n+  rm -rf .git/lfs/objects\n+\n+  pushd dir2\n+    git lfs pull 2>&1 | tee pull.log\n+    if [ \"0\" -ne \"${PIPESTATUS[0]}\" ], then\n+      echo >&2 \"fatal: expected pull to succeed ...\"\n+      exit 1\n+    fi\n+    grep '\"dir1/a\\.dat\": not a directory' pull.log\n+    grep '\"dir2/dir3/dir4/a\\.dat\": not a directory' pull.log\n+    [ -z \"$(grep \"is beyond a symbolic link\" pull.log)\" ]\n+  popd\n+\n+  assert_local_object \"$contents_oid\" 1\n+\n+  [ -L \"dir1\" ]\n+  [ -L \"dir2/dir3\" ]\n+  [ ! -e \"link1/a.dat\" ]\n+  [ ! -e \"link2/dir4\" ]\n+  assert_clean_index\n+\n   # test with symlink to file and dangling symlink\n+  rm -rf .git/lfs/objects\n+\n   rm -rf dir1 dir2/dir3 ../link*\n   touch ../link1\n   ln -s ../link1 dir1\n@@ -347,12 +405,8 @@ begin_test \"pull: skip directory symlink conflicts\"\n     echo >&2 \"fatal: expected pull to succeed ...\"\n     exit 1\n   fi\n-  if [ \"$IS_WINDOWS\" -eq 1 ], then\n-    grep 'could not check out \"dir1/a\\.dat\": could not create working directory file' pull.log\n-  else\n-    grep 'Checkout error for \"dir1/a\\.dat\": lstat' pull.log\n-  fi\n-  grep 'could not check out \"dir2/dir3/dir4/a\\.dat\": could not create working directory file' pull.log\n+  grep '\"dir1/a\\.dat\": not a directory' pull.log\n+  grep '\"dir2/dir3/dir4/a\\.dat\": not a directory' pull.log\n \n   assert_local_object \"$contents_oid\" 1\n \n@@ -364,7 +418,7 @@ begin_test \"pull: skip directory symlink conflicts\"\n \n   rm -rf .git/lfs/objects\n \n-  rm -rf dir1 dir2/dir3\n+  rm -rf dir1 dir2/dir3 link*\n   touch link1\n   ln -s link1 dir1\n   ln -s ../link2 dir2/dir3\n@@ -374,12 +428,8 @@ begin_test \"pull: skip directory symlink conflicts\"\n     echo >&2 \"fatal: expected pull to succeed ...\"\n     exit 1\n   fi\n-  if [ \"$IS_WINDOWS\" -eq 1 ], then\n-    grep 'could not check out \"dir1/a\\.dat\": could not create working directory file' pull.log\n-  else\n-    grep 'Checkout error for \"dir1/a\\.dat\": lstat' pull.log\n-  fi\n-  grep 'could not check out \"dir2/dir3/dir4/a\\.dat\": could not create working directory file' pull.log\n+  grep '\"dir1/a\\.dat\": not a directory' pull.log\n+  grep '\"dir2/dir3/dir4/a\\.dat\": not a directory' pull.log\n \n   assert_local_object \"$contents_oid\" 1\n \n@@ -397,12 +447,8 @@ begin_test \"pull: skip directory symlink conflicts\"\n       echo >&2 \"fatal: expected pull to succeed ...\"\n       exit 1\n     fi\n-    if [ \"$IS_WINDOWS\" -eq 1 ], then\n-      grep 'could not check out \"dir1/a\\.dat\": could not create working directory file' pull.log\n-    else\n-      grep 'Checkout error for \"dir1/a\\.dat\": lstat' pull.log\n-    fi\n-    grep 'could not check out \"dir2/dir3/dir4/a\\.dat\": could not create working directory file' pull.log\n+    grep '\"dir1/a\\.dat\": not a directory' pull.log\n+    grep '\"dir2/dir3/dir4/a\\.dat\": not a directory' pull.log\n   popd\n \n   assert_local_object \"$contents_oid\" 1\n@@ -610,20 +656,26 @@ begin_test \"pull: skip case-based symlink conflicts\"\n   mkdir dir1\n   ln -s ../link1 A.dat\n   ln -s ../../link2 dir1/a.dat\n+  ln -s ../link3 DIR3\n+  ln -s ../../link4 dir1/dir2\n \n-  git add A.dat dir1\n+  git add A.dat dir1 DIR3\n   git commit -m \"initial commit\"\n \n-  rm A.dat dir1/a.dat\n+  rm A.dat dir1/* DIR3\n \n   echo \"*.dat filter=lfs diff=lfs merge=lfs -text\" >.gitattributes\n \n   contents=\"a\"\n   contents_oid=\"$(calc_oid \"$contents\")\"\n+  mkdir dir3 dir1/DIR2\n   printf \"%s\" \"$contents\" >a.dat\n   printf \"%s\" \"$contents\" >dir1/A.dat\n+  printf \"%s\" \"$contents\" >dir3/a.dat\n+  printf \"%s\" \"$contents\" >dir1/DIR2/a.dat\n \n-  git -c core.ignoreCase=false add .gitattributes a.dat dir1/A.dat\n+  git -c core.ignoreCase=false add .gitattributes a.dat dir1/A.dat \\\n+    dir3/a.dat dir1/DIR2/a.dat\n   git commit -m \"case-conflicting commit\"\n \n   git push origin main\n@@ -640,7 +692,8 @@ begin_test \"pull: skip case-based symlink conflicts\"\n   cd \"${reponame}-assert\"\n   refute_local_object \"$contents_oid\" 1\n \n-  rm -rf *.dat dir1 ../link*\n+  rm -rf *.dat dir1 *3 ../link*\n+  mkdir ../link3 ../link4\n \n   git lfs pull\n \n@@ -650,12 +703,18 @@ begin_test \"pull: skip case-based symlink conflicts\"\n   [ \"$contents\" = \"$(cat \"a.dat\")\" ]\n   [ -f \"dir1/A.dat\" ]\n   [ \"$contents\" = \"$(cat \"dir1/A.dat\")\" ]\n+  [ -f \"dir3/a.dat\" ]\n+  [ \"$contents\" = \"$(cat \"dir3/a.dat\")\" ]\n+  [ -f \"dir1/DIR2/a.dat\" ]\n+  [ \"$contents\" = \"$(cat \"dir1/DIR2/a.dat\")\" ]\n   [ ! -e \"../link1\" ]\n   [ ! -e \"../link2\" ]\n+  [ ! -e \"../link3/a.dat\" ]\n+  [ ! -e \"../link4/a.dat\" ]\n   assert_clean_index\n \n-  rm -rf a.dat dir1/A.dat\n-  git checkout -- A.dat dir1/a.dat\n+  rm -rf a.dat dir1/A.dat dir3 dir1/DIR2\n+  git checkout -- A.dat dir1/a.dat DIR3 dir1/dir2\n \n   git lfs pull 2>&1 | tee pull.log\n   if [ \"0\" -ne \"${PIPESTATUS[0]}\" ], then\n@@ -666,6 +725,9 @@ begin_test \"pull: skip case-based symlink conflicts\"\n     # case-insensitive filesystem\n     grep '\"a\\.dat\": not a regular file' pull.log\n     grep '\"dir1/A\\.dat\": not a regular file' pull.log\n+    grep '\"dir3/a\\.dat\": not a directory' pull.log\n+    grep '\"dir1/DIR2/a\\.dat\": not a directory' pull.log\n+    [ -z \"$(grep \"is beyond a symbolic link\" pull.log)\" ]\n   fi\n \n   if [ \"$collision\" -eq \"0\" ], then\n@@ -674,13 +736,21 @@ begin_test \"pull: skip case-based symlink conflicts\"\n     [ \"$contents\" = \"$(cat \"a.dat\")\" ]\n     [ -f \"dir1/A.dat\" ]\n     [ \"$contents\" = \"$(cat \"dir1/A.dat\")\" ]\n+    [ -f \"dir3/a.dat\" ]\n+    [ \"$contents\" = \"$(cat \"dir3/a.dat\")\" ]\n+    [ -f \"dir1/DIR2/a.dat\" ]\n+    [ \"$contents\" = \"$(cat \"dir1/DIR2/a.dat\")\" ]\n   else\n     # case-insensitive filesystem\n     [ -L \"a.dat\" ]\n     [ -L \"dir1/A.dat\" ]\n+    [ -L \"dir3\" ]\n+    [ -L \"dir1/DIR2\" ]\n   fi\n   [ ! -e \"../link1\" ]\n   [ ! -e \"../link2\" ]\n+  [ ! -e \"../link3/a.dat\" ]\n+  [ ! -e \"../link4/a.dat\" ]\n   assert_clean_index\n )\n end_test"
        },
        {
          "filename": "tools/dir_walker.go",
          "old_url": "https://raw.githubusercontent.com/git-lfs/git-lfs/e735de58a72f1333a16054985b85b863fbaec138/tools/dir_walker.go",
          "new_url": "https://raw.githubusercontent.com/git-lfs/git-lfs/0cffe93176b870055c9dadbb3cc9a4a440e98396/tools/dir_walker.go",
          "diff": "@@ -0,0 +1,107 @@\n+package tools\n+\n+import (\n+\t\"os\"\n+\t\"strings\"\n+\n+\t\"github.com/git-lfs/git-lfs/v3/errors\"\n+\t\"github.com/git-lfs/git-lfs/v3/tr\"\n+)\n+\n+var (\n+\terrInvalidDir = errors.New(tr.Tr.Get(\"invalid directory\"))\n+\terrNotDir     = errors.New(tr.Tr.Get(\"not a directory\"))\n+)\n+\n+type DirWalker struct {\n+\tparentPath string\n+\tpath       string\n+\tconfig     repositoryPermissionFetcher\n+}\n+\n+// The parentPath parameter is assumed to be a valid path to a directory\n+// in the filesystem.\n+//\n+// The filePath parameter must be a relative file path as provided by Git,\n+// with only the \"/\" character as a separator and no empty or \".\" or \"..\"\n+// path segments.  Absolute paths are not supported.\n+func NewDirWalkerForFile(parentPath string, filePath string, config repositoryPermissionFetcher) *DirWalker {\n+\tvar path string\n+\ti := strings.LastIndexByte(filePath, '/')\n+\tif i >= 0 {\n+\t\tpath = filePath[0:i]\n+\t}\n+\n+\treturn &DirWalker{\n+\t\tparentPath: parentPath,\n+\t\tpath:       path,\n+\t\tconfig:     config,\n+\t}\n+}\n+\n+// walk() checks each directory in a relative path, starting from the\n+// initial parent path, and optionally creates any missing directories\n+// in the path.\n+//\n+// If an existing file or something else other than a directory conflicts\n+// with a directory in the path, walk() returns an error.\n+//\n+// If the create option is false, walk() returns ErrNotExist when a\n+// directory is not found.\n+//\n+// Note that for performance reasons and to be consistent with Git's\n+// implementation, walk() does not guard against TOCTOU (time-of-check/\n+// time-of-use) races, as the methods of the os.Root type do.\n+func (w *DirWalker) walk(create bool) error {\n+\tcurrentPath := w.parentPath\n+\n+\tn := len(w.path)\n+\tfor n > 0 {\n+\t\tcurrentDir := w.path\n+\t\tnextDirIndex := n\n+\t\ti := strings.IndexByte(w.path, '/')\n+\t\tif i >= 0 {\n+\t\t\tcurrentDir = w.path[0:i]\n+\t\t\tnextDirIndex = i + 1\n+\t\t}\n+\n+\t\t// These should never occur in Git paths.\n+\t\tif currentDir == \"\" || currentDir == \".\" || currentDir == \"..\" {\n+\t\t\treturn errors.Join(errors.New(tr.Tr.Get(\"invalid directory %q in path: %q\", currentDir, w.path)), errInvalidDir)\n+\t\t}\n+\n+\t\tif currentPath == \"\" {\n+\t\t\tcurrentPath = currentDir\n+\t\t} else {\n+\t\t\tcurrentPath += \"/\" + currentDir\n+\t\t}\n+\n+\t\tstat, err := os.Lstat(currentPath)\n+\t\tif err != nil {\n+\t\t\tif !os.IsNotExist(err) || !create {\n+\t\t\t\treturn err\n+\t\t\t}\n+\n+\t\t\terr = Mkdir(currentPath, w.config)\n+\t\t\tif err != nil {\n+\t\t\t\treturn err\n+\t\t\t}\n+\t\t} else if !stat.Mode().IsDir() {\n+\t\t\treturn errors.Join(errors.New(tr.Tr.Get(\"not a directory: %q\", currentPath)), errNotDir)\n+\t\t}\n+\n+\t\tw.parentPath = currentPath\n+\t\tw.path = w.path[nextDirIndex:]\n+\t\tn -= nextDirIndex\n+\t}\n+\n+\treturn nil\n+}\n+\n+func (w *DirWalker) Walk() error {\n+\treturn w.walk(false)\n+}\n+\n+func (w *DirWalker) WalkAndCreate() error {\n+\treturn w.walk(true)\n+}"
        },
        {
          "filename": "tools/dir_walker_test.go",
          "old_url": "https://raw.githubusercontent.com/git-lfs/git-lfs/e735de58a72f1333a16054985b85b863fbaec138/tools/dir_walker_test.go",
          "new_url": "https://raw.githubusercontent.com/git-lfs/git-lfs/0cffe93176b870055c9dadbb3cc9a4a440e98396/tools/dir_walker_test.go",
          "diff": "@@ -0,0 +1,473 @@\n+package tools\n+\n+import (\n+\t\"errors\"\n+\t\"fmt\"\n+\t\"os\"\n+\t\"testing\"\n+\n+\t\"github.com/stretchr/testify/assert\"\n+\t\"github.com/stretchr/testify/require\"\n+)\n+\n+type newDirWalkerForFileTestCase struct {\n+\tfilePath        string\n+\texpectedDirPath string\n+}\n+\n+func (c *newDirWalkerForFileTestCase) Assert(t *testing.T) {\n+\tw := NewDirWalkerForFile(\"\", c.filePath, nil)\n+\tassert.Equal(t, c.expectedDirPath, w.path)\n+}\n+\n+func TestNewDirWalkerForFile(t *testing.T) {\n+\tfor desc, c := range map[string]*newDirWalkerForFileTestCase{\n+\t\t\"filename only\":            {\"foo.bin\", \"\"},\n+\t\t\"path with one dir\":        {\"abc/foo.bin\", \"abc\"},\n+\t\t\"path with two dirs\":       {\"abc/def/foo.bin\", \"abc/def\"},\n+\t\t\"path with leading slash\":  {\"/foo.bin\", \"\"},\n+\t\t\"path with trailing slash\": {\"abc/\", \"abc\"},\n+\t\t\"bare slash\":               {\"/\", \"\"},\n+\t\t\"empty path\":               {\"\", \"\"},\n+\t} {\n+\t\tt.Run(desc, c.Assert)\n+\t}\n+}\n+\n+type dirWalkerTestConfig struct{}\n+\n+func (c *dirWalkerTestConfig) RepositoryPermissions(executable bool) os.FileMode {\n+\treturn os.FileMode(0755)\n+}\n+\n+type dirWalkerWalkTestCase struct {\n+\tparentPath string\n+\tpath       string\n+\tcreate     bool\n+\n+\texistsPath string\n+\texistsFile string\n+\texistsLink string\n+\n+\texpectedParentPath string\n+\texpectedPath       string\n+\texpectedErr        error\n+\n+\twalker *DirWalker\n+}\n+\n+func (c *dirWalkerWalkTestCase) prependParentPath(path string) string {\n+\tif path == \"\" {\n+\t\treturn c.parentPath\n+\t} else if c.parentPath == \"\" {\n+\t\treturn path\n+\t} else if path[0] == '/' {\n+\t\treturn \"/\" + c.parentPath + path\n+\t} else {\n+\t\treturn c.parentPath + \"/\" + path\n+\t}\n+}\n+\n+func (c *dirWalkerWalkTestCase) setupPaths(t *testing.T, parentPath string) error {\n+\tc.parentPath = parentPath\n+\n+\tif parentPath != \"\" {\n+\t\tif err := os.MkdirAll(parentPath, 0755), err != nil {\n+\t\t\treturn fmt.Errorf(\"unable to create path: %w\", err)\n+\t\t}\n+\t}\n+\n+\tif c.existsPath != \"\" {\n+\t\tc.existsPath = c.prependParentPath(c.existsPath)\n+\t\tif err := os.MkdirAll(c.existsPath, 0755), err != nil {\n+\t\t\treturn fmt.Errorf(\"unable to create path: %w\", err)\n+\t\t}\n+\t}\n+\n+\tif c.existsFile != \"\" {\n+\t\tc.existsFile = c.prependParentPath(c.existsFile)\n+\t\tf, err := os.Create(c.existsFile)\n+\t\tif err != nil {\n+\t\t\treturn fmt.Errorf(\"unable to create file: %w\", err)\n+\t\t}\n+\t\tf.Close()\n+\t}\n+\n+\tif c.existsLink != \"\" {\n+\t\tc.existsLink = c.prependParentPath(c.existsLink)\n+\t\tif err := os.Symlink(t.TempDir(), c.existsLink), err != nil {\n+\t\t\treturn fmt.Errorf(\"unable to create symbolic link: %w\", err)\n+\t\t}\n+\t}\n+\n+\tc.expectedParentPath = c.prependParentPath(c.expectedParentPath)\n+\n+\treturn nil\n+}\n+\n+func (c *dirWalkerWalkTestCase) Assert(t *testing.T) {\n+\tc.walker.parentPath = c.parentPath\n+\tc.walker.path = c.path\n+\n+\terr := c.walker.walk(c.create)\n+\n+\tassert.Equal(t, c.expectedParentPath, c.walker.parentPath, \"found path does not match\")\n+\tassert.Equal(t, c.expectedPath, c.walker.path, \"missing path does not match\")\n+\tif c.expectedErr == nil {\n+\t\tassert.NoError(t, err)\n+\t} else {\n+\t\tassert.Error(t, err)\n+\t\tassert.True(t, errors.Is(err, c.expectedErr), \"wrong error type\")\n+\t}\n+}\n+\n+func TestDirWalkerWalk(t *testing.T) {\n+\twd, err := os.Getwd()\n+\trequire.NoError(t, err)\n+\n+\tdefer os.Chdir(wd)\n+\n+\tfor desc, c := range map[string]*dirWalkerWalkTestCase{\n+\t\t\"empty path\": {},\n+\t\t\"one extant dir\": {\n+\t\t\tpath:               \"abc\",\n+\t\t\texistsPath:         \"abc\",\n+\t\t\texpectedParentPath: \"abc\",\n+\t\t},\n+\t\t\"one missing dir\": {\n+\t\t\tpath:         \"abc\",\n+\t\t\texpectedPath: \"abc\",\n+\t\t\texpectedErr:  os.ErrNotExist,\n+\t\t},\n+\t\t\"two extant dirs\": {\n+\t\t\tpath:               \"abc/def\",\n+\t\t\texistsPath:         \"abc/def\",\n+\t\t\texpectedParentPath: \"abc/def\",\n+\t\t},\n+\t\t\"two missing dirs\": {\n+\t\t\tpath:         \"abc/def\",\n+\t\t\texpectedPath: \"abc/def\",\n+\t\t\texpectedErr:  os.ErrNotExist,\n+\t\t},\n+\t\t\"three extant dirs\": {\n+\t\t\tpath:               \"abc/def/ghi\",\n+\t\t\texistsPath:         \"abc/def/ghi\",\n+\t\t\texpectedParentPath: \"abc/def/ghi\",\n+\t\t},\n+\t\t\"three missing dirs\": {\n+\t\t\tpath:         \"abc/def/ghi\",\n+\t\t\texpectedPath: \"abc/def/ghi\",\n+\t\t\texpectedErr:  os.ErrNotExist,\n+\t\t},\n+\t\t\"one extant dir and one missing dir\": {\n+\t\t\tpath:               \"abc/def\",\n+\t\t\texistsPath:         \"abc\",\n+\t\t\texpectedParentPath: \"abc\",\n+\t\t\texpectedPath:       \"def\",\n+\t\t\texpectedErr:        os.ErrNotExist,\n+\t\t},\n+\t\t\"one extant dir and two missing dirs\": {\n+\t\t\tpath:               \"abc/def/ghi\",\n+\t\t\texistsPath:         \"abc\",\n+\t\t\texpectedParentPath: \"abc\",\n+\t\t\texpectedPath:       \"def/ghi\",\n+\t\t\texpectedErr:        os.ErrNotExist,\n+\t\t},\n+\t\t\"two extant dirs and one missing dir\": {\n+\t\t\tpath:               \"abc/def/ghi\",\n+\t\t\texistsPath:         \"abc/def\",\n+\t\t\texpectedParentPath: \"abc/def\",\n+\t\t\texpectedPath:       \"ghi\",\n+\t\t\texpectedErr:        os.ErrNotExist,\n+\t\t},\n+\t\t\"one missing dir with trailing slash\": {\n+\t\t\tpath:         \"abc/\",\n+\t\t\texpectedPath: \"abc/\",\n+\t\t\texpectedErr:  os.ErrNotExist,\n+\t\t},\n+\t\t\"one extant dir with trailing slash\": {\n+\t\t\tpath:               \"abc/\",\n+\t\t\texistsPath:         \"abc\",\n+\t\t\texpectedParentPath: \"abc\",\n+\t\t},\n+\t\t\"two extant dirs with trailing slash\": {\n+\t\t\tpath:               \"abc/def/\",\n+\t\t\texistsPath:         \"abc/def\",\n+\t\t\texpectedParentPath: \"abc/def\",\n+\t\t},\n+\t\t\"one extant dir and one missing dir with trailing slash\": {\n+\t\t\tpath:               \"abc/def/\",\n+\t\t\texistsPath:         \"abc\",\n+\t\t\texpectedParentPath: \"abc\",\n+\t\t\texpectedPath:       \"def/\",\n+\t\t\texpectedErr:        os.ErrNotExist,\n+\t\t},\n+\t\t\"one conflicting file\": {\n+\t\t\tpath:         \"abc\",\n+\t\t\texistsFile:   \"abc\",\n+\t\t\texpectedPath: \"abc\",\n+\t\t\texpectedErr:  errNotDir,\n+\t\t},\n+\t\t\"one extant dir and one conflicting file\": {\n+\t\t\tpath:               \"abc/def\",\n+\t\t\texistsPath:         \"abc\",\n+\t\t\texistsFile:         \"abc/def\",\n+\t\t\texpectedParentPath: \"abc\",\n+\t\t\texpectedPath:       \"def\",\n+\t\t\texpectedErr:        errNotDir,\n+\t\t},\n+\t\t\"two extant dirs and one conflicting file\": {\n+\t\t\tpath:               \"abc/def/ghi\",\n+\t\t\texistsPath:         \"abc/def\",\n+\t\t\texistsFile:         \"abc/def/ghi\",\n+\t\t\texpectedParentPath: \"abc/def\",\n+\t\t\texpectedPath:       \"ghi\",\n+\t\t\texpectedErr:        errNotDir,\n+\t\t},\n+\t\t\"one extant dir, one conflicting file, and one missing dir\": {\n+\t\t\tpath:               \"abc/def/ghi\",\n+\t\t\texistsPath:         \"abc\",\n+\t\t\texistsFile:         \"abc/def\",\n+\t\t\texpectedParentPath: \"abc\",\n+\t\t\texpectedPath:       \"def/ghi\",\n+\t\t\texpectedErr:        errNotDir,\n+\t\t},\n+\t\t\"one conflicting symlink\": {\n+\t\t\tpath:         \"abc\",\n+\t\t\texistsLink:   \"abc\",\n+\t\t\texpectedPath: \"abc\",\n+\t\t\texpectedErr:  errNotDir,\n+\t\t},\n+\t\t\"one extant dir and one conflicting symlink\": {\n+\t\t\tpath:               \"abc/def\",\n+\t\t\texistsPath:         \"abc\",\n+\t\t\texistsLink:         \"abc/def\",\n+\t\t\texpectedParentPath: \"abc\",\n+\t\t\texpectedPath:       \"def\",\n+\t\t\texpectedErr:        errNotDir,\n+\t\t},\n+\t\t\"two extant dirs and one conflicting symlink\": {\n+\t\t\tpath:               \"abc/def/ghi\",\n+\t\t\texistsPath:         \"abc/def\",\n+\t\t\texistsLink:         \"abc/def/ghi\",\n+\t\t\texpectedParentPath: \"abc/def\",\n+\t\t\texpectedPath:       \"ghi\",\n+\t\t\texpectedErr:        errNotDir,\n+\t\t},\n+\t\t\"one extant dir, one conflicting symlink, and one missing dir\": {\n+\t\t\tpath:               \"abc/def/ghi\",\n+\t\t\texistsPath:         \"abc\",\n+\t\t\texistsLink:         \"abc/def\",\n+\t\t\texpectedParentPath: \"abc\",\n+\t\t\texpectedPath:       \"def/ghi\",\n+\t\t\texpectedErr:        errNotDir,\n+\t\t},\n+\t\t\"one extant dir (not modified)\": {\n+\t\t\tpath:               \"abc\",\n+\t\t\tcreate:             true,\n+\t\t\texistsPath:         \"abc\",\n+\t\t\texpectedParentPath: \"abc\",\n+\t\t},\n+\t\t\"one created dir\": {\n+\t\t\tpath:               \"abc\",\n+\t\t\tcreate:             true,\n+\t\t\texpectedParentPath: \"abc\",\n+\t\t},\n+\t\t\"two extant dirs (not modified)\": {\n+\t\t\tpath:               \"abc/def\",\n+\t\t\tcreate:             true,\n+\t\t\texistsPath:         \"abc/def\",\n+\t\t\texpectedParentPath: \"abc/def\",\n+\t\t},\n+\t\t\"two created dirs\": {\n+\t\t\tpath:               \"abc/def\",\n+\t\t\tcreate:             true,\n+\t\t\texpectedParentPath: \"abc/def\",\n+\t\t},\n+\t\t\"three extant dirs (not modified)\": {\n+\t\t\tpath:               \"abc/def/ghi\",\n+\t\t\tcreate:             true,\n+\t\t\texistsPath:         \"abc/def/ghi\",\n+\t\t\texpectedParentPath: \"abc/def/ghi\",\n+\t\t},\n+\t\t\"three created dirs\": {\n+\t\t\tpath:               \"abc/def/ghi\",\n+\t\t\tcreate:             true,\n+\t\t\texpectedParentPath: \"abc/def/ghi\",\n+\t\t},\n+\t\t\"one extant dir and one created dir\": {\n+\t\t\tpath:               \"abc/def\",\n+\t\t\tcreate:             true,\n+\t\t\texistsPath:         \"abc\",\n+\t\t\texpectedParentPath: \"abc/def\",\n+\t\t},\n+\t\t\"one extant dir and two created dirs\": {\n+\t\t\tpath:               \"abc/def/ghi\",\n+\t\t\tcreate:             true,\n+\t\t\texistsPath:         \"abc\",\n+\t\t\texpectedParentPath: \"abc/def/ghi\",\n+\t\t},\n+\t\t\"two extant dirs and one created dir\": {\n+\t\t\tpath:               \"abc/def/ghi\",\n+\t\t\tcreate:             true,\n+\t\t\texistsPath:         \"abc/def\",\n+\t\t\texpectedParentPath: \"abc/def/ghi\",\n+\t\t},\n+\t\t\"one created dir with trailing slash\": {\n+\t\t\tpath:               \"abc/\",\n+\t\t\tcreate:             true,\n+\t\t\texpectedParentPath: \"abc\",\n+\t\t},\n+\t\t\"one extant dir with trailing slash (not modified)\": {\n+\t\t\tpath:               \"abc/\",\n+\t\t\tcreate:             true,\n+\t\t\texistsPath:         \"abc\",\n+\t\t\texpectedParentPath: \"abc\",\n+\t\t},\n+\t\t\"two extant dirs with trailing slash (not modified)\": {\n+\t\t\tpath:               \"abc/def/\",\n+\t\t\tcreate:             true,\n+\t\t\texistsPath:         \"abc/def\",\n+\t\t\texpectedParentPath: \"abc/def\",\n+\t\t},\n+\t\t\"one extant dir and one created dir with trailing slash\": {\n+\t\t\tpath:               \"abc/def/\",\n+\t\t\tcreate:             true,\n+\t\t\texistsPath:         \"abc\",\n+\t\t\texpectedParentPath: \"abc/def\",\n+\t\t},\n+\t\t\"one conflicting file (not modified)\": {\n+\t\t\tpath:         \"abc\",\n+\t\t\tcreate:       true,\n+\t\t\texistsFile:   \"abc\",\n+\t\t\texpectedPath: \"abc\",\n+\t\t\texpectedErr:  errNotDir,\n+\t\t},\n+\t\t\"one extant dir and one conflicting file (not modified)\": {\n+\t\t\tpath:               \"abc/def\",\n+\t\t\tcreate:             true,\n+\t\t\texistsPath:         \"abc\",\n+\t\t\texistsFile:         \"abc/def\",\n+\t\t\texpectedParentPath: \"abc\",\n+\t\t\texpectedPath:       \"def\",\n+\t\t\texpectedErr:        errNotDir,\n+\t\t},\n+\t\t\"two extant dirs and one conflicting file (not modified)\": {\n+\t\t\tpath:               \"abc/def/ghi\",\n+\t\t\tcreate:             true,\n+\t\t\texistsPath:         \"abc/def\",\n+\t\t\texistsFile:         \"abc/def/ghi\",\n+\t\t\texpectedParentPath: \"abc/def\",\n+\t\t\texpectedPath:       \"ghi\",\n+\t\t\texpectedErr:        errNotDir,\n+\t\t},\n+\t\t\"one extant dir, one conflicting file, and one missing dir (not modified)\": {\n+\t\t\tpath:               \"abc/def/ghi\",\n+\t\t\tcreate:             true,\n+\t\t\texistsPath:         \"abc\",\n+\t\t\texistsFile:         \"abc/def\",\n+\t\t\texpectedParentPath: \"abc\",\n+\t\t\texpectedPath:       \"def/ghi\",\n+\t\t\texpectedErr:        errNotDir,\n+\t\t},\n+\t\t\"one conflicting symlink (not modified)\": {\n+\t\t\tpath:         \"abc\",\n+\t\t\tcreate:       true,\n+\t\t\texistsLink:   \"abc\",\n+\t\t\texpectedPath: \"abc\",\n+\t\t\texpectedErr:  errNotDir,\n+\t\t},\n+\t\t\"one extant dir and one conflicting symlink (not modified)\": {\n+\t\t\tpath:               \"abc/def\",\n+\t\t\tcreate:             true,\n+\t\t\texistsPath:         \"abc\",\n+\t\t\texistsLink:         \"abc/def\",\n+\t\t\texpectedParentPath: \"abc\",\n+\t\t\texpectedPath:       \"def\",\n+\t\t\texpectedErr:        errNotDir,\n+\t\t},\n+\t\t\"two extant dirs and one conflicting symlink (not modified)\": {\n+\t\t\tpath:               \"abc/def/ghi\",\n+\t\t\tcreate:             true,\n+\t\t\texistsPath:         \"abc/def\",\n+\t\t\texistsLink:         \"abc/def/ghi\",\n+\t\t\texpectedParentPath: \"abc/def\",\n+\t\t\texpectedPath:       \"ghi\",\n+\t\t\texpectedErr:        errNotDir,\n+\t\t},\n+\t\t\"one extant dir, one conflicting symlink, and one missing dir (not modified)\": {\n+\t\t\tpath:               \"abc/def/ghi\",\n+\t\t\tcreate:             true,\n+\t\t\texistsPath:         \"abc\",\n+\t\t\texistsLink:         \"abc/def\",\n+\t\t\texpectedParentPath: \"abc\",\n+\t\t\texpectedPath:       \"def/ghi\",\n+\t\t\texpectedErr:        errNotDir,\n+\t\t},\n+\t\t\"invalid bare slash\": {\n+\t\t\tpath:         \"/\",\n+\t\t\texpectedPath: \"/\",\n+\t\t\texpectedErr:  errInvalidDir,\n+\t\t},\n+\t\t\"invalid multiple slashes\": {\n+\t\t\tpath:               \"abc//def\",\n+\t\t\texistsPath:         \"abc\",\n+\t\t\texpectedParentPath: \"abc\",\n+\t\t\texpectedPath:       \"/def\",\n+\t\t\texpectedErr:        errInvalidDir,\n+\t\t},\n+\t\t\"invalid leading slash\": {\n+\t\t\tpath:         \"/abc\",\n+\t\t\texistsPath:   \"abc\",\n+\t\t\texpectedPath: \"/abc\",\n+\t\t\texpectedErr:  errInvalidDir,\n+\t\t},\n+\t\t\"invalid bare dot component\": {\n+\t\t\tpath:         \".\",\n+\t\t\texpectedPath: \".\",\n+\t\t\texpectedErr:  errInvalidDir,\n+\t\t},\n+\t\t\"invalid dot component\": {\n+\t\t\tpath:               \"abc/./def\",\n+\t\t\texistsPath:         \"abc/def\",\n+\t\t\texpectedParentPath: \"abc\",\n+\t\t\texpectedPath:       \"./def\",\n+\t\t\texpectedErr:        errInvalidDir,\n+\t\t},\n+\t\t\"invalid bare double-dot component\": {\n+\t\t\tpath:         \"..\",\n+\t\t\texpectedPath: \"..\",\n+\t\t\texpectedErr:  errInvalidDir,\n+\t\t},\n+\t\t\"invalid double-dot component\": {\n+\t\t\tpath:               \"abc/../def\",\n+\t\t\texistsPath:         \"abc\",\n+\t\t\texpectedParentPath: \"abc\",\n+\t\t\texpectedPath:       \"../def\",\n+\t\t\texpectedErr:        errInvalidDir,\n+\t\t},\n+\t} {\n+\t\tif err := os.Chdir(t.TempDir()), err != nil {\n+\t\t\tt.Errorf(\"unable to change directory: %s\", err)\n+\t\t}\n+\n+\t\tc.walker = &DirWalker{\n+\t\t\tconfig: &dirWalkerTestConfig{},\n+\t\t}\n+\n+\t\tif err := c.setupPaths(t, \"\"), err != nil {\n+\t\t\tt.Error(err)\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\tt.Run(desc, c.Assert)\n+\n+\t\t// retest with parent path, note that this alters the test case\n+\t\tif err := c.setupPaths(t, \"foo/bar\"), err != nil {\n+\t\t\tt.Error(err)\n+\t\t\tcontinue\n+\t\t}\n+\n+\t\tt.Run(desc+\" with parent path\", c.Assert)\n+\t}\n+}"
        },
        {
          "filename": "tools/filetools.go",
          "old_url": "https://raw.githubusercontent.com/git-lfs/git-lfs/e735de58a72f1333a16054985b85b863fbaec138/tools/filetools.go",
          "new_url": "https://raw.githubusercontent.com/git-lfs/git-lfs/0cffe93176b870055c9dadbb3cc9a4a440e98396/tools/filetools.go",
          "diff": "@@ -121,6 +121,15 @@ type repositoryPermissionFetcher interface {\n \tRepositoryPermissions(executable bool) os.FileMode\n }\n \n+// Mkdir makes a directory with the\n+// permissions specified by the core.sharedRepository setting.\n+func Mkdir(path string, config repositoryPermissionFetcher) error {\n+\tumask := 0777 & ^config.RepositoryPermissions(true)\n+\treturn doWithUmask(int(umask), func() error {\n+\t\treturn os.Mkdir(path, config.RepositoryPermissions(true))\n+\t})\n+}\n+\n // MkdirAll makes a directory and any intervening directories with the\n // permissions specified by the core.sharedRepository setting.\n func MkdirAll(path string, config repositoryPermissionFetcher) error {"
        }
      ]
    }
  ]
]